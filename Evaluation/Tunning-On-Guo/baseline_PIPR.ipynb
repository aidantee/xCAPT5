{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anhvt00/PIPR/blob/master/Evaluation/Tunning-On-Guo/baseline_PIPR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3tRl6ZSk8Oi"
      },
      "source": [
        "This notebook use for tunning model using embeddings file and language model embedder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wz47D5H_R0UR"
      },
      "source": [
        "### Check GPU hardware"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8kCH-Zfj2J_",
        "outputId": "50bcd1b1-7af1-443f-aa54-ce9d4ecee479"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Jan  9 15:08:20 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-chmuAHjK8D0"
      },
      "source": [
        "### Download embedding files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SrKgkXJA3wPb",
        "outputId": "3f466a23-0cc4-4074-c1d1-3b1828b27a1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-01-09 15:08:21--  https://raw.githubusercontent.com/anhvt00/PIPR/master/embeddings/seq2tensor.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1104 (1.1K) [text/plain]\n",
            "Saving to: ‘seq2tensor.py’\n",
            "\n",
            "seq2tensor.py       100%[===================>]   1.08K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-01-09 15:08:22 (45.8 MB/s) - ‘seq2tensor.py’ saved [1104/1104]\n",
            "\n",
            "--2022-01-09 15:08:22--  https://raw.githubusercontent.com/anhvt00/PIPR/master/embeddings/ac5_aph_CT.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2530 (2.5K) [text/plain]\n",
            "Saving to: ‘ac5_aph_CT.txt’\n",
            "\n",
            "ac5_aph_CT.txt      100%[===================>]   2.47K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-01-09 15:08:23 (23.0 MB/s) - ‘ac5_aph_CT.txt’ saved [2530/2530]\n",
            "\n",
            "--2022-01-09 15:08:23--  https://raw.githubusercontent.com/anhvt00/PIPR/master/yeast/preprocessed/protein_dictionary.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1415079 (1.3M) [text/plain]\n",
            "Saving to: ‘protein_dictionary.tsv’\n",
            "\n",
            "protein_dictionary. 100%[===================>]   1.35M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2022-01-09 15:08:23 (20.7 MB/s) - ‘protein_dictionary.tsv’ saved [1415079/1415079]\n",
            "\n",
            "--2022-01-09 15:08:23--  https://raw.githubusercontent.com/anhvt00/PIPR/master/yeast/preprocessed/protein_pairs.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 179008 (175K) [text/plain]\n",
            "Saving to: ‘protein_pairs.tsv’\n",
            "\n",
            "protein_pairs.tsv   100%[===================>] 174.81K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2022-01-09 15:08:24 (5.95 MB/s) - ‘protein_pairs.tsv’ saved [179008/179008]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Download file seq2tensor.py for converting protein sequences to tensors\n",
        "!wget https://raw.githubusercontent.com/anhvt00/PIPR/master/embeddings/seq2tensor.py\n",
        "\n",
        "# Download file ac5_aph.txt for ac5_aph embedding \n",
        "!wget https://raw.githubusercontent.com/anhvt00/PIPR/master/embeddings/ac5_aph_CT.txt\n",
        "\n",
        "### Download interaction pairs and dictionary files\n",
        "# Download dictionary file (id: sequence)\n",
        "!wget https://raw.githubusercontent.com/anhvt00/PIPR/master/yeast/preprocessed/protein_dictionary.tsv\n",
        "# !wget https://raw.githubusercontent.com/anhvt00/PIPR/master/yeast/yeast_DSCRIPT/yeast_dictionary.tsv\n",
        "\n",
        "# Download pairs of proteins with labels file\n",
        "!wget https://raw.githubusercontent.com/anhvt00/PIPR/master/yeast/preprocessed/protein_pairs.tsv\n",
        "# !wget https://raw.githubusercontent.com/anhvt00/PIPR/master/yeast/yeast_DSCRIPT/yeast_pairs.tsv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiMnInVvlEjY"
      },
      "source": [
        "### Import libraries "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FBCZs6wgdV7E",
        "outputId": "36380b5c-8e54-499a-f16a-3e8f55f02f38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorboard_plugin_profile\n",
            "  Downloading tensorboard_plugin_profile-2.5.0-py3-none-any.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 4.4 MB/s \n",
            "\u001b[?25hCollecting gviz-api>=1.9.0\n",
            "  Downloading gviz_api-1.10.0-py2.py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard_plugin_profile) (1.0.1)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard_plugin_profile) (3.17.3)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard_plugin_profile) (1.15.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard_plugin_profile) (57.4.0)\n",
            "Installing collected packages: gviz-api, tensorboard-plugin-profile\n",
            "Successfully installed gviz-api-1.10.0 tensorboard-plugin-profile-2.5.0\n"
          ]
        }
      ],
      "source": [
        "# Libraries for system and debug\n",
        "import sys\n",
        "import pdb\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# Class for converting sequences to tensors\n",
        "from seq2tensor import s2t\n",
        "\n",
        "# Libraries for neural network training\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, GRU, LSTM, Bidirectional, Input, Conv1D, Conv2D\n",
        "from tensorflow.keras.layers import Add, Flatten, subtract, multiply, concatenate\n",
        "from tensorflow.keras.layers import MaxPooling1D, AveragePooling1D, GlobalAveragePooling1D, MaxPooling2D\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop\n",
        "from tensorflow.keras.layers import Dropout, BatchNormalization\n",
        "from tensorflow.keras.utils import Sequence\n",
        "from tensorflow.keras import mixed_precision\n",
        "from tensorflow import keras\n",
        "from tensorboard.plugins.hparams import api as hp\n",
        "from tensorflow.keras.utils import get_custom_objects\n",
        "from tensorflow.keras.layers import Activation\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "# Import accessory modules\n",
        "import numpy as np\n",
        "import h5py\n",
        "import gc\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "# For tensorboard extension\n",
        "!pip install -U tensorboard_plugin_profile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjxFKiABLDob"
      },
      "source": [
        "### Set CUDA environment variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjvCG7HCp0Af",
        "outputId": "cdc44d52-453f-4722-f466-af4b751bf6b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 Physical GPUs, 1 Logical GPUs\n"
          ]
        }
      ],
      "source": [
        "### Setting RAM GPU for training growth \n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "  try:\n",
        "    # Currently, memory growth needs to be the same across GPUs\n",
        "    for gpu in gpus:\n",
        "      tf.config.experimental.set_memory_growth(gpu, True)\n",
        "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
        "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
        "  except RuntimeError as e:\n",
        "    # Memory growth must be set before GPUs have been initialized\n",
        "    print(e)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ============================================\n",
        "# Optimisation Flags - Do not remove\n",
        "# ============================================\n",
        "\n",
        "# Disables caching (when set to 1) or enables caching (when set to 0) for just-in-time-compilation. When disabled,\n",
        "# no binary code is added to or retrieved from the cache.\n",
        "os.environ['CUDA_CACHE_DISABLE'] = '0' # orig is 0\n",
        "\n",
        "# When set to 1, forces the device driver to ignore any binary code embedded in an application \n",
        "# (see Application Compatibility) and to just-in-time compile embedded PTX code instead.\n",
        "# If a kernel does not have embedded PTX code, it will fail to load. This environment variable can be used to\n",
        "# validate that PTX code is embedded in an application and that its just-in-time compilation works as expected to guarantee application \n",
        "# forward compatibility with future architectures.\n",
        "os.environ['CUDA_FORCE_PTX_JIT'] = '1'# no orig\n",
        "\n",
        "\n",
        "os.environ['HOROVOD_GPU_ALLREDUCE'] = 'NCCL'\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "\n",
        "os.environ['TF_GPU_THREAD_MODE'] = 'gpu_private'\n",
        "os.environ['TF_GPU_THREAD_COUNT']='1'\n",
        "\n",
        "os.environ['TF_USE_CUDNN_BATCHNORM_SPATIAL_PERSISTENT'] = '1'\n",
        "\n",
        "os.environ['TF_ADJUST_HUE_FUSED'] = '1'\n",
        "os.environ['TF_ADJUST_SATURATION_FUSED'] = '1'\n",
        "os.environ['TF_ENABLE_WINOGRAD_NONFUSED'] = '1'\n",
        "\n",
        "os.environ['TF_SYNC_ON_FINISH'] = '0'\n",
        "os.environ['TF_AUTOTUNE_THRESHOLD'] = '2'\n",
        "os.environ['TF_DISABLE_NVTX_RANGES'] = '1'\n",
        "os.environ[\"TF_ENABLE_AUTO_MIXED_PRECISION_GRAPH_REWRITE\"] = \"1\"\n",
        "\n",
        "\n",
        "\n",
        "# =================================================\n",
        "# mixed_precision.set_global_policy('mixed_float16')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYxewWysW5U5"
      },
      "source": [
        "### Hyperparameter set by default\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TGt47faMW5U6"
      },
      "outputs": [],
      "source": [
        "# Default hyperparameters\n",
        "CONV_HIDDEN_DIM = 48\n",
        "RNN_HIDDEN = 48\n",
        "N_EPOCHS = 100\n",
        "HIDDEN_DIM=50\n",
        "BATCH_SIZE = 32\n",
        "DTYPE='float16'\n",
        "LEARNING_RATE=.001\n",
        "EPSILON=1e-6\n",
        "adam = Adam(learning_rate=LEARNING_RATE, amsgrad=True, epsilon=EPSILON)\n",
        "MAX_DATASET_SIZE = 11187\n",
        "DATASET_SIZE = MAX_DATASET_SIZE\n",
        "KERNEL_SIZE = 3\n",
        "POOLING_KERNEL = 3\n",
        "seq_size=1500\n",
        "dim = 1024\n",
        "# 1 for language model embedding\n",
        "flags_embedding = 0\n",
        "# 1 for loading from drive\n",
        "available_data = 0\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjTWZj-Bzebi"
      },
      "source": [
        "### Load the embeddings from drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uHFMvEvzzd_3"
      },
      "outputs": [],
      "source": [
        "if available_data == 1:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    # seq_tensor = np.load('/content/drive/MyDrive/seq_tensor.npy', allow_pickle=True)\n",
        "    seq_tensor = np.load('/content/drive/MyDrive/prottranst5uniref_2000.npy', allow_pickle=True)\n",
        "\n",
        "    class_labels = np.load('/content/drive/MyDrive/class_labels.npy', allow_pickle=True)\n",
        "    seq_index1 = np.load('/content/drive/MyDrive/seq_index1.npy', allow_pickle=True)\n",
        "    seq_index2 = np.load('/content/drive/MyDrive/seq_index2.npy', allow_pickle=True)\n",
        "    seq_tensor_physicochemical= np.load('/content/drive/MyDrive/physicochemical.npy', allow_pickle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KHCkDUohpFt8"
      },
      "outputs": [],
      "source": [
        "# seq_tensor = np.concatenate((seq_tensor, seq_tensor_physicochemical), axis=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TpKS4zsW5U8"
      },
      "source": [
        "### Use universal embedding files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OkpDLs-yW5U8"
      },
      "outputs": [],
      "source": [
        "if available_data == 0:\n",
        "  id2seq_file = 'protein_dictionary.tsv'\n",
        "  id2index = {}\n",
        "  seqs = []\n",
        "  index = 0\n",
        "  sid1_index = 0\n",
        "  sid2_index = 1\n",
        "  ds_file = 'protein_pairs.tsv'\n",
        "  label_index = 2\n",
        "  use_emb = 'ac5_aph.txt'\n",
        "\n",
        "\n",
        "  # Create line variable as a list of protein sequences with index is the number of protein sequences\n",
        "  # id2index is a dictionary of protein id and incremental index number \n",
        "  for line in open(id2seq_file):\n",
        "      line = line.strip().split('\\t')\n",
        "      id2index[line[0]] = index\n",
        "      seqs.append(line[1])\n",
        "      index += 1\n",
        "\n",
        "  seq_array = []\n",
        "  id2_aid = {}\n",
        "  sid = 0\n",
        "\n",
        "  seq2t = s2t(use_emb)\n",
        "\n",
        "  max_data = -1\n",
        "  limit_data = max_data > 0\n",
        "  raw_data = []\n",
        "  skip_head = True\n",
        "  x = None\n",
        "  count = 0\n",
        "\n",
        "  # Create sequence array as a list of protein strings\n",
        "  for line in tqdm(open(ds_file)):\n",
        "      if skip_head:\n",
        "          skip_head = False\n",
        "          continue\n",
        "      line = line.rstrip('\\n').rstrip('\\r').split('\\t')\n",
        "      if id2index.get(line[sid1_index]) is None or id2index.get(line[sid2_index]) is None:\n",
        "          continue\n",
        "      if id2_aid.get(line[sid1_index]) is None:\n",
        "          id2_aid[line[sid1_index]] = sid\n",
        "          sid += 1\n",
        "          seq_array.append(seqs[id2index[line[sid1_index]]])\n",
        "      line[sid1_index] = id2_aid[line[sid1_index]]\n",
        "      if id2_aid.get(line[sid2_index]) is None:\n",
        "          id2_aid[line[sid2_index]] = sid\n",
        "          sid += 1\n",
        "          seq_array.append(seqs[id2index[line[sid2_index]]])\n",
        "      line[sid2_index] = id2_aid[line[sid2_index]]\n",
        "      raw_data.append(line)\n",
        "      if limit_data:\n",
        "          count += 1\n",
        "          if count >= max_data:\n",
        "              break\n",
        "\n",
        "  len_m_seq = np.array([len(line.split()) for line in seq_array])\n",
        "  avg_m_seq = int(np.average(len_m_seq)) + 1\n",
        "  max_m_seq = max(len_m_seq)\n",
        "  dim = seq2t.dim\n",
        "\n",
        "  # seq_tensor is tensor representation of dataset having shape of (number_of_sequences, padding_length, embedding_dim_of_aa)\n",
        "  # Random for distribution of class labels\n",
        "  np.random.seed(42)\n",
        "  np.random.shuffle(raw_data)\n",
        "  seq_tensor = np.array([seq2t.embed_normalized(line, seq_size) for line in tqdm(seq_array)]).astype('float16')\n",
        "\n",
        "  # Extract index of 1st and 2nd sequences in pairs\n",
        "  seq_index1 = np.array([line[sid1_index] for line in tqdm(raw_data)])\n",
        "  seq_index2 = np.array([line[sid2_index] for line in tqdm(raw_data)])\n",
        "\n",
        "  # Assign labels for pairs of sequences\n",
        "  class_map = {'0':1,'1':0}\n",
        "  class_labels = np.zeros((len(raw_data), 2))\n",
        "  for i in range(len(raw_data)):\n",
        "      class_labels[i][class_map[raw_data[i][label_index]]] = 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thgXgMlTW5U9"
      },
      "source": [
        "### Use language model for embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_nzJn-AW5U9"
      },
      "outputs": [],
      "source": [
        "if flags_embedding == 1:\n",
        "    !pip install bio-embeddings[all] # Need to restart runtime for the first run \n",
        "\n",
        "    # Choose protein language model for embedder\n",
        "    from Bio import SeqIO # From Biopython library import SeqIO module to handle sequences when read and write different file formats\n",
        "\n",
        "    # Chooose language model embedder class from package bio-embeddings -> moduel embed\n",
        "    # from bio_embeddings.embed import CPCProtEmbedder, ProtTransT5XLU50Embedder, FastTextEmbedder, GloveEmbedder, PLUSRNNEmbedder, ProtTransBertBFDEmbedder, SeqVecEmbedder, UniRepEmbedder, Word2VecEmbedder, ProtTransXLNetUniRef100Embedder\n",
        "    #   from bio_embeddings.embed import ProtTransBertBFDEmbedder\n",
        "    from bio_embeddings.embed import ProtTransT5UniRef50Embedder\n",
        "    embedder = ProtTransT5UniRef50Embedder()\n",
        "\n",
        "    # Download raw sequences and create a list of sequences\n",
        "    !wget https://raw.githubusercontent.com/anhvt00/PIPR/master/yeast/preprocessed/protein_preprocessed.txt\n",
        "    with open('protein_preprocessed.txt') as file:\n",
        "        sequences = file.readlines()\n",
        "        sequences = [sequence.rstrip() for sequence in sequences]\n",
        "\n",
        "    # Install in the case of using A100 for pytorch compatibility\\\n",
        "    A100_status = !nvidia-smi | grep 'A100'\n",
        "    if A100_status:\n",
        "        !pip install torch==1.8.1+cu111 torchvision==0.9.1+cu111 torchaudio==0.8.1 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "\n",
        "    # embeddings = []\n",
        "    # for sequence in sequences:\n",
        "    #     embeddings.append(embedder.embed(sequence))\n",
        "\n",
        "    # # Start embedding \n",
        "    # # Embedding in generator form, need to iterate (flexible)\n",
        "    embeddings = embedder.embed_many(sequences)\n",
        "\n",
        "    # # Use list function to convert generator to list (true form of dataset)\n",
        "    embeddings = list(embeddings)\n",
        "\n",
        "    # # Average pooling in sequence dimension\n",
        "    # reduced_embeddings = [ProtTransBertBFDEmbedder.reduce_per_protein(e) for e in embeddings]\n",
        "\n",
        "    # # Padding to create fixed size tensor\n",
        "    seq_tensor= tf.keras.preprocessing.sequence.pad_sequences(embeddings,  padding='post', dtype='float16', truncating='post', maxlen=seq_size)\n",
        "    dim = seq_tensor.shape[2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "189z3qijJ2F2"
      },
      "source": [
        "### Define custom function "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LcEhSLxsONax"
      },
      "outputs": [],
      "source": [
        "def leaky_relu(x, alpha = .3):\n",
        "   return tf.keras.backend.maximum(alpha*x, x)\n",
        "\n",
        "get_custom_objects().update({'leaky_relu': leaky_relu})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HyK_KJ6LUfR"
      },
      "source": [
        "### Search for optimal configurations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SlRW8Nh71zjs"
      },
      "outputs": [],
      "source": [
        "# HP_EPSILON = hp.HParam('epsilon', hp.Discrete([1e-5, 1e-6, 1e-7]))\n",
        "HP_EPSILON = hp.HParam('epsilon', hp.Discrete([1e-6]))\n",
        "\n",
        "# HP_LEARNING_RATE = hp.HParam('learning_rate', hp.Discrete([1e-2, 1e-3, 1e-4]))\n",
        "HP_LEARNING_RATE = hp.HParam('learning_rate', hp.Discrete([1e-3]))\n",
        "\n",
        "# HP_FIRST_DENSE = hp.HParam('first_dense', hp.Discrete([100]))\n",
        "HP_FIRST_DENSE = hp.HParam('first_dense', hp.Discrete([48]))\n",
        "\n",
        "# HP_KERNEL_SIZE = hp.HParam('kernel_size', hp.Discrete([2, 3, 4]))\n",
        "HP_KERNEL_SIZE = hp.HParam('kernel_size', hp.Discrete([3]))\n",
        "\n",
        "# HP_POOLING_KERNEL = hp.HParam('pooling_kernel', hp.Discrete([2, 3, 4]))\n",
        "HP_POOLING_KERNEL = hp.HParam('pooling_kernel', hp.Discrete([3]))\n",
        "\n",
        "HP_CONV_HIDDEN_DIM = hp.HParam('conv_hidden_dim', hp.Discrete([50]))\n",
        "HP_RNN_HIDDEN_DIM = hp.HParam('rnn_hidden_dim', hp.Discrete([50]))\n",
        "\n",
        "HP_ACTIVATION = hp.HParam('activation', hp.Discrete(['leaky_relu']))\n",
        "# HP_ACTIVATION = hp.HParam('activation', hp.Discrete(['linear']))\n",
        "\n",
        "# HP_ACTIVATION_CONV = hp.HParam('activation_conv', hp.Discrete([ 'leaky_relu']))\n",
        "HP_ACTIVATION_CONV = hp.HParam('activation_conv', hp.Discrete(['linear']))\n",
        "\n",
        "HP_REGULARIZER = hp.HParam('regularizer', hp.Discrete([0]))\n",
        "\n",
        "HP_CONV_PADDING = hp.HParam('conv_padding', hp.Discrete(['valid']))\n",
        "\n",
        "HP_DROPOUT = hp.HParam('dropout', hp.Discrete([0e-1]))\n",
        "\n",
        "HP_BATCH_SIZE = hp.HParam('batch_size', hp.Discrete([256]))\n",
        "\n",
        "HP_LEAKY_RELU = hp.HParam('leaky_relu', hp.Discrete([3e-1]))\n",
        "\n",
        "METRIC_ACCURACY = 'accuracy'\n",
        "\n",
        "with tf.summary.create_file_writer('logs/hparam_tuning').as_default():\n",
        "  hp.hparams_config(\n",
        "    hparams=[HP_EPSILON,HP_LEARNING_RATE,HP_FIRST_DENSE, HP_KERNEL_SIZE, HP_POOLING_KERNEL, HP_CONV_HIDDEN_DIM, HP_RNN_HIDDEN_DIM, HP_ACTIVATION, HP_ACTIVATION_CONV, HP_REGULARIZER, HP_CONV_PADDING, HP_DROPOUT, HP_BATCH_SIZE, HP_LEAKY_RELU],\n",
        "    metrics=[hp.Metric(METRIC_ACCURACY, display_name='Accuracy')],\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03xD1lTqltN2"
      },
      "source": [
        "### Create dataset from generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1rrgaWsfl4Op"
      },
      "outputs": [],
      "source": [
        "def generator_pair(dataset):\n",
        "  for index in dataset:\n",
        "    yield {\"seq1\": seq_tensor[seq_index1[index]], \"seq2\": seq_tensor[seq_index2[index]]}, class_labels[index]\n",
        "\n",
        "def generator_pair_predict(dataset):\n",
        "  for index in dataset:\n",
        "    yield {\"seq1\": seq_tensor[seq_index1[index]], \"seq2\": seq_tensor[seq_index2[index]]}\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KgZZ2JvoBLw"
      },
      "source": [
        "### Split the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1DkEaQYum2O"
      },
      "outputs": [],
      "source": [
        "### k-fold cross-validation\n",
        "from sklearn.model_selection import KFold, ShuffleSplit\n",
        "from sklearn.model_selection import train_test_split\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "tries = 5\n",
        "cur = 0\n",
        "recalls = []\n",
        "accuracy = []\n",
        "total = []\n",
        "total_truth = []\n",
        "train_test = []\n",
        "for train, test in kf.split(class_labels):\n",
        "    # redundant because same position\n",
        "    if np.sum(class_labels[train], 0)[0] > 0.8 * len(train) or np.sum(class_labels[train], 0)[0] < 0.2 * len(train):\n",
        "        continue\n",
        "    train_test.append((train, test))\n",
        "    cur += 1\n",
        "    if cur >= tries:\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G34oBACWLqbw"
      },
      "source": [
        "### Define callbacks for monitor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VQdf1RAsYVcA"
      },
      "outputs": [],
      "source": [
        "### Define tensorboard callback to optimize resource using of model\n",
        "logs = \"logs/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "tboard_callback = tf.keras.callbacks.TensorBoard(log_dir = logs,\n",
        "                                                 histogram_freq = 1,\n",
        "                                                 profile_batch = '20, 29')\n",
        "\n",
        "### Learning rate schedule for optimization during training\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
        "    monitor=\"val_accuracy\",\n",
        "    factor=0.5,\n",
        "    patience=10,\n",
        "    verbose=0,\n",
        "    mode=\"auto\",\n",
        "    min_delta=1e-2,\n",
        "    min_lr=1e-4)\n",
        "\n",
        "# Schedule early stopping\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_accuracy', \n",
        "    verbose=1,\n",
        "    patience=30,\n",
        "    mode='max',\n",
        "    restore_best_weights=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLqkyCvrruF-"
      },
      "source": [
        "### Define performance metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Um2gRMVrZPi"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow-addons\n",
        "import tensorflow_addons as tfa\n",
        "METRICS = [\n",
        "      # keras.metrics.Accuracy(name='accuracy'),\n",
        "      # keras.metrics.TruePositives(name='tp'),\n",
        "      # keras.metrics.FalsePositives(name='fp'),\n",
        "      # keras.metrics.TrueNegatives(name='tn'),\n",
        "      # keras.metrics.FalseNegatives(name='fn'), \n",
        "      keras.metrics.BinaryAccuracy(name='accuracy'),\n",
        "      keras.metrics.Precision(name='precision'),\n",
        "      keras.metrics.Recall(name='recall'),\n",
        "      tfa.metrics.MatthewsCorrelationCoefficient(num_classes=2, name='mcc'),\n",
        "      tfa.metrics.F1Score(num_classes=2, threshold=0.5, name='f1-score'),\n",
        "      keras.metrics.AUC(name='auc'),\n",
        "      keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
        "]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRvlZNzVipPt"
      },
      "source": [
        "### Original architecture PIPR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QnH_6e2tHjTg"
      },
      "outputs": [],
      "source": [
        "def build_model(hparams):\n",
        "    # Input of sequence tensor representations \n",
        "    seq_input1 = Input(shape=(seq_size, dim), name='seq1')\n",
        "    seq_input2 = Input(shape=(seq_size, dim), name='seq2')\n",
        "\n",
        "    # Define Conv1D and Bi-RNN (GRU/LSTM) use in architecture\n",
        "    l1=Conv1D(hparams[HP_CONV_HIDDEN_DIM], hparams[HP_KERNEL_SIZE], activation=hparams[HP_ACTIVATION_CONV], padding=hparams[HP_CONV_PADDING])\n",
        "    r1=Bidirectional(LSTM(hparams[HP_RNN_HIDDEN_DIM], return_sequences=True))\n",
        "    l2=Conv1D(hparams[HP_CONV_HIDDEN_DIM], hparams[HP_KERNEL_SIZE], activation=hparams[HP_ACTIVATION_CONV], padding=hparams[HP_CONV_PADDING])\n",
        "    r2=Bidirectional(LSTM(hparams[HP_RNN_HIDDEN_DIM], return_sequences=True))\n",
        "    l3=Conv1D(hparams[HP_CONV_HIDDEN_DIM], hparams[HP_KERNEL_SIZE], activation=hparams[HP_ACTIVATION_CONV], padding=hparams[HP_CONV_PADDING])\n",
        "    r3=Bidirectional(LSTM(hparams[HP_RNN_HIDDEN_DIM], return_sequences=True))\n",
        "    l4=Conv1D(hparams[HP_CONV_HIDDEN_DIM], hparams[HP_KERNEL_SIZE], activation=hparams[HP_ACTIVATION_CONV], padding=hparams[HP_CONV_PADDING])\n",
        "    r4=Bidirectional(LSTM(hparams[HP_RNN_HIDDEN_DIM], return_sequences=True))\n",
        "    l5=Conv1D(hparams[HP_CONV_HIDDEN_DIM], hparams[HP_KERNEL_SIZE], activation=hparams[HP_ACTIVATION_CONV], padding=hparams[HP_CONV_PADDING])\n",
        "    r5=Bidirectional(LSTM(hparams[HP_RNN_HIDDEN_DIM], return_sequences=True))\n",
        "    l6=Conv1D(hparams[HP_CONV_HIDDEN_DIM], hparams[HP_KERNEL_SIZE], activation=hparams[HP_ACTIVATION_CONV], padding=hparams[HP_CONV_PADDING])\n",
        "    \n",
        "    # Siamese architecture\n",
        "\n",
        "    ### 1st sibling\n",
        "\n",
        "    # 1st Block RCNN \n",
        "    s1=MaxPooling1D(hparams[HP_POOLING_KERNEL])(l1(seq_input1))\n",
        "    s1=concatenate([r1(s1), s1])\n",
        "\n",
        "    # 2nd Block RCNN\n",
        "    s1=MaxPooling1D(hparams[HP_POOLING_KERNEL])(l2(s1))\n",
        "    s1=concatenate([r2(s1), s1])\n",
        "\n",
        "    # 3rd Block RCNN\n",
        "    s1=MaxPooling1D(hparams[HP_POOLING_KERNEL])(l3(s1))\n",
        "    s1=concatenate([r3(s1), s1])\n",
        "\n",
        "    # 4th Block RCNN \n",
        "    s1=MaxPooling1D(hparams[HP_POOLING_KERNEL])(l4(s1))\n",
        "    s1=concatenate([r4(s1), s1])\n",
        "\n",
        "    # 5th Block RCNN\n",
        "    s1=MaxPooling1D(hparams[HP_POOLING_KERNEL])(l5(s1))\n",
        "    s1=concatenate([r5(s1), s1])\n",
        "    \n",
        "    # Last convolution\n",
        "    s1=l6(s1)\n",
        "    s1=GlobalAveragePooling1D()(s1)\n",
        "\n",
        "    ### 2nd sibling\n",
        "\n",
        "    # 1st block RCNN\n",
        "    s2=MaxPooling1D(hparams[HP_POOLING_KERNEL])(l1(seq_input2))\n",
        "    s2=concatenate([r1(s2), s2])\n",
        "\n",
        "    # 2nd block RCNN\n",
        "    s2=MaxPooling1D(hparams[HP_POOLING_KERNEL])(l2(s2))\n",
        "    s2=concatenate([r2(s2), s2])\n",
        "\n",
        "    # 3rd block RCNN\n",
        "    s2=MaxPooling1D(hparams[HP_POOLING_KERNEL])(l3(s2))\n",
        "    s2=concatenate([r3(s2), s2])\n",
        "\n",
        "    # 4th block RCNN\n",
        "    s2=MaxPooling1D(hparams[HP_POOLING_KERNEL])(l4(s2))\n",
        "    s2=concatenate([r4(s2), s2])\n",
        "\n",
        "    # 5th block RCNN\n",
        "    s2=MaxPooling1D(hparams[HP_POOLING_KERNEL])(l5(s2))\n",
        "    s2=concatenate([r5(s2), s2])\n",
        "\n",
        "    # Last convolution\n",
        "    s2=l6(s2)\n",
        "    s2=GlobalAveragePooling1D()(s2)\n",
        "\n",
        "    ### Combine two siblings of siamese architecture\n",
        "    merge_text = multiply([s1, s2])\n",
        "    \n",
        "\n",
        "    #### MLP Part\n",
        "    # Set initializer\n",
        "    he_initializer = tf.keras.initializers.HeUniform()\n",
        "    \n",
        "    # First dense\n",
        "    x = Dense(hparams[HP_FIRST_DENSE], activation=hparams[HP_ACTIVATION])(merge_text)\n",
        "    # x = tf.keras.layers.LeakyReLU(alpha=.3)(x)\n",
        "    x = Dropout(hparams[HP_DROPOUT])(x)\n",
        "\n",
        "    # Second dense\n",
        "    x = Dense(int((hparams[HP_CONV_HIDDEN_DIM]+7)/2), activation=hparams[HP_ACTIVATION])(x)\n",
        "    # x = tf.keras.layers.LeakyReLU(alpha=.3)(x)\n",
        "    x = Dropout(hparams[HP_DROPOUT])(x)\n",
        "\n",
        "    # Last softmax\n",
        "    main_output = Dense(2, activation='softmax')(x)\n",
        "\n",
        "    # Combine to form functional model\n",
        "    merge_model = Model(inputs=[seq_input1, seq_input2], outputs=[main_output])\n",
        "    return merge_model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(hparams):\n",
        "    # Input of sequence tensor representations \n",
        "    seq_input1 = Input(shape=(seq_size, dim), name='seq1')\n",
        "    seq_input2 = Input(shape=(seq_size, dim), name='seq2')\n",
        "\n",
        "    merge = concatenate([seq_input1, seq_input2])\n",
        "    x = GlobalAveragePooling1D()(merge)\n",
        "    main_output = Dense(2, activation='softmax')(x)\n",
        "    merge_model = Model(inputs=[seq_input1, seq_input2], outputs=[main_output])\n",
        "    \n",
        "    return merge_model"
      ],
      "metadata": {
        "id": "L7-0Lw2Urkp7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkG4si5QSf15"
      },
      "source": [
        "### Summary of model architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0wMXJo9lqd6l"
      },
      "outputs": [],
      "source": [
        "hparams = {\n",
        "  HP_EPSILON: EPSILON,\n",
        "  HP_LEARNING_RATE: LEARNING_RATE,\n",
        "  HP_FIRST_DENSE: 100,\n",
        "  HP_KERNEL_SIZE: 3,\n",
        "  HP_POOLING_KERNEL: 3,\n",
        "  HP_CONV_HIDDEN_DIM: 50,\n",
        "  HP_RNN_HIDDEN_DIM: 50,\n",
        "  HP_ACTIVATION: 'leaky_relu',\n",
        "  HP_ACTIVATION_CONV: 'relu',\n",
        "  HP_REGULARIZER: 0,\n",
        "  HP_CONV_PADDING: 'valid',\n",
        "  HP_DROPOUT: 3e-1,\n",
        "  HP_BATCH_SIZE: 256,\n",
        "  HP_LEAKY_RELU: 3e-1\n",
        "}\n",
        "\n",
        "model = build_model(hparams)\n",
        "tf.keras.utils.plot_model(model, to_file='model.png', show_shapes=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LjrzFALNxTZ"
      },
      "source": [
        "### Config Train-test process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DObbKWD68hGR"
      },
      "outputs": [],
      "source": [
        "def train_test_model(hparams):\n",
        "  training_time = 1\n",
        "  num_hit = 0.\n",
        "  num_total = 0.\n",
        "  num_pos = 0.\n",
        "  num_true_pos = 0.\n",
        "  num_false_pos = 0.\n",
        "  num_true_neg = 0.\n",
        "  num_false_neg = 0.\n",
        "  for train, test in train_test:\n",
        "      merge_model = None\n",
        "      merge_model = build_model(hparams)  \n",
        "\n",
        "\n",
        "      merge_model.compile(optimizer=Adam(learning_rate=hparams[HP_LEARNING_RATE], amsgrad=True, epsilon=hparams[HP_EPSILON]), loss='categorical_crossentropy', metrics=METRICS)\n",
        "      # Create train\n",
        "      train_dataset = tf.data.Dataset.from_generator(generator_pair, args=[train], output_types=({\"seq1\": DTYPE, \"seq2\": DTYPE}, DTYPE), output_shapes = ({\"seq1\": (seq_size, dim), \"seq2\": (seq_size, dim)}, (2,)) )\n",
        "      train_dataset = train_dataset.shuffle(1024).repeat(N_EPOCHS).batch(hparams[HP_BATCH_SIZE])\n",
        "      train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "\n",
        "      # Create val\n",
        "      val_dataset = tf.data.Dataset.from_generator(generator_pair, args=[test], output_types=({\"seq1\": DTYPE, \"seq2\": DTYPE}, DTYPE), output_shapes = ({\"seq1\": (seq_size, dim), \"seq2\": (seq_size, dim)}, (2,)) )\n",
        "      val_dataset = val_dataset.batch(hparams[HP_BATCH_SIZE])\n",
        "      val_dataset = val_dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "      # Save the best model base on val_accuracy\n",
        "      checkpoint = ModelCheckpoint(filepath='my_best_model.hdf5', monitor='val_accuracy',verbose=1, save_best_only=True, mode='max')\n",
        "      # Fit model\n",
        "      print(f'==================== Training time  {training_time} =====================')\n",
        "      merge_model.fit(train_dataset, steps_per_epoch=len(train)//hparams[HP_BATCH_SIZE], epochs=N_EPOCHS, validation_data=val_dataset, callbacks=[early_stopping, checkpoint, reduce_lr])\n",
        "      \n",
        "      print(f'==================End training {training_time}========================')\n",
        "      # # Create test\n",
        "      # test_dataset = tf.data.Dataset.from_generator(generator_pair, args=[test], output_types=({\"seq1\": DTYPE, \"seq2\": DTYPE}, DTYPE), output_shapes = ({\"seq1\": (seq_size, dim), \"seq2\": (seq_size, dim)}, (2,)) )\n",
        "      # test_dataset = test_dataset.batch(hparams[HP_BATCH_SIZE])\n",
        "      # res = merge_model.evaluate(test_dataset)\n",
        "      # Create pred\n",
        "      pred_dataset = tf.data.Dataset.from_generator(generator_pair_predict, args=[test], output_types=({\"seq1\": DTYPE, \"seq2\": DTYPE}), output_shapes = ({\"seq1\": (seq_size, dim), \"seq2\": (seq_size, dim)}) )\n",
        "      pred_dataset = pred_dataset.batch(BATCH_SIZE)\n",
        "      pred_dataset = pred_dataset.prefetch(tf.data.AUTOTUNE)\n",
        "      pred = merge_model.predict(pred_dataset)\n",
        "\n",
        "      # Performance metrics\n",
        "      for i in range(len(class_labels[test])):\n",
        "          num_total += 1\n",
        "          if np.argmax(class_labels[test][i]) == np.argmax(pred[i]):\n",
        "              num_hit += 1\n",
        "          if class_labels[test][i][0] > 0.:\n",
        "              num_pos += 1.\n",
        "              if pred[i][0] > pred[i][1]:\n",
        "                  num_true_pos += 1\n",
        "              else:\n",
        "                  num_false_neg += 1\n",
        "          else:\n",
        "              if pred[i][0] > pred[i][1]:\n",
        "                  num_false_pos += 1\n",
        "              else:\n",
        "                  num_true_neg += 1\n",
        "      accuracy = num_hit / num_total\n",
        "      prec = num_true_pos / (num_true_pos + num_false_pos)\n",
        "      recall = num_true_pos / num_pos\n",
        "      spec = num_true_neg / (num_true_neg + num_false_neg)\n",
        "      f1 = 2. * prec * recall / (prec + recall)\n",
        "      mcc = (num_true_pos * num_true_neg - num_false_pos * num_false_neg) / ((num_true_pos + num_true_neg) * (num_true_pos + num_false_neg) * (num_false_pos + num_true_neg) * (num_false_pos + num_false_neg)) ** 0.5\n",
        "      training_time += 1\n",
        "      print (f'accuracy: {accuracy}, precision: {prec}, recall: {recall}, specificity: {spec}, mcc: {mcc} ,f1-score: {f1}')\n",
        "  return accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hUrIncXN1vp"
      },
      "source": [
        "### Log configurations and accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Frknl0rEDtm"
      },
      "outputs": [],
      "source": [
        "def run(run_dir, hparams):\n",
        "  with tf.summary.create_file_writer(run_dir).as_default():\n",
        "    hp.hparams(hparams)  # record the values used in this trial\n",
        "    accuracy = train_test_model(hparams)\n",
        "    tf.summary.scalar(METRIC_ACCURACY, accuracy, step=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ONZlF6POBUX"
      },
      "source": [
        "### Loop over all configurations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "63YbvCPBGBNB"
      },
      "outputs": [],
      "source": [
        "# Remove log directory from last run\n",
        "%%time\n",
        "!rm -rf ./logs\n",
        "session_num = 0\n",
        "for epsilon in HP_EPSILON.domain.values:\n",
        "  for learning_rate in HP_LEARNING_RATE.domain.values:\n",
        "    for first_dense in HP_FIRST_DENSE.domain.values:\n",
        "      for kernel_size in HP_KERNEL_SIZE.domain.values:\n",
        "        for pooling_kernel in HP_POOLING_KERNEL.domain.values:\n",
        "          for conv_hidden_dim in HP_CONV_HIDDEN_DIM.domain.values:\n",
        "            for rnn_hidden_dim in HP_RNN_HIDDEN_DIM.domain.values:\n",
        "              for activation in HP_ACTIVATION.domain.values:\n",
        "                for activation_conv in HP_ACTIVATION_CONV.domain.values:\n",
        "                  for regularizer in HP_REGULARIZER.domain.values:\n",
        "                    for conv_padding in HP_CONV_PADDING.domain.values:\n",
        "                      for dropout in HP_DROPOUT.domain.values:\n",
        "                        for batch_size in HP_BATCH_SIZE.domain.values:\n",
        "                          for leaky_relu in HP_LEAKY_RELU.domain.values:\n",
        "                            hparams = {\n",
        "                                HP_EPSILON: epsilon,\n",
        "                                HP_LEARNING_RATE: learning_rate,\n",
        "                                HP_FIRST_DENSE: first_dense,\n",
        "                                HP_KERNEL_SIZE: kernel_size,\n",
        "                                HP_POOLING_KERNEL: pooling_kernel,\n",
        "                                HP_CONV_HIDDEN_DIM: conv_hidden_dim,\n",
        "                                HP_RNN_HIDDEN_DIM: rnn_hidden_dim,\n",
        "                                HP_ACTIVATION: activation,\n",
        "                                HP_ACTIVATION_CONV: activation_conv,\n",
        "                                HP_REGULARIZER: regularizer,\n",
        "                                HP_CONV_PADDING: conv_padding,\n",
        "                                HP_DROPOUT: dropout,\n",
        "                                HP_BATCH_SIZE: batch_size,\n",
        "                                HP_LEAKY_RELU: leaky_relu\n",
        "                            }\n",
        "                            run_name = \"run-%d\" % session_num\n",
        "                            print('--- Starting trial: %s' % run_name)\n",
        "                            print({h.name: hparams[h] for h in hparams})\n",
        "                            run('logs/hparam_tuning/' + run_name, hparams)\n",
        "                            session_num += 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhjyFk1XOHNJ"
      },
      "source": [
        "### Tensorboard monitor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sjolcDW-w5jY"
      },
      "outputs": [],
      "source": [
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir=/content/logs"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "tunning-dataset-customized.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}