{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anhvt00/PIPR/blob/master/models/PIPR_LGBM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3tRl6ZSk8Oi"
      },
      "source": [
        "This notebook use for tunning model using embeddings file and language model embedder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wz47D5H_R0UR"
      },
      "source": [
        "### Check GPU hardware"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8kCH-Zfj2J_",
        "outputId": "d3ea8fe9-661c-4052-d10f-2ac7936c4a34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Feb  4 03:45:06 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-chmuAHjK8D0"
      },
      "source": [
        "### Download embedding files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SrKgkXJA3wPb",
        "outputId": "b5e2737a-65cb-4fe8-f2fa-c07d6d1204d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-02-04 03:45:07--  https://raw.githubusercontent.com/anhvt00/PIPR/master/embeddings/seq2tensor.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1104 (1.1K) [text/plain]\n",
            "Saving to: ‘seq2tensor.py’\n",
            "\n",
            "seq2tensor.py       100%[===================>]   1.08K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-02-04 03:45:07 (54.4 MB/s) - ‘seq2tensor.py’ saved [1104/1104]\n",
            "\n",
            "--2022-02-04 03:45:07--  https://raw.githubusercontent.com/anhvt00/PIPR/master/embeddings/ac5_aph.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2335 (2.3K) [text/plain]\n",
            "Saving to: ‘ac5_aph.txt’\n",
            "\n",
            "ac5_aph.txt         100%[===================>]   2.28K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-02-04 03:45:07 (28.9 MB/s) - ‘ac5_aph.txt’ saved [2335/2335]\n",
            "\n",
            "--2022-02-04 03:45:07--  https://raw.githubusercontent.com/anhvt00/PIPR/master/data/Golden-tunning-datasets/S.cerevisae/yeast_pairs.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 179008 (175K) [text/plain]\n",
            "Saving to: ‘yeast_pairs.tsv’\n",
            "\n",
            "yeast_pairs.tsv     100%[===================>] 174.81K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2022-02-04 03:45:07 (14.0 MB/s) - ‘yeast_pairs.tsv’ saved [179008/179008]\n",
            "\n",
            "--2022-02-04 03:45:08--  https://raw.githubusercontent.com/anhvt00/PIPR/master/data/Golden-tunning-datasets/H.pylori/hp_pairs.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 30791 (30K) [text/plain]\n",
            "Saving to: ‘hp_pairs.tsv’\n",
            "\n",
            "hp_pairs.tsv        100%[===================>]  30.07K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2022-02-04 03:45:08 (30.1 MB/s) - ‘hp_pairs.tsv’ saved [30791/30791]\n",
            "\n",
            "--2022-02-04 03:45:08--  https://raw.githubusercontent.com/anhvt00/PIPR/master/data/Golden-tunning-datasets/H.pylori/hp_dict.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 505633 (494K) [text/plain]\n",
            "Saving to: ‘hp_dict.tsv’\n",
            "\n",
            "hp_dict.tsv         100%[===================>] 493.78K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2022-02-04 03:45:08 (20.6 MB/s) - ‘hp_dict.tsv’ saved [505633/505633]\n",
            "\n",
            "--2022-02-04 03:45:08--  https://raw.githubusercontent.com/anhvt00/PIPR/master/data/Golden-tunning-datasets/S.cerevisae/yeast_dictionary.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1415079 (1.3M) [text/plain]\n",
            "Saving to: ‘yeast_dictionary.tsv’\n",
            "\n",
            "yeast_dictionary.ts 100%[===================>]   1.35M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2022-02-04 03:45:09 (41.9 MB/s) - ‘yeast_dictionary.tsv’ saved [1415079/1415079]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Download file seq2tensor.py for converting protein sequences to tensors\n",
        "!wget https://raw.githubusercontent.com/anhvt00/PIPR/master/embeddings/seq2tensor.py\n",
        "\n",
        "# Download file ac5_aph.txt for ac5_aph embedding \n",
        "!wget https://raw.githubusercontent.com/anhvt00/PIPR/master/embeddings/ac5_aph.txt\n",
        "\n",
        "### Download interaction pairs and dictionary files\n",
        "# Download dictionary file (id: sequence)\n",
        "# Download dictionary file (id: sequence)\n",
        "# !wget https://raw.githubusercontent.com/anhvt00/PIPR/master/data/Tunning-architecture-dataset/yeast_pairs.tsv\n",
        "!wget https://raw.githubusercontent.com/anhvt00/PIPR/master/data/Golden-tunning-datasets/S.cerevisae/yeast_pairs.tsv\n",
        "!wget https://raw.githubusercontent.com/anhvt00/PIPR/master/data/Golden-tunning-datasets/H.pylori/hp_pairs.tsv\n",
        "!wget https://raw.githubusercontent.com/anhvt00/PIPR/master/data/Golden-tunning-datasets/H.pylori/hp_dict.tsv\n",
        "# Download pairs of proteins with labels file\n",
        "# !wget https://raw.githubusercontent.com/anhvt00/PIPR/master/data/Tunning-architecture-dataset/yeast_dictionary.tsv\n",
        "!wget https://raw.githubusercontent.com/anhvt00/PIPR/master/data/Golden-tunning-datasets/S.cerevisae/yeast_dictionary.tsv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiMnInVvlEjY"
      },
      "source": [
        "### Import libraries "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FBCZs6wgdV7E",
        "outputId": "7c4970fd-ccd5-412a-9822-07115a1f9930"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorboard_plugin_profile\n",
            "  Downloading tensorboard_plugin_profile-2.5.0-py3-none-any.whl (1.1 MB)\n",
            "\u001b[?25l\r\u001b[K     |▎                               | 10 kB 20.4 MB/s eta 0:00:01\r\u001b[K     |▋                               | 20 kB 20.4 MB/s eta 0:00:01\r\u001b[K     |▉                               | 30 kB 15.3 MB/s eta 0:00:01\r\u001b[K     |█▏                              | 40 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 51 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 61 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |██                              | 71 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 81 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 92 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |███                             | 102 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 112 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 122 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 133 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |████                            | 143 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 153 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 163 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████                           | 174 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 184 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 194 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 204 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 215 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 225 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 235 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████                         | 245 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 256 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 266 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████                        | 276 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 286 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 296 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 307 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 317 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 327 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 337 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 348 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 358 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 368 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 378 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 389 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 399 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 409 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 419 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 430 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 440 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 450 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 460 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 471 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 481 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 491 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 501 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 512 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 522 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 532 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 542 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 552 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 563 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 573 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 583 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 593 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 604 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 614 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 624 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 634 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 645 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 655 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 665 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 675 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 686 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 696 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 706 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 716 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 727 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 737 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 747 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 757 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 768 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 778 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 788 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 798 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 808 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 819 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 829 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 839 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 849 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 860 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 870 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 880 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 890 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 901 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 911 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 921 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 931 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 942 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 952 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 962 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 972 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 983 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 993 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 1.0 MB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.0 MB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.0 MB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 1.0 MB 7.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.0 MB 7.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.1 MB 7.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.1 MB 7.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.1 MB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.1 MB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.1 MB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.1 MB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.1 MB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.1 MB 7.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard_plugin_profile) (3.17.3)\n",
            "Collecting gviz-api>=1.9.0\n",
            "  Downloading gviz_api-1.10.0-py2.py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard_plugin_profile) (1.0.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard_plugin_profile) (1.15.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard_plugin_profile) (57.4.0)\n",
            "Installing collected packages: gviz-api, tensorboard-plugin-profile\n",
            "Successfully installed gviz-api-1.10.0 tensorboard-plugin-profile-2.5.0\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.7/dist-packages (2.2.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from lightgbm) (1.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from lightgbm) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from lightgbm) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->lightgbm) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->lightgbm) (3.0.0)\n"
          ]
        }
      ],
      "source": [
        "# Libraries for system and debug\n",
        "import sys\n",
        "import pdb\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# Class for converting sequences to tensors\n",
        "from seq2tensor import s2t\n",
        "\n",
        "\n",
        "# Libraries for neural network training\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, GRU, LSTM, Bidirectional, Input, Conv1D, Conv2D\n",
        "from tensorflow.keras.layers import Add, Flatten, subtract, multiply, concatenate\n",
        "from tensorflow.keras.layers import MaxPooling1D, AveragePooling1D, GlobalAveragePooling1D, MaxPooling2D\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
        "from tensorflow.keras.layers import Dropout, BatchNormalization\n",
        "from tensorflow.keras.utils import Sequence\n",
        "from tensorflow.keras import mixed_precision\n",
        "from tensorflow import keras\n",
        "from tensorboard.plugins.hparams import api as hp\n",
        "from tensorflow.keras.utils import get_custom_objects\n",
        "from tensorflow.keras.layers import Activation\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn.metrics as metrics\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import matthews_corrcoef,accuracy_score, precision_score,recall_score\n",
        "from sklearn.manifold import TSNE\n",
        "# Import accessory modules\n",
        "import numpy as np\n",
        "import h5py\n",
        "import gc\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "import pandas as pd\n",
        "\n",
        "# For tensorboard extension\n",
        "!pip install -U tensorboard_plugin_profile\n",
        "\n",
        "\n",
        "!pip install lightgbm\n",
        "from lightgbm import LGBMClassifier\n",
        "from xgboost import XGBClassifier\n",
        "import lightgbm as lgb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjxFKiABLDob"
      },
      "source": [
        "### Set CUDA environment variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjvCG7HCp0Af",
        "outputId": "f9168192-68e9-4ff8-e661-11b3bab61656"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 Physical GPUs, 1 Logical GPUs\n"
          ]
        }
      ],
      "source": [
        "### Setting RAM GPU for training growth \n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "  try:\n",
        "    # Currently, memory growth needs to be the same across GPUs\n",
        "    for gpu in gpus:\n",
        "      tf.config.experimental.set_memory_growth(gpu, True)\n",
        "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
        "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
        "  except RuntimeError as e:\n",
        "    # Memory growth must be set before GPUs have been initialized\n",
        "    print(e)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ============================================\n",
        "# Optimisation Flags - Do not remove\n",
        "# ============================================\n",
        "\n",
        "# Disables caching (when set to 1) or enables caching (when set to 0) for just-in-time-compilation. When disabled,\n",
        "# no binary code is added to or retrieved from the cache.\n",
        "os.environ['CUDA_CACHE_DISABLE'] = '0' # orig is 0\n",
        "\n",
        "# When set to 1, forces the device driver to ignore any binary code embedded in an application \n",
        "# (see Application Compatibility) and to just-in-time compile embedded PTX code instead.\n",
        "# If a kernel does not have embedded PTX code, it will fail to load. This environment variable can be used to\n",
        "# validate that PTX code is embedded in an application and that its just-in-time compilation works as expected to guarantee application \n",
        "# forward compatibility with future architectures.\n",
        "os.environ['CUDA_FORCE_PTX_JIT'] = '1'# no orig\n",
        "\n",
        "\n",
        "os.environ['HOROVOD_GPU_ALLREDUCE'] = 'NCCL'\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "\n",
        "os.environ['TF_GPU_THREAD_MODE'] = 'gpu_private'\n",
        "os.environ['TF_GPU_THREAD_COUNT']='1'\n",
        "\n",
        "os.environ['TF_USE_CUDNN_BATCHNORM_SPATIAL_PERSISTENT'] = '1'\n",
        "\n",
        "os.environ['TF_ADJUST_HUE_FUSED'] = '1'\n",
        "os.environ['TF_ADJUST_SATURATION_FUSED'] = '1'\n",
        "os.environ['TF_ENABLE_WINOGRAD_NONFUSED'] = '1'\n",
        "\n",
        "os.environ['TF_SYNC_ON_FINISH'] = '0'\n",
        "os.environ['TF_AUTOTUNE_THRESHOLD'] = '2'\n",
        "os.environ['TF_DISABLE_NVTX_RANGES'] = '1'\n",
        "os.environ[\"TF_ENABLE_AUTO_MIXED_PRECISION_GRAPH_REWRITE\"] = \"1\"\n",
        "\n",
        "\n",
        "\n",
        "# =================================================\n",
        "# mixed_precision.set_global_policy('mixed_float16')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYxewWysW5U5"
      },
      "source": [
        "### Hyperparameter set by default\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "TGt47faMW5U6"
      },
      "outputs": [],
      "source": [
        "# Default hyperparameters\n",
        "CONV_HIDDEN_DIM = 50\n",
        "RNN_HIDDEN = 50\n",
        "N_EPOCHS = 80\n",
        "HIDDEN_DIM=50\n",
        "BATCH_SIZE = 32\n",
        "DTYPE='float32'\n",
        "LEARNING_RATE=.001\n",
        "EPSILON=1e-6\n",
        "adam = Adam(learning_rate=LEARNING_RATE, amsgrad=True, epsilon=EPSILON)\n",
        "MAX_DATASET_SIZE = 11187\n",
        "DATASET_SIZE = MAX_DATASET_SIZE\n",
        "KERNEL_SIZE = 3\n",
        "POOLING_KERNEL = 3\n",
        "seq_size=3500\n",
        "dim = 1024\n",
        "# 1 for language model embedding\n",
        "flags_embedding = 0\n",
        "# 1 for loading from drive\n",
        "available_data = 0\n",
        "augment_data = True\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjTWZj-Bzebi"
      },
      "source": [
        "### Load the embeddings from drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "uHFMvEvzzd_3"
      },
      "outputs": [],
      "source": [
        "if available_data == 1:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    seq_tensor = np.load('/content/drive/MyDrive/seq_tensor.npy', allow_pickle=True)\n",
        "    # Load contextual embeddings here\n",
        "    # seq_tensor = np.load('/content/drive/MyDrive/Embeddings_Guo/15_amino_Word2VecEmbedder.npy', allow_pickle=True)\n",
        "\n",
        "    class_labels = np.load('/content/drive/MyDrive/class_labels.npy', allow_pickle=True)\n",
        "    seq_index1 = np.load('/content/drive/MyDrive/seq_index1.npy', allow_pickle=True)\n",
        "    seq_index2 = np.load('/content/drive/MyDrive/seq_index2.npy', allow_pickle=True)\n",
        "    # seq_tensor_physicochemical= np.load('/content/drive/MyDrive/physicochemical.npy', allow_pickle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kiTiIFTn8Uyz"
      },
      "source": [
        "### Conjoint Triad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "doTa8fDt_B3C"
      },
      "outputs": [],
      "source": [
        "# # https://sci-hub.se/https://doi.org/10.3390/ijms18112373\n",
        "# # After that, the conjoint triad method is introduced to extract the sequence information, which includes the properties of one amino acid and its vicinal amino acids and regards any \n",
        "# # three continuous amino acids as a unit. Firstly, they replaced each amino acid in the protein sequence by the index depending on its grouping. For instance, protein sequence\n",
        "# # “VCCPPVCVVCPPVCVPVPPCCV” is replaced by 0112201001220102022110. Then, binary space (V, F) stands for a protein sequence. Here, V is the vector sp… acids grouped into seven classes, the size V \n",
        "# # should be 7 × 7 × 7; therefore, i = 0, 1, · · · , 342. The detailed definition and description is shown in Figure 4. Clearly, each protein has a corresponding F vector. Nevertheless, the value\n",
        "# # of fi relates to the length of amino acid sequence. A longer amino acid sequence generally have a larger value of fi, which complicates the comparison between two heterogeneous proteins. \n",
        "# # As such they employed the normalization to solve this problem as follows: di = (fi - min fi) / max fi for i = 1 to 343, where the value of di is normalized in the range [0, 1]. fi is the\n",
        "# # frequency of conjoint triad unit vi appearing in the protein sequence. Finally, they connected the vector spaces of two proteins to present the interaction features. Thus, a 686-dimensional \n",
        "# # vector (343 for each protein) is generated for each pair of proteins.\n",
        "\n",
        "# # original paper: https://www.pnas.org/content/104/11/4337 (shen et al., 2007)\n",
        "# ct_encoding = {\n",
        "#     \"A\": 0,\n",
        "#     \"G\": 0,\n",
        "#     \"V\": 0,\n",
        "#     \"C\": 1,\n",
        "#     \"F\": 2,\n",
        "#     \"I\": 2,\n",
        "#     \"L\": 2,\n",
        "#     \"P\": 2,\n",
        "#     \"M\": 3,\n",
        "#     \"S\": 3,\n",
        "#     \"T\": 3,\n",
        "#     \"Y\": 3,\n",
        "#     \"H\": 4,\n",
        "#     \"N\": 4,\n",
        "#     \"Q\": 4,\n",
        "#     \"W\": 4,\n",
        "#     \"K\": 5,\n",
        "#     \"R\": 5,\n",
        "#     \"D\": 6,\n",
        "#     \"E\": 6,\n",
        "# }\n",
        "\n",
        "# f = open('encode_ct.txt', 'w')\n",
        "# for aa, value in ct_encoding.items():\n",
        "#   f.write(str(aa)+'\\t'+str(value)+'\\n')\n",
        "\n",
        "# f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1QbOrTYesLt",
        "outputId": "d1212feb-4042-41b5-e941-534f438fa728"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4 0.125\n",
            "13 0.125\n",
            "23 0.125\n",
            "71 0.125\n",
            "89 0.125\n",
            "149 0.125\n",
            "158 0.125\n",
            "276 0.125\n"
          ]
        }
      ],
      "source": [
        "\"\"\"Implementation of CT coding method\n",
        "\"\"\"\n",
        "\n",
        "__all__ = ['ct_code_of']\n",
        "\n",
        "# AAC: Classification of amino acids.\n",
        "AAC = {\n",
        "    '1': ['A', 'G', 'V'],\n",
        "    '2': ['I', 'L', 'F', 'P'],\n",
        "    '3': ['Y', 'M', 'T', 'S'],\n",
        "    '4': ['H', 'N', 'Q', 'W'],\n",
        "    '5': ['R', 'K'],\n",
        "    '6': ['D', 'E'],\n",
        "    '7': ['C']\n",
        "}\n",
        "\n",
        "# AAC_R: Reverse of AAC.\n",
        "AAC_R = {}\n",
        "for C, AAS in AAC.items():\n",
        "    for AA in AAS:\n",
        "        AAC_R[AA] = C\n",
        "\n",
        "def classification_of(AA):\n",
        "    \"\"\"Get classification of amino acids.\"\"\"\n",
        "    return AAC_R[AA]\n",
        "\n",
        "def classification_sequence_of(PS):\n",
        "    \"\"\"Make classification sequence from protein sequence.\"\"\"\n",
        "    CS = ''\n",
        "    for I, CH in enumerate(PS):\n",
        "        CS = CS + classification_of(CH)\n",
        "    return CS\n",
        "\n",
        "def ct_code_of(PS):\n",
        "    \"\"\"Get CT Code of protein sequence.\"\"\"\n",
        "    CT_Code = [0]*343\n",
        "    CS = classification_sequence_of(PS)\n",
        "    for I in range(len(CS)-2):\n",
        "        SubCS = CS[I:I+3]\n",
        "        CT_Code_Index = int(SubCS[0]) + (int(SubCS[1])-1)*7 + (int(SubCS[2])-1)*7*7\n",
        "        CT_Code[CT_Code_Index-1] = CT_Code[CT_Code_Index-1] + 1\n",
        "    SUM = sum(CT_Code)\n",
        "    CT_Code = [N*1.0/SUM for N in CT_Code]\n",
        "    # Normalizing CT_Code\n",
        "    # MIN_CODE = min(CT_Code)\n",
        "    # MAX_CODE = max(CT_Code)\n",
        "    # CT_Code = [(N-MIN_CODE)*1.0/(MAX_CODE-MIN_CODE) for N in CT_Code]\n",
        "    return CT_Code\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "    for I, Frequency in enumerate(ct_code_of('MREIVHIQAG')):\n",
        "        if Frequency>0 :\n",
        "            print(I+1, Frequency)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEzG9WEi8g88"
      },
      "source": [
        "### Local Descriptor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39B9qBKQLWxk",
        "outputId": "bae845c0-b72b-4235-a414-edd1ca9d37d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PS= VCCPPVCVVCPPVCVPVPPCCV\n",
            "CS= 1772217117221712122771\n",
            "LD_INFO= (22, {'1': [1, 6, 8, 9, 13, 15, 17, 22], '7': [2, 3, 7, 10, 14, 20, 21], '2': [4, 5, 11, 12, 16, 18, 19]}, {'17': 7, '27': 3, '12': 5})\n",
            "LD_CODE= [0.36363636363636365, 0.3181818181818182, 0, 0, 0, 0, 0.3181818181818182, 0.23809523809523808, 0, 0, 0, 0, 0.3333333333333333, 0, 0, 0, 0, 0.14285714285714285, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.045454545454545456, 0.2727272727272727, 0.4090909090909091, 0.6818181818181818, 1.0, 0.18181818181818182, 0.18181818181818182, 0.5, 0.7272727272727273, 0.8636363636363636, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.09090909090909091, 0.09090909090909091, 0.3181818181818182, 0.6363636363636364, 0.9545454545454546]\n",
            "LD_CODE= [0.2, 0.4, 0, 0, 0, 0, 0.4, 0, 0, 0, 0, 0, 0.25, 0, 0, 0, 0, 0.25, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.2, 0.2, 0.2, 0.2, 0.2, 0.8, 1.0, 0.8, 0.8, 1.0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.4, 0.6, 0.4, 0.4, 0.6, 0.5, 0.16666666666666666, 0, 0, 0, 0, 0.3333333333333333, 0, 0, 0, 0, 0, 0.6, 0, 0, 0, 0, 0.2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.16666666666666666, 0.6666666666666666, 0.16666666666666666, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.3333333333333333, 0.8333333333333334, 0.3333333333333333, 0.3333333333333333, 0.8333333333333334, 0.4, 0.4, 0, 0, 0, 0, 0.2, 0.5, 0, 0, 0, 0, 0.5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.4, 0.8, 0.4, 0.4, 0.8, 0.2, 1.0, 0.2, 0.2, 1.0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.6, 0.6, 0.6, 0.6, 0.6, 0.3333333333333333, 0.3333333333333333, 0, 0, 0, 0, 0.3333333333333333, 0.2, 0, 0, 0, 0, 0.2, 0, 0, 0, 0, 0.2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.16666666666666666, 1.0, 0.16666666666666666, 0.16666666666666666, 1.0, 0.3333333333333333, 0.5, 0.3333333333333333, 0.3333333333333333, 0.5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.6666666666666666, 0.8333333333333334, 0.6666666666666666, 0.6666666666666666, 0.8333333333333334, 0.36363636363636365, 0.2727272727272727, 0, 0, 0, 0, 0.36363636363636365, 0.1, 0, 0, 0, 0, 0.4, 0, 0, 0, 0, 0.2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.09090909090909091, 0.09090909090909091, 0.5454545454545454, 0.7272727272727273, 0.8181818181818182, 0.36363636363636365, 1.0, 0.36363636363636365, 0.45454545454545453, 1.0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.18181818181818182, 0.18181818181818182, 0.2727272727272727, 0.6363636363636364, 0.9090909090909091, 0.36363636363636365, 0.36363636363636365, 0, 0, 0, 0, 0.2727272727272727, 0.4, 0, 0, 0, 0, 0.3, 0, 0, 0, 0, 0.1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.18181818181818182, 0.18181818181818182, 0.36363636363636365, 0.5454545454545454, 1.0, 0.09090909090909091, 0.09090909090909091, 0.45454545454545453, 0.6363636363636364, 0.7272727272727273, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.2727272727272727, 0.9090909090909091, 0.2727272727272727, 0.8181818181818182, 0.9090909090909091, 0.45454545454545453, 0.2727272727272727, 0, 0, 0, 0, 0.2727272727272727, 0.2, 0, 0, 0, 0, 0.5, 0, 0, 0, 0, 0.1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.09090909090909091, 0.09090909090909091, 0.2727272727272727, 0.36363636363636365, 0.9090909090909091, 0.5454545454545454, 1.0, 0.5454545454545454, 0.6363636363636364, 1.0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.18181818181818182, 0.8181818181818182, 0.18181818181818182, 0.45454545454545453, 0.8181818181818182, 0.375, 0.3125, 0, 0, 0, 0, 0.3125, 0.2, 0, 0, 0, 0, 0.4, 0, 0, 0, 0, 0.13333333333333333, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0625, 0.0625, 0.5, 0.5625, 0.9375, 0.25, 0.25, 0.3125, 0.6875, 1.0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.125, 0.125, 0.1875, 0.4375, 0.875, 0.4117647058823529, 0.29411764705882354, 0, 0, 0, 0, 0.29411764705882354, 0.25, 0, 0, 0, 0, 0.375, 0, 0, 0, 0, 0.125, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.058823529411764705, 0.058823529411764705, 0.23529411764705882, 0.5882352941176471, 1.0, 0.35294117647058826, 0.35294117647058826, 0.4117647058823529, 0.6470588235294118, 0.8235294117647058, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.11764705882352941, 0.11764705882352941, 0.29411764705882354, 0.5294117647058824, 0.9411764705882353, 0.35294117647058826, 0.4117647058823529, 0, 0, 0, 0, 0.23529411764705882, 0.3125, 0, 0, 0, 0, 0.3125, 0, 0, 0, 0, 0.125, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.23529411764705882, 0.23529411764705882, 0.4117647058823529, 0.6470588235294118, 0.8823529411764706, 0.11764705882352941, 0.11764705882352941, 0.5294117647058824, 0.8235294117647058, 1.0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.058823529411764705, 0.058823529411764705, 0.29411764705882354, 0.47058823529411764, 0.7058823529411765]\n"
          ]
        }
      ],
      "source": [
        "\"\"\"Implementation of LD coding method\n",
        "\"\"\"\n",
        "\n",
        "__all__ = ['ld_code_of']\n",
        "\n",
        "# AAC: Classification of amino acids.\n",
        "AAC = {\n",
        "    # '0': ['X'],\n",
        "    '1': ['A', 'G', 'V'],\n",
        "    '2': ['I', 'L', 'F', 'P'],\n",
        "    '3': ['Y', 'M', 'T', 'S'],\n",
        "    '4': ['H', 'N', 'Q', 'W'],\n",
        "    '5': ['R', 'K'],\n",
        "    '6': ['D', 'E'],\n",
        "    '7': ['C']\n",
        "}\n",
        "\n",
        "# AAC_R: Reverse of AAC.\n",
        "AAC_R = {}\n",
        "for C, AAS in AAC.items():\n",
        "    for AA in AAS:\n",
        "        AAC_R[AA] = C\n",
        "\n",
        "def classification_of(AA):\n",
        "    \"\"\"Get classification of amino acids.\"\"\"\n",
        "    return AAC_R[AA]\n",
        "\n",
        "def classification_sequence_of(PS):\n",
        "    \"\"\"Make classification sequence from protein sequence.\"\"\"\n",
        "    CS = ''\n",
        "    for I, CH in enumerate(PS):\n",
        "        if CH == 'X':\n",
        "          CS = CS\n",
        "        else:\n",
        "          CS = CS + classification_of(CH)\n",
        "    return CS\n",
        "\n",
        "def ld_info_of(CS):\n",
        "    L = len(CS)\n",
        "    C = {}\n",
        "    T = {}\n",
        "    for I, CH in enumerate(CS):\n",
        "        if CH not in C:\n",
        "            C[CH] = []\n",
        "        C[CH].append(I+1)\n",
        "        if I > 0:\n",
        "            PCH = CS[I-1]\n",
        "            if PCH != CH:\n",
        "                if int(PCH)<int(CH):\n",
        "                    TIndex = PCH + CH\n",
        "                else:\n",
        "                    TIndex = CH + PCH\n",
        "                if TIndex not in T:\n",
        "                    T[TIndex] = 0\n",
        "                T[TIndex] = T[TIndex]+1\n",
        "    return L, C, T\n",
        "\n",
        "def ld_code_of_0(CS):\n",
        "    RC = [0]*7\n",
        "    RT = [0]*21\n",
        "    RD = [0]*35\n",
        "    L, C, T = ld_info_of(CS)\n",
        "    for Class, Indexs in C.items():\n",
        "        Len = len(Indexs)\n",
        "        RC[int(Class)-1]=Len*1.0/L\n",
        "        Residues = [1, int(Len*0.25), int(Len*0.5), int(Len*0.75), Len]\n",
        "        # Residues = list(map(lambda x:x*1.0/L, Residues))\n",
        "        Residues = list(map(lambda x:Indexs[x-1]*1.0/L, Residues))\n",
        "        RD[(int(Class)-1)*5:int(Class)*5] = Residues\n",
        "    for Trans, Frequency in T.items():\n",
        "        PI, I = int(Trans[0])-1, int(Trans[1])-1\n",
        "        Index = int((21-(6-PI)*(6-PI+1)/2)+(I-PI-1))\n",
        "        RT[Index] = Frequency*1.0/(L-1)\n",
        "    # return RC, RT, RD\n",
        "    return RC+RT+RD\n",
        "\n",
        "def ld_code_of(PS):\n",
        "    \"\"\"Get LD Code of protein sequence.\"\"\"\n",
        "    CS = classification_sequence_of(PS)\n",
        "    L = len(CS)\n",
        "    A = ld_code_of_0(CS[          0:int(L*0.25)])\n",
        "    B = ld_code_of_0(CS[int(L*0.25):int(L*0.50)])\n",
        "    C = ld_code_of_0(CS[int(L*0.50):int(L*0.75)])\n",
        "    D = ld_code_of_0(CS[int(L*0.75):L          ])\n",
        "    E = ld_code_of_0(CS[          0:int(L*0.50)])\n",
        "    F = ld_code_of_0(CS[int(L*0.50):L          ])\n",
        "    G = ld_code_of_0(CS[int(L*0.25):int(L*0.75)])\n",
        "    H = ld_code_of_0(CS[          0:int(L*0.75)])\n",
        "    I = ld_code_of_0(CS[int(L*0.25):L          ])\n",
        "    J = ld_code_of_0(CS[int(L*0.125):int(L*0.875)])\n",
        "    return A+B+C+D+E+F+G+H+I+J\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "    PS = 'VCCPPVCVVCPPVCVPVPPCCV'\n",
        "    print('PS=', PS)\n",
        "    CS = classification_sequence_of(PS)\n",
        "    print('CS=', CS)\n",
        "    LD_INFO = ld_info_of(CS)\n",
        "    print('LD_INFO=', LD_INFO)\n",
        "    LD_CODE = ld_code_of_0(CS)\n",
        "    print('LD_CODE=', LD_CODE)\n",
        "    LD_CODE = ld_code_of(PS)\n",
        "    print('LD_CODE=', LD_CODE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ihzqb1mB8j6B"
      },
      "source": [
        "### Auto Covariance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "4Ri4QqyxGwL0"
      },
      "outputs": [],
      "source": [
        "# alphabet = [\"A\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"K\", \"L\", \"M\", \"N\", \"P\", \"Q\", \"R\", \"S\", \"T\", \"V\", \"W\", \"Y\"]\n",
        "# chem_props = np.array([0.62, -0.5,0.007187,8.1,0.046,1.181,27.5,0.29,-1,-0.03661,5.5,0.128,1.461,44.6,-0.9,3,-0.02382,13,0.105,1.587,40,-0.74,3,0.006802,12.3,0.151,1.862,62,1.19,-2.5,0.037552,5.2,0.29,2.228,115.5,0.48,0,0.179052,9,0,0.881,0,-0.4,-0.5,-0.01069,10.4,0.23,2.025,79,1.38,-1.8,0.021631,5.2,0.186,1.81,93.5,-1.5,3,0.017708,11.3,0.219,2.258,100,1.06,-1.8,0.051672,4.9,0.186,1.931,93.5,0.64,-1.3,0.002683,5.7,0.221,2.034,94.1,-0.78,2,0.005392,11.6,0.134,1.655,58.7,0.12,0,0.239531,8,0.131,1.468,41.9,-0.85,0.2,0.049211,10.5,0.18,1.932,80.7,-2.53,3,0.043587,10.5,0.291,2.56,105,-0.18,0.3,0.004627,9.2,0.062,1.298,29.3,-0.05,-0.4,0.003352,8.6,0.108,1.525,51.3,1.08,-1.5,0.057004,5.9,0.14,1.645,71.5,0.81,-3.4,0.037977,5.4,0.409,2.663,145.5,0.26,-2.3,117.3,6.2,0.298,2.368,0.023599])\n",
        "# chem_props = chem_props.reshape(len(alphabet),7)\n",
        "\n",
        "# # a = np.asarray([ [1,2,3], [4,5,6], [7,8,9] ])\n",
        "# # for i in range(7):\n",
        "# import pandas as pd \n",
        "# df = pd.DataFrame(chem_props)\n",
        "# df = df.rename(index={0: \"A\", 1: \"C\", 2: \"D\", 3: \"E\", 4: \"F\", 5: \"G\", 6: \"H\", 7: \"I\", 8: \"K\", 9: \"L\", 10: \"M\", 11: \"N\", 12: \"P\", 13:\"Q\", 14:\"R\", 15: \"S\", 16:\"T\", 17: \"V\", 18: \"W\", 19: \"Y\"})\n",
        "# df.to_csv(\"encode_ac.txt\", sep='\\t', header=None)\n",
        "# # df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXJI0Myaf0Rj",
        "outputId": "d7196ac0-339c-42c4-ac27-c68dffc3f07c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.10790803428842154, 0.11173882903839012, -0.14460761970272953, -0.07535489086506163, 0.006237248073411141, 0.0013678539469620598, 0.05270459435279943, 0.1828958100504284, -0.06548284241673244, -0.06356580523179726, -0.0827079689124269, 0.0017860602217885661, 0.13334465990147337, -0.024160265667395202, -0.037172464278242986, -0.06483520719302295, -0.13815087172208335, 0.0897187757953958, -0.040758691200317373, 0.18293023336208467, 0.12094364277218479, 0.09689785077692985, 0.049025209707293, 0.06979361502005867, 0.218261048339127, 0.1893236184497817, 0.018408227449015546, 0.08829965543046875, 0.052992276171663556, -0.12454362437003005, 0.32508896178416097, 0.06277731652196941, -0.17305532936980655, -0.06880909521329583, 0.007179034232072711, -0.0009166973877763069, 0.10437143296785639, 0.21149735698003105, -0.05294867285423665, 0.04381139122214881, 0.17104549428953786, 0.032328074948299, -0.07969681488713316, -0.051078254991167714, 0.07857539050459156, 0.006677363314985386, -0.16450598856132026, -0.005023355716242042, 0.13447492229141475, 0.2725606506999623, 0.06972775465272221, -0.10439079233878017, -0.042462529041505166, -0.06525527769839443, -0.007620719414048265, 0.2929423870463783, 0.1698837839119438, 0.34368375048203115, 0.05364604359784541, 0.0865836477533407, -0.09979393067695948, -0.1023334318205191, 0.09404673973154629, -0.09341365460725153, -0.08143986121176419, 0.10615002323508783, -0.08491195292088012, -0.08693758734869886, 0.12068427066929377, -0.07602626943625378, 0.12012154373173536, 0.134815923662001, -0.06800504219689996, 0.13491253696728084, -0.07093044946192124, -0.07318664579059081, -0.0742398973620677, -0.07670748508358097, -0.07744721943693919, 0.3542482387516324, -0.08082791763539843, -0.08396376436926489, -0.06936817429741346, 0.1553165092704602, -0.057744888658659295, -0.06009246184686986, -0.06153552146714124, -0.06366557817467383, -0.06522554712943501, -0.06681398915483294, 0.2742891805951771, 0.0230304043317034, -0.12292307987298794, -0.10812428048967351, -0.07140079055430806, -0.12313018528106721, 0.029119289807633546, 0.12958368953587043, -0.0781614311723816, 0.02444938842952771, 0.17196013425884496, 0.013359731523309278, -0.05043821949700768, -0.05622880675505674, 0.09371760849494398, -0.03494851113768867, -0.24735071296973993, -0.06242874059343693, 0.08926891500575204, 0.1498449073149161, 0.007879507848047874, -0.13648669759815008, -0.04722946844876689, -0.08230829240123116, -0.08349532175232492, 0.2327225476156886, 0.1705090730041967, 0.29656740391166486, 0.124518743273069, 0.053158802172273534, 0.08429033960587115, 0.0784269594239277, 0.09858020626009595, -0.11899949623741407, -0.04247129246150611, 0.07352898646383896, -0.20920354321570217, 0.041066825607296874, -0.019035819330898193, -0.05140892548426074, 0.0026747731573285812, -0.011297630148535838, -0.08553000724400059, -0.016733659903766872, -0.04098354508224823, 0.08220009310606198, 0.01349952571069666, -0.0030177472257386824, 0.011802538631382685, -0.012382573878674481, -0.039488829507421226, -0.021417524901129907, -0.13347769450074365, 0.023441603483421957, -0.0648941442216763, -0.03879933455225307, 0.06554742475995017, -0.09162228182983588, 0.0007407293592178221, 0.007811157463376443, 0.08901384659482531, 0.09686261607218066, 0.13941681960649904, -0.13000565009692766, -0.021766381430552543, 0.13660769049296473, -0.20453043142370772, 0.027468349774399044, -0.03435692650993974, -0.054976281770174794, -0.004913069849397187, -0.015493718156269785, -0.07198828822266251, -0.032557246717684354, -0.041356143651355044, 0.07391663877343498, -0.016644490035137377, -0.018413903228021555, -0.015284487422400444, -0.02969323181356979, 0.0032449767663577257, -0.016378731691578895, -0.11054490073761429, 0.03452649153659403, -0.05347842900159755, 0.009133573842133277, 0.09119946731010772, -0.10786912443540668, -0.03945050855068204, -0.026254579235587553, 0.046881420880807656, 0.014121866300099857, 0.10260631465175607, -0.08728678622993302, -0.037565207510200496, -0.007288668744566924, 0.1007613325881951, -0.05234734188176808, 0.05612806804452678, -0.06486998122640711, -0.029519493378660673, -0.029339986264426934, 0.025886588935373107, 0.06350573350290536, -0.0925061804413994, 0.023658082282278073, -0.00022690216577556002, -0.0049267887928321295, 0.016573618705815727, 0.12849803544634225, 0.02032403430626441, -0.07137030432764936, -0.043789209010392544, 0.05080259733110751, -0.05631605447914313, -0.00287696514061479, 0.10973743165259096, -0.07342673761532045, -0.06931273815410258, -0.14370165374581315]\n"
          ]
        }
      ],
      "source": [
        "'''Implementation of AC coding method\n",
        "'''\n",
        "\n",
        "__all__ = ['ac_code_of']\n",
        "\n",
        "# PCPNS: Physicochemical property names\n",
        "PCPNS = ['H1', 'H2', 'NCI', 'P1', 'P2', 'SASA', 'V']\n",
        "\n",
        "# AAPCPVS: Physicochemical property values of amino acid\n",
        "AAPCPVS = {\n",
        "    'A': { 'H1': 0.62, 'H2':-0.5, 'NCI': 0.007187, 'P1': 8.1, 'P2':0.046, 'SASA':1.181, 'V': 27.5 },\n",
        "    'C': { 'H1': 0.29, 'H2':-1.0, 'NCI':-0.036610, 'P1': 5.5, 'P2':0.128, 'SASA':1.461, 'V': 44.6 },\n",
        "    'D': { 'H1':-0.90, 'H2': 3.0, 'NCI':-0.023820, 'P1':13.0, 'P2':0.105, 'SASA':1.587, 'V': 40.0 },\n",
        "    'E': { 'H1': 0.74, 'H2': 3.0, 'NCI': 0.006802, 'P1':12.3, 'P2':0.151, 'SASA':1.862, 'V': 62.0 },\n",
        "    'F': { 'H1': 1.19, 'H2':-2.5, 'NCI': 0.037552, 'P1': 5.2, 'P2':0.290, 'SASA':2.228, 'V':115.5 },\n",
        "    'G': { 'H1': 0.48, 'H2': 0.0, 'NCI': 0.179052, 'P1': 9.0, 'P2':0.000, 'SASA':0.881, 'V':  0.0 },\n",
        "    'H': { 'H1':-0.40, 'H2':-0.5, 'NCI':-0.010690, 'P1':10.4, 'P2':0.230, 'SASA':2.025, 'V': 79.0 },\n",
        "    'I': { 'H1': 1.38, 'H2':-1.8, 'NCI': 0.021631, 'P1': 5.2, 'P2':0.186, 'SASA':1.810, 'V': 93.5 },\n",
        "    'K': { 'H1':-1.50, 'H2': 3.0, 'NCI': 0.017708, 'P1':11.3, 'P2':0.219, 'SASA':2.258, 'V':100.0 },\n",
        "    'L': { 'H1': 1.06, 'H2':-1.8, 'NCI': 0.051672, 'P1': 4.9, 'P2':0.186, 'SASA':1.931, 'V': 93.5 },\n",
        "    'M': { 'H1': 0.64, 'H2':-1.3, 'NCI': 0.002683, 'P1': 5.7, 'P2':0.221, 'SASA':2.034, 'V': 94.1 },\n",
        "    'N': { 'H1':-0.78, 'H2': 2.0, 'NCI': 0.005392, 'P1':11.6, 'P2':0.134, 'SASA':1.655, 'V': 58.7 },\n",
        "    'P': { 'H1': 0.12, 'H2': 0.0, 'NCI': 0.239531, 'P1': 8.0, 'P2':0.131, 'SASA':1.468, 'V': 41.9 },\n",
        "    'Q': { 'H1':-0.85, 'H2': 0.2, 'NCI': 0.049211, 'P1':10.5, 'P2':0.180, 'SASA':1.932, 'V': 80.7 },\n",
        "    'R': { 'H1':-2.53, 'H2': 3.0, 'NCI': 0.043587, 'P1':10.5, 'P2':0.291, 'SASA':2.560, 'V':105.0 },\n",
        "    'S': { 'H1':-0.18, 'H2': 0.3, 'NCI': 0.004627, 'P1': 9.2, 'P2':0.062, 'SASA':1.298, 'V': 29.3 },\n",
        "    'T': { 'H1':-0.05, 'H2':-0.4, 'NCI': 0.003352, 'P1': 8.6, 'P2':0.108, 'SASA':1.525, 'V': 51.3 },\n",
        "    'V': { 'H1': 1.08, 'H2':-1.5, 'NCI': 0.057004, 'P1': 5.9, 'P2':0.140, 'SASA':1.645, 'V': 71.5 },\n",
        "    'W': { 'H1': 0.81, 'H2':-3.4, 'NCI': 0.037977, 'P1': 5.4, 'P2':0.409, 'SASA':2.663, 'V':145.5 },\n",
        "    'Y': { 'H1': 0.26, 'H2':-2.3, 'NCI': 117.3000, 'P1': 6.2, 'P2':0.298, 'SASA':2.368, 'V':  0.023599 },\n",
        "}\n",
        "\n",
        "import math\n",
        "\n",
        "def avg_sd(NUMBERS):\n",
        "    AVG = sum(NUMBERS)/len(NUMBERS)\n",
        "    TEM = [pow(NUMBER-AVG, 2) for NUMBER in NUMBERS]\n",
        "    DEV = sum(TEM)/len(TEM)\n",
        "    SD = math.sqrt(DEV)\n",
        "    return (AVG, SD)\n",
        "\n",
        "# PCPVS: Physicochemical property values\n",
        "PCPVS = {'H1':[], 'H2':[], 'NCI':[], 'P1':[], 'P2':[], 'SASA':[], 'V':[]}\n",
        "for AA, PCPS in AAPCPVS.items():\n",
        "    for PCPN in PCPNS:\n",
        "        PCPVS[PCPN].append(PCPS[PCPN])\n",
        "\n",
        "# PCPASDS: Physicochemical property avg and sds\n",
        "PCPASDS = {}\n",
        "for PCP, VS in PCPVS.items():\n",
        "    PCPASDS[PCP] = avg_sd(VS)\n",
        "\n",
        "# NORMALIZED_AAPCPVS\n",
        "NORMALIZED_AAPCPVS = {}\n",
        "for AA, PCPS in AAPCPVS.items():\n",
        "    NORMALIZED_PCPVS = {}\n",
        "    for PCP, V in PCPS.items():\n",
        "        NORMALIZED_PCPVS[PCP] = (V-PCPASDS[PCP][0])/PCPASDS[PCP][1]\n",
        "    NORMALIZED_AAPCPVS[AA] = NORMALIZED_PCPVS\n",
        "\n",
        "def pcp_value_of(AA, PCP):\n",
        "    \"\"\"Get physicochemical properties value of amino acid.\"\"\"\n",
        "    return NORMALIZED_AAPCPVS[AA][PCP];\n",
        "\n",
        "def pcp_sequence_of(PS, PCP):\n",
        "    \"\"\"Make physicochemical properties sequence of protein sequence.\"\"\"\n",
        "    PCPS = []\n",
        "    for I, CH in enumerate(PS):\n",
        "        PCPS.append(pcp_value_of(CH, PCP))\n",
        "    # Centralization\n",
        "    AVG = sum(PCPS)/len(PCPS)\n",
        "    for I, PCP in enumerate(PCPS):\n",
        "        PCPS[I] = PCP - AVG\n",
        "    return PCPS\n",
        "\n",
        "def ac_values_of(PS, PCP, LAG):\n",
        "    \"\"\"Get ac values of protein sequence.\"\"\"\n",
        "    AVS = []\n",
        "    PCPS = pcp_sequence_of(PS, PCP)\n",
        "    for LG in range(1, LAG+1):\n",
        "        SUM = 0\n",
        "        for I in range(len(PCPS)-LG):\n",
        "            SUM = SUM + PCPS[I]*PCPS[I+LG]\n",
        "        SUM = SUM / (len(PCPS)-LG)\n",
        "        AVS.append(SUM)\n",
        "    return AVS\n",
        "\n",
        "def all_ac_values_of(PS, LAG):\n",
        "    \"\"\"Get all ac values of protein sequence.\"\"\"\n",
        "    AAVS = []\n",
        "    for PCP in PCPS:\n",
        "        AVS = ac_values_of(PS, PCP, LAG)\n",
        "        AAVS = AAVS + AVS\n",
        "    return AAVS\n",
        "\n",
        "def ac_code_of(PS):\n",
        "    \"\"\"Get ac code of protein sequence.\"\"\"\n",
        "    AC_Code = all_ac_values_of(PS, 30)\n",
        "    # Normalizing AC_Code\n",
        "    # MIN_CODE = min(AC_Code)\n",
        "    # MAX_CODE = max(AC_Code)\n",
        "    # AC_Code = [(N-MIN_CODE)*1.0/(MAX_CODE-MIN_CODE) for N in AC_Code]\n",
        "    return AC_Code\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "    AC = ac_code_of('MKFVYKEEHPFEKRRSEGEKIRKKYPDRVPVIVEKAPKARIGDLDKKKYLVPSDLTVGQFYFLIRKRIHLRAEDALFFFVNNVIPPTSATMGQLYQEHHEEDFFLYIAYSDESVYGL')\n",
        "    print(AC)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8ncX7ZW8ma7"
      },
      "source": [
        "### Pseudo amino acid composition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "CHIsFXbi07ja"
      },
      "outputs": [],
      "source": [
        "def paac(str_, lambda_=0):\n",
        "  # str_=\"ATTRCDEQGGGMFSTQW\"\n",
        "  # lambda_ = 3\n",
        "  len_=len(str_)\n",
        "  tt=['A', 'R', 'N', 'D', 'C', 'E', 'Q', 'G', 'H', 'I',  'L', 'K', 'M', 'F', 'P', 'S', 'T', 'W', 'Y', 'V']\n",
        "  A = [0.62,  -0.5, 15]\n",
        "  R = [-2.53,   3, 101]\n",
        "  N = [-0.78,  0.2, 58]\n",
        "  D = [-0.9,    3, 59]\n",
        "  C = [0.29,    -1, 47]\n",
        "  E = [-0.74,   3, 73]\n",
        "  Q = [-0.85,  0.2, 72]\n",
        "  G =[0.48,    0, 1]\n",
        "  H = [-0.4,  -0.5, 82]\n",
        "  I = [1.38, -1.8, 57]\n",
        "  L = [1.06,  -1.8, 57]\n",
        "  K = [-1.5,    3, 73]\n",
        "  M = [0.64,  -1.3, 75]\n",
        "  F =[1.19, -2.5, 91]\n",
        "  P = [0.12,     0, 42]\n",
        "  S = [-0.18, 0.3, 31]\n",
        "  T = [-0.05, -0.4, 45]\n",
        "  W = [0.81, -3.4, 130] \n",
        "  Y = [0.26,  -2.3, 107]\n",
        "  V = [1.08, -1.5, 43]\n",
        "  X = [0, 0, 0]\n",
        "  H1=[A[0],R[0],N[0],D[0],C[0],E[0],Q[0],G[0],H[0],I[0],L[0],K[0],M[0],F[0],P[0],S[0],T[0],W[0],Y[0],V[0]]\n",
        "  H2=[A[1],R[1],N[1],D[1],C[1],E[1],Q[1],G[1],H[1],I[1],L[1],K[1],M[1],F[1],P[1],S[1],T[1],W[1],Y[1],V[1]]\n",
        "  M=[A[2],R[2],N[2],D[2],C[2],E[2],Q[2],G[2],H[2],I[2],L[2],K[2],M[2],F[2],P[2],S[2],T[2],W[2],Y[2],V[2]]\n",
        "  # Normalization\n",
        "  mean_H1=np.mean(H1)\n",
        "  std_H1=np.std(H1)\n",
        "  H1=(H1-mean_H1)/(std_H1)\n",
        "\n",
        "\n",
        "\n",
        "  mean_H2=np.mean(H2)\n",
        "  std_H2=np.std(H2)\n",
        "  H2=(H2-mean_H2)/(std_H2)\n",
        "\n",
        "  mean_M=np.mean(M)\n",
        "  std_M=np.std(M)\n",
        "  M=(M-mean_M)/(std_M)\n",
        "  data=np.zeros((1,len_))\n",
        "  f=np.zeros((1,20))\n",
        "\n",
        "  for j in range(len_):\n",
        "      for k in range(20):\n",
        "          # if strcmp(str(j),tt(k))==1\n",
        "          if str_[j] == tt[k]:\n",
        "              # print(j, k)\n",
        "              data[:,j]=int(k)+1\n",
        "              f[:,k]=f[:,k]+1\n",
        "  data = data.astype('int32')\n",
        "  Theta=np.zeros((lambda_,len_))\n",
        "  H=np.hstack((H1,H2,M))\n",
        "  H=H.reshape(3,-1)\n",
        "  for i in range(lambda_):\n",
        "      # for j=1:len-i\n",
        "      for j in range(len_-i):\n",
        "          if j+i+1<len_:\n",
        "              Theta[i,j]=np.mean(np.mean((H[:, data[:,j]-1]-H[:, data[:,j+i+1]-1])**2))\n",
        "\n",
        "  theta=np.zeros((1,lambda_))\n",
        "  for j in range(lambda_):\n",
        "      theta[:,j]=np.mean(Theta[j,:(len_-j-1)])\n",
        "\n",
        "  f=f/len_\n",
        "  XC=f/(1+0.05*np.sum(theta))\n",
        "  XC2=(0.05*theta)/(1+0.05*np.sum(theta))\n",
        "\n",
        "  paac = np.hstack((XC, XC2))\n",
        "  paac = paac.reshape(-1,).tolist()\n",
        "  return paac\n",
        "\n",
        "\n",
        "# 23 dimension paac vector\n",
        "# paac(seq, 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGEuw7JBLpcY"
      },
      "source": [
        "### Amino acid composition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "oGMJF17fN9vn"
      },
      "outputs": [],
      "source": [
        "def aac(seq):\n",
        "  aa_list = ['A', 'R', 'N', 'D', 'C', 'E', 'Q', 'G', 'H', 'I',  'L', 'K', 'M', 'F', 'P', 'S', 'T', 'W', 'Y', 'V']\n",
        "  sum_freq = 0\n",
        "  for i in range(20):\n",
        "    sum_freq += seq.count(aa_list[i])\n",
        "\n",
        "  aa_freq = []\n",
        "  for i in range(20):\n",
        "    aa_freq.append(seq.count(aa_list[i])/sum_freq)\n",
        "  return aa_freq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EdaN4-dO-WQ"
      },
      "source": [
        "### Concatenate of protein sequence features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ih153Zn6RTxj"
      },
      "outputs": [],
      "source": [
        "def encode_seq(seq):\n",
        "  encoding = aac(seq)+ct_code_of(seq)+ld_code_of(seq)\n",
        "  encoding = np.array(encoding)\n",
        "  encoding = encoding.reshape(-1, )\n",
        "  return encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TpKS4zsW5U8"
      },
      "source": [
        "### Use universal embedding files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OkpDLs-yW5U8",
        "outputId": "d07f26b6-d76e-4e27-bec7-52ca7e1267c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2916it [00:00, 177656.59it/s]\n",
            "100%|██████████| 1441/1441 [00:00<00:00, 2182.92it/s]\n",
            "100%|██████████| 2915/2915 [00:00<00:00, 838400.61it/s]\n",
            "100%|██████████| 2915/2915 [00:00<00:00, 894330.78it/s]\n"
          ]
        }
      ],
      "source": [
        "if available_data == 0:\n",
        "  id2seq_file = 'hp_dict.tsv'\n",
        "  id2index = {}\n",
        "  seqs = []\n",
        "  index = 0\n",
        "  sid1_index = 0\n",
        "  sid2_index = 1\n",
        "  ds_file = 'hp_pairs.tsv'\n",
        "  label_index = 2\n",
        "  use_emb = 'ac5_aph.txt'\n",
        "\n",
        "\n",
        "  # Create line variable as a list of protein sequences with index is the number of protein sequences\n",
        "  # id2index is a dictionary of protein id and incremental index number \n",
        "  for line in open(id2seq_file):\n",
        "      line = line.strip().split('\\t')\n",
        "      id2index[line[0]] = index\n",
        "      seqs.append(line[1])\n",
        "      index += 1\n",
        "\n",
        "  seq_array = []\n",
        "  id2_aid = {}\n",
        "  sid = 0\n",
        "\n",
        "  seq2t = s2t(use_emb)\n",
        "\n",
        "  max_data = -1\n",
        "  limit_data = max_data > 0\n",
        "  raw_data = []\n",
        "  skip_head = True\n",
        "  x = None\n",
        "  count = 0\n",
        "\n",
        "  # Create sequence array as a list of protein strings\n",
        "  for line in tqdm(open(ds_file)):\n",
        "      if skip_head:\n",
        "          skip_head = False\n",
        "          continue\n",
        "      line = line.rstrip('\\n').rstrip('\\r').split('\\t')\n",
        "      if id2index.get(line[sid1_index]) is None or id2index.get(line[sid2_index]) is None:\n",
        "          continue\n",
        "      if id2_aid.get(line[sid1_index]) is None:\n",
        "          id2_aid[line[sid1_index]] = sid\n",
        "          sid += 1\n",
        "          seq_array.append(seqs[id2index[line[sid1_index]]])\n",
        "      line[sid1_index] = id2_aid[line[sid1_index]]\n",
        "      if id2_aid.get(line[sid2_index]) is None:\n",
        "          id2_aid[line[sid2_index]] = sid\n",
        "          sid += 1\n",
        "          seq_array.append(seqs[id2index[line[sid2_index]]])\n",
        "      line[sid2_index] = id2_aid[line[sid2_index]]\n",
        "      raw_data.append(line)\n",
        "      if limit_data:\n",
        "          count += 1\n",
        "          if count >= max_data:\n",
        "              break\n",
        "\n",
        "  len_m_seq = np.array([len(line.split()) for line in seq_array])\n",
        "  avg_m_seq = int(np.average(len_m_seq)) + 1\n",
        "  max_m_seq = max(len_m_seq)\n",
        "  dim = seq2t.dim\n",
        "\n",
        "  # seq_tensor is tensor representation of dataset having shape of (number_of_sequences, padding_length, embedding_dim_of_aa)\n",
        "  # Random for distribution of class labels\n",
        "  np.random.seed(42)\n",
        "  np.random.shuffle(raw_data)\n",
        "\n",
        "  seq_tensor = np.array([seq2t.embed_normalized(line, seq_size) for line in tqdm(seq_array)])\n",
        "  # seq_tensor = np.array([encode_seq(line) for line in tqdm(seq_array)])\n",
        "\n",
        "  # Extract index of 1st and 2nd sequences in pairs\n",
        "  seq_index1 = np.array([line[sid1_index] for line in tqdm(raw_data)])\n",
        "  seq_index2 = np.array([line[sid2_index] for line in tqdm(raw_data)])\n",
        "\n",
        "  # Assign labels for pairs of sequences\n",
        "  class_map = {'0':1,'1':0}\n",
        "  class_labels = np.zeros((len(raw_data), 2))\n",
        "  for i in range(len(raw_data)):\n",
        "      class_labels[i][class_map[raw_data[i][label_index]]] = 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thgXgMlTW5U9"
      },
      "source": [
        "### Use language model for embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "S_nzJn-AW5U9"
      },
      "outputs": [],
      "source": [
        "if flags_embedding == 1:\n",
        "    !pip install bio-embeddings[all] # Need to restart runtime for the first run \n",
        "\n",
        "    # Choose protein language model for embedder\n",
        "    from Bio import SeqIO # From Biopython library import SeqIO module to handle sequences when read and write different file formats\n",
        "\n",
        "    # Chooose language model embedder class from package bio-embeddings -> moduel embed\n",
        "    # from bio_embeddings.embed import CPCProtEmbedder, ProtTransT5XLU50Embedder, FastTextEmbedder, GloveEmbedder, PLUSRNNEmbedder, ProtTransBertBFDEmbedder, SeqVecEmbedder, UniRepEmbedder, Word2VecEmbedder, ProtTransXLNetUniRef100Embedder\n",
        "    #   from bio_embeddings.embed import ProtTransBertBFDEmbedder\n",
        "    from bio_embeddings.embed import ProtTransT5UniRef50Embedder\n",
        "    embedder = ProtTransT5UniRef50Embedder()\n",
        "\n",
        "    # Download raw sequences and create a list of sequences\n",
        "    !wget https://raw.githubusercontent.com/anhvt00/PIPR/master/yeast/preprocessed/protein_preprocessed.txt\n",
        "    with open('protein_preprocessed.txt') as file:\n",
        "        sequences = file.readlines()\n",
        "        sequences = [sequence.rstrip() for sequence in sequences]\n",
        "\n",
        "    # Install in the case of using A100 for pytorch compatibility\\\n",
        "    A100_status = !nvidia-smi | grep 'A100'\n",
        "    if A100_status:\n",
        "        !pip install torch==1.8.1+cu111 torchvision==0.9.1+cu111 torchaudio==0.8.1 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "\n",
        "    # embeddings = []\n",
        "    # for sequence in sequences:\n",
        "    #     embeddings.append(embedder.embed(sequence))\n",
        "\n",
        "    # # Start embedding \n",
        "    # # Embedding in generator form, need to iterate (flexible)\n",
        "    embeddings = embedder.embed_many(sequences)\n",
        "\n",
        "    # # Use list function to convert generator to list (true form of dataset)\n",
        "    embeddings = list(embeddings)\n",
        "\n",
        "    # # Average pooling in sequence dimension\n",
        "    # reduced_embeddings = [ProtTransBertBFDEmbedder.reduce_per_protein(e) for e in embeddings]\n",
        "\n",
        "    # # Padding to create fixed size tensor\n",
        "    seq_tensor= tf.keras.preprocessing.sequence.pad_sequences(embeddings,  padding='post', dtype='float16', truncating='post', maxlen=seq_size)\n",
        "    dim = seq_tensor.shape[2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "189z3qijJ2F2"
      },
      "source": [
        "### Define custom function "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "LcEhSLxsONax"
      },
      "outputs": [],
      "source": [
        "def leaky_relu(x, alpha = .3):\n",
        "   return tf.keras.backend.maximum(alpha*x, x)\n",
        "\n",
        "get_custom_objects().update({'leaky_relu': leaky_relu})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HyK_KJ6LUfR"
      },
      "source": [
        "### Search for optimal configurations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "SlRW8Nh71zjs"
      },
      "outputs": [],
      "source": [
        "HP_EPSILON = hp.HParam('epsilon', hp.Discrete([1e-6]))\n",
        "\n",
        "HP_LEARNING_RATE = hp.HParam('learning_rate', hp.Discrete([1e-3]))\n",
        "\n",
        "HP_FIRST_DENSE = hp.HParam('first_dense', hp.Discrete([100]))\n",
        "\n",
        "HP_KERNEL_SIZE = hp.HParam('kernel_size', hp.Discrete([3]))\n",
        "\n",
        "HP_POOLING_KERNEL = hp.HParam('pooling_kernel', hp.Discrete([3]))\n",
        "\n",
        "HP_CONV_HIDDEN_DIM = hp.HParam('conv_hidden_dim', hp.Discrete([50]))\n",
        "HP_RNN_HIDDEN_DIM = hp.HParam('rnn_hidden_dim', hp.Discrete([50]))\n",
        "\n",
        "HP_ACTIVATION = hp.HParam('activation', hp.Discrete(['leaky_relu']))\n",
        "\n",
        "HP_ACTIVATION_CONV = hp.HParam('activation_conv', hp.Discrete(['linear']))\n",
        "\n",
        "HP_REGULARIZER = hp.HParam('regularizer', hp.Discrete([0]))\n",
        "\n",
        "HP_CONV_PADDING = hp.HParam('conv_padding', hp.Discrete(['valid']))\n",
        "\n",
        "HP_DROPOUT = hp.HParam('dropout', hp.Discrete([2e-1]))\n",
        "\n",
        "HP_BATCH_SIZE = hp.HParam('batch_size', hp.Discrete([32]))\n",
        "\n",
        "HP_LEAKY_RELU = hp.HParam('leaky_relu', hp.Discrete([3e-1]))\n",
        "\n",
        "METRIC_ACCURACY = 'accuracy'\n",
        "\n",
        "with tf.summary.create_file_writer('logs/hparam_tuning').as_default():\n",
        "  hp.hparams_config(\n",
        "    hparams=[HP_EPSILON,HP_LEARNING_RATE,HP_FIRST_DENSE, HP_KERNEL_SIZE, HP_POOLING_KERNEL, HP_CONV_HIDDEN_DIM, HP_RNN_HIDDEN_DIM, HP_ACTIVATION, HP_ACTIVATION_CONV, HP_REGULARIZER, HP_CONV_PADDING, HP_DROPOUT, HP_BATCH_SIZE, HP_LEAKY_RELU],\n",
        "    metrics=[hp.Metric(METRIC_ACCURACY, display_name='Accuracy')],\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03xD1lTqltN2"
      },
      "source": [
        "### Create dataset from generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "1rrgaWsfl4Op"
      },
      "outputs": [],
      "source": [
        "def generator_pair(dataset):\n",
        "  for index in dataset:\n",
        "    yield {\"seq1\": seq_tensor[seq_index1[index]], \"seq2\": seq_tensor[seq_index2[index]]}, class_labels[index]\n",
        "\n",
        "def generator_pair_predict(dataset):\n",
        "  for index in dataset:\n",
        "    yield {\"seq1\": seq_tensor[seq_index1[index]], \"seq2\": seq_tensor[seq_index2[index]]}\n",
        "\n",
        "# def generator_pair_augment(dataset, augment_data):\n",
        "#   if augment_data == False:\n",
        "#     for index in dataset:\n",
        "#       yield {\"seq1\": seq_tensor[seq_index1[index]], \"seq2\": seq_tensor[seq_index2[index]]}, class_labels[index]\n",
        "#   else:\n",
        "#     for index in dataset:\n",
        "#       yield {\"seq1\": seq_tensor[seq_index1[index]], \"seq2\": seq_tensor[seq_index2[index]]}, class_labels[index]\n",
        "#       yield {\"seq2\": seq_tensor[seq_index1[index]], \"seq1\": seq_tensor[seq_index2[index]]}, class_labels[index]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KgZZ2JvoBLw"
      },
      "source": [
        "### Split the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "O1DkEaQYum2O"
      },
      "outputs": [],
      "source": [
        "### k-fold cross-validation\n",
        "from sklearn.model_selection import KFold, ShuffleSplit\n",
        "from sklearn.model_selection import train_test_split\n",
        "kf = KFold(n_splits=5, random_state=42, shuffle=True)\n",
        "tries = 5\n",
        "cur = 0\n",
        "recalls = []\n",
        "accuracy = []\n",
        "total = []\n",
        "total_truth = []\n",
        "train_test = []\n",
        "for train, test in kf.split(class_labels):\n",
        "    # redundant because same position\n",
        "    # if np.sum(class_labels[train], 0)[0] > 0.8 * len(train) or np.sum(class_labels[train], 0)[0] < 0.2 * len(train):\n",
        "    #     continue\n",
        "    train_test.append((train, test))\n",
        "    cur += 1\n",
        "    if cur >= tries:\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G34oBACWLqbw"
      },
      "source": [
        "### Define callbacks for monitor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "VQdf1RAsYVcA"
      },
      "outputs": [],
      "source": [
        "### Define tensorboard callback to optimize resource using of model\n",
        "logs = \"logs/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "tboard_callback = tf.keras.callbacks.TensorBoard(log_dir = logs,\n",
        "                                                 histogram_freq = 1,\n",
        "                                                 profile_batch = '20, 29')\n",
        "\n",
        "### Learning rate schedule for optimization during training\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
        "    monitor=\"val_loss\",\n",
        "    factor=0.,\n",
        "    patience=10,\n",
        "    verbose=0,\n",
        "    mode=\"auto\",\n",
        "    min_lr=1e-4)\n",
        "\n",
        "# Schedule early stopping\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss', \n",
        "    verbose=1,\n",
        "    patience=10,\n",
        "    mode='auto',\n",
        "    restore_best_weights=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLqkyCvrruF-"
      },
      "source": [
        "### Define performance metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Um2gRMVrZPi",
        "outputId": "68bb35d1-7ae0-4e66-968c-1a6c0957ec1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.15.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[?25l\r\u001b[K     |▎                               | 10 kB 24.2 MB/s eta 0:00:01\r\u001b[K     |▋                               | 20 kB 21.7 MB/s eta 0:00:01\r\u001b[K     |▉                               | 30 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |█▏                              | 40 kB 14.1 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 51 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 61 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |██                              | 71 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 81 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 92 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |███                             | 102 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 112 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 122 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 133 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 143 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 153 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 163 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████                           | 174 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 184 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 194 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 204 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 215 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 225 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 235 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████                         | 245 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 256 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 266 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████                        | 276 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 286 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 296 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 307 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 317 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 327 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 337 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 348 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 358 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 368 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 378 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 389 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 399 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 409 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 419 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 430 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 440 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 450 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 460 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 471 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 481 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 491 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 501 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 512 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 522 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 532 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 542 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 552 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 563 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 573 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 583 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 593 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 604 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 614 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 624 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 634 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 645 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 655 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 665 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 675 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 686 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 696 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 706 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 716 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 727 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 737 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 747 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 757 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 768 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 778 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 788 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 798 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 808 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 819 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 829 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 839 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 849 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 860 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 870 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 880 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 890 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 901 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 911 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 921 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 931 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 942 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 952 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 962 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 972 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 983 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 993 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.0 MB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.0 MB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.0 MB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.0 MB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.0 MB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.1 MB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 1.1 MB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.1 MB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.1 MB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.1 MB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.1 MB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.1 MB 7.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.15.0\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow-addons\n",
        "import tensorflow_addons as tfa\n",
        "METRICS = [\n",
        "      # keras.metrics.Accuracy(name='accuracy'),\n",
        "      # keras.metrics.TruePositives(name='tp'),\n",
        "      # keras.metrics.FalsePositives(name='fp'),\n",
        "      # keras.metrics.TrueNegatives(name='tn'),\n",
        "      # keras.metrics.FalseNegatives(name='fn'), \n",
        "      keras.metrics.BinaryAccuracy(name='accuracy'),\n",
        "      keras.metrics.Precision(name='precision'),\n",
        "      keras.metrics.Recall(name='recall'),\n",
        "      tfa.metrics.MatthewsCorrelationCoefficient(num_classes=2, name='mcc'),\n",
        "      tfa.metrics.F1Score(num_classes=2, threshold=0.5, name='f1-score'),\n",
        "      keras.metrics.AUC(name='auc'),\n",
        "      keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
        "]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRvlZNzVipPt"
      },
      "source": [
        "### Original architecture PIPR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "QnH_6e2tHjTg"
      },
      "outputs": [],
      "source": [
        "def build_model(hparams):\n",
        "    # Input of sequence tensor representations \n",
        "    seq_input1 = Input(shape=(seq_size, dim), name='seq1')\n",
        "    seq_input2 = Input(shape=(seq_size, dim), name='seq2')\n",
        "\n",
        "    # Define Conv1D and Bi-RNN (GRU/LSTM) use in architecture\n",
        "    l1=Conv1D(hparams[HP_CONV_HIDDEN_DIM], hparams[HP_KERNEL_SIZE], activation=hparams[HP_ACTIVATION_CONV], padding=hparams[HP_CONV_PADDING])\n",
        "    r1=Bidirectional(GRU(hparams[HP_RNN_HIDDEN_DIM], return_sequences=True))\n",
        "    l2=Conv1D(hparams[HP_CONV_HIDDEN_DIM], hparams[HP_KERNEL_SIZE], activation=hparams[HP_ACTIVATION_CONV], padding=hparams[HP_CONV_PADDING])\n",
        "    r2=Bidirectional(GRU(hparams[HP_RNN_HIDDEN_DIM], return_sequences=True))\n",
        "    l3=Conv1D(hparams[HP_CONV_HIDDEN_DIM], hparams[HP_KERNEL_SIZE], activation=hparams[HP_ACTIVATION_CONV], padding=hparams[HP_CONV_PADDING])\n",
        "    r3=Bidirectional(GRU(hparams[HP_RNN_HIDDEN_DIM], return_sequences=True))\n",
        "    l4=Conv1D(hparams[HP_CONV_HIDDEN_DIM], hparams[HP_KERNEL_SIZE], activation=hparams[HP_ACTIVATION_CONV], padding=hparams[HP_CONV_PADDING])\n",
        "    r4=Bidirectional(GRU(hparams[HP_RNN_HIDDEN_DIM], return_sequences=True))\n",
        "    l5=Conv1D(hparams[HP_CONV_HIDDEN_DIM], hparams[HP_KERNEL_SIZE], activation=hparams[HP_ACTIVATION_CONV], padding=hparams[HP_CONV_PADDING])\n",
        "    r5=Bidirectional(GRU(hparams[HP_RNN_HIDDEN_DIM], return_sequences=True))\n",
        "    l6=Conv1D(hparams[HP_CONV_HIDDEN_DIM], hparams[HP_KERNEL_SIZE], activation=hparams[HP_ACTIVATION_CONV], padding=hparams[HP_CONV_PADDING])\n",
        "    \n",
        "    # Siamese architecture\n",
        "\n",
        "    ### 1st sibling\n",
        "\n",
        "    # 1st Block RCNN \n",
        "    s1=MaxPooling1D(hparams[HP_POOLING_KERNEL])(l1(seq_input1))\n",
        "    s1=concatenate([r1(s1), s1])\n",
        "\n",
        "    # 2nd Block RCNN\n",
        "    s1=MaxPooling1D(hparams[HP_POOLING_KERNEL])(l2(s1))\n",
        "    s1=concatenate([r2(s1), s1])\n",
        "\n",
        "    # 3rd Block RCNN\n",
        "    s1=MaxPooling1D(hparams[HP_POOLING_KERNEL])(l3(s1))\n",
        "    s1=concatenate([r3(s1), s1])\n",
        "\n",
        "    # 4th Block RCNN \n",
        "    s1=MaxPooling1D(hparams[HP_POOLING_KERNEL])(l4(s1))\n",
        "    s1=concatenate([r4(s1), s1])\n",
        "\n",
        "    # 5th Block RCNN\n",
        "    s1=MaxPooling1D(hparams[HP_POOLING_KERNEL])(l5(s1))\n",
        "    s1=concatenate([r5(s1), s1])\n",
        "    \n",
        "    # Last convolution\n",
        "    s1=l6(s1)\n",
        "    s1=GlobalAveragePooling1D()(s1)\n",
        "\n",
        "    ### 2nd sibling\n",
        "\n",
        "    # 1st block RCNN\n",
        "    s2=MaxPooling1D(hparams[HP_POOLING_KERNEL])(l1(seq_input2))\n",
        "    s2=concatenate([r1(s2), s2])\n",
        "\n",
        "    # 2nd block RCNN\n",
        "    s2=MaxPooling1D(hparams[HP_POOLING_KERNEL])(l2(s2))\n",
        "    s2=concatenate([r2(s2), s2])\n",
        "\n",
        "    # 3rd block RCNN\n",
        "    s2=MaxPooling1D(hparams[HP_POOLING_KERNEL])(l3(s2))\n",
        "    s2=concatenate([r3(s2), s2])\n",
        "\n",
        "    # 4th block RCNN\n",
        "    s2=MaxPooling1D(hparams[HP_POOLING_KERNEL])(l4(s2))\n",
        "    s2=concatenate([r4(s2), s2])\n",
        "\n",
        "    # 5th block RCNN\n",
        "    s2=MaxPooling1D(hparams[HP_POOLING_KERNEL])(l5(s2))\n",
        "    s2=concatenate([r5(s2), s2])\n",
        "\n",
        "    # Last convolution\n",
        "    s2=l6(s2)\n",
        "    s2=GlobalAveragePooling1D()(s2)\n",
        "\n",
        "    merged_text = multiply([s1, s2])\n",
        "\n",
        "    # ## Combine two siblings of siamese architecture\n",
        "    # abs_sub = tf.keras.layers.Lambda(lambda x: tf.abs(x))(s1-s2)\n",
        "    # merged_text = concatenate([s1, s2])\n",
        "    \n",
        "    # merged = Dense(100, activation='leaky_relu',  kernel_regularizer='l2')(merged_text)\n",
        "    # merged = Dropout(.2)(merged)\n",
        " \n",
        "    # merged_cos = Dense(100, activation='leaky_relu',  kernel_regularizer='l2')(tf.math.cos(math.pi*merged_text))\n",
        "    # merged_cos = Dropout(.2)(merged_cos)\n",
        "    \n",
        "    # merged_sin = Dense(100, activation='leaky_relu',  kernel_regularizer='l2')(tf.math.sin(math.pi*merged_text))\n",
        "    # merged_sin = Dropout(.2)(merged_sin)\n",
        "    \n",
        "    # merged_text = merged + merged_cos + merged_sin\n",
        "\n",
        "    #### MLP Part\n",
        "    # Set initializer\n",
        "    \n",
        "    # First dense\n",
        "    x = Dense(hparams[HP_FIRST_DENSE], activation=hparams[HP_ACTIVATION])(merged_text)\n",
        "    # x = tf.keras.layers.LeakyReLU(alpha=.3)(x)\n",
        "    x = Dropout(hparams[HP_DROPOUT])(x)\n",
        "\n",
        "    # Second dense\n",
        "    x = Dense(int((hparams[HP_CONV_HIDDEN_DIM]+7)/2), activation=hparams[HP_ACTIVATION])(x)\n",
        "    # x = tf.keras.layers.LeakyReLU(alpha=.3)(x)\n",
        "    x = Dropout(hparams[HP_DROPOUT])(x)\n",
        "\n",
        "    # Last softmax\n",
        "    main_output = Dense(2, activation='softmax')(x)\n",
        "\n",
        "    # Combine to form functional model\n",
        "    merge_model = Model(inputs=[seq_input1, seq_input2], outputs=[main_output])\n",
        "    return merge_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkG4si5QSf15"
      },
      "source": [
        "### Summary of model architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        },
        "id": "0wMXJo9lqd6l",
        "outputId": "39c18d3b-2aec-4875-f45e-060c893b4846"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-12e3b95290fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'model.png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36mplot_model\u001b[0;34m(model, to_file, show_shapes, show_dtype, show_layer_names, rankdir, expand_nested, dpi, layer_range, show_layer_activations)\u001b[0m\n\u001b[1;32m    424\u001b[0m       \u001b[0mdpi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdpi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m       \u001b[0mlayer_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer_range\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m       show_layer_activations=show_layer_activations)\n\u001b[0m\u001b[1;32m    427\u001b[0m   \u001b[0mto_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath_to_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mdot\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36mmodel_to_dot\u001b[0;34m(model, show_shapes, show_dtype, show_layer_names, rankdir, expand_nested, dpi, subgraph, layer_range, show_layer_activations)\u001b[0m\n\u001b[1;32m    308\u001b[0m       \u001b[0mnode_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_ib-'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mnode_key\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_network_nodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0minbound_layer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minbound_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m           \u001b[0minbound_layer_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minbound_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mexpand_nested\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/node.py\u001b[0m in \u001b[0;36minbound_layers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    270\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m     inbound_layers = tf.nest.map_structure(lambda t: t._keras_history.layer,\n\u001b[0;32m--> 272\u001b[0;31m                                         self.call_args[0])\n\u001b[0m\u001b[1;32m    273\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minbound_layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 869\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    870\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 869\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    870\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/node.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_input\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m     inbound_layers = tf.nest.map_structure(lambda t: t._keras_history.layer,\n\u001b[0m\u001b[1;32m    272\u001b[0m                                         self.call_args[0])\n\u001b[1;32m    273\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minbound_layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy_ops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnp_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m         np_config.enable_numpy_behavior()\"\"\".format(type(self).__name__, name))\n\u001b[0;32m--> 442\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute '_keras_history'"
          ]
        }
      ],
      "source": [
        "hparams = {\n",
        "  HP_EPSILON: EPSILON,\n",
        "  HP_LEARNING_RATE: LEARNING_RATE,\n",
        "  HP_FIRST_DENSE: 100,\n",
        "  HP_KERNEL_SIZE: 3,\n",
        "  HP_POOLING_KERNEL: 3,\n",
        "  HP_CONV_HIDDEN_DIM: 50,\n",
        "  HP_RNN_HIDDEN_DIM: 50,\n",
        "  HP_ACTIVATION: 'leaky_relu',\n",
        "  HP_ACTIVATION_CONV: 'relu',\n",
        "  HP_REGULARIZER: 0,\n",
        "  HP_CONV_PADDING: 'valid',\n",
        "  HP_DROPOUT: 3e-1,\n",
        "  HP_BATCH_SIZE: 256,\n",
        "  HP_LEAKY_RELU: 3e-1\n",
        "}\n",
        "\n",
        "model = build_model(hparams)\n",
        "tf.keras.utils.plot_model(model, to_file='model.png', show_shapes=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LjrzFALNxTZ"
      },
      "source": [
        "### Config Train-test process"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf my_best_model_*"
      ],
      "metadata": {
        "id": "foo3Wq7TrpC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "DObbKWD68hGR"
      },
      "outputs": [],
      "source": [
        "def train_test_model(hparams):\n",
        "  training_time = 1\n",
        "  num_hit = 0.\n",
        "  num_total = 0.\n",
        "  num_pos = 0.\n",
        "  num_true_pos = 0.\n",
        "  num_false_pos = 0.\n",
        "  num_true_neg = 0.\n",
        "  num_false_neg = 0.\n",
        "  for train, test in train_test:\n",
        "      merge_model = None\n",
        "      merge_model = build_model(hparams)  \n",
        "\n",
        "\n",
        "      merge_model.compile(optimizer=Adam(learning_rate=hparams[HP_LEARNING_RATE], amsgrad=True, epsilon=hparams[HP_EPSILON]), loss='categorical_crossentropy', metrics=METRICS)\n",
        "      # Create train\n",
        "      train_dataset = tf.data.Dataset.from_generator(generator_pair, args=[train], output_types=({\"seq1\": DTYPE, \"seq2\": DTYPE}, DTYPE), output_shapes = ({\"seq1\": (seq_size, dim), \"seq2\": (seq_size, dim)}, (2,)) )\n",
        "      train_dataset = train_dataset.shuffle(256).repeat(N_EPOCHS).batch(hparams[HP_BATCH_SIZE])\n",
        "      train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "\n",
        "      # Create val\n",
        "      val_dataset = tf.data.Dataset.from_generator(generator_pair, args=[test], output_types=({\"seq1\": DTYPE, \"seq2\": DTYPE}, DTYPE), output_shapes = ({\"seq1\": (seq_size, dim), \"seq2\": (seq_size, dim)}, (2,)) )\n",
        "      val_dataset = val_dataset.batch(hparams[HP_BATCH_SIZE])\n",
        "      val_dataset = val_dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "      # Save the best model base on val_accuracy\n",
        "      checkpoint = ModelCheckpoint(filepath=f'my_best_model_{training_time}.hdf5', monitor='val_accuracy',verbose=1, save_best_only=True, mode='max')\n",
        "      # Fit model\n",
        "      print(f'==================== Training time  {training_time} =====================')\n",
        "      \n",
        "      # Log learning curve to each training time\n",
        "      history = {}\n",
        "      history[f'training time {training_time}'] = merge_model.fit(train_dataset, steps_per_epoch=len(train)//hparams[HP_BATCH_SIZE], epochs=N_EPOCHS, validation_data=val_dataset, callbacks=[checkpoint, reduce_lr, early_stopping], shuffle=True)\n",
        "      # merge_model.fit([seq_tensor[seq_index1[train]], seq_tensor[seq_index2[train]]], class_labels[train], batch_size = hparams[HP_BATCH_SIZE], epochs=N_EPOCHS, validation_data=([seq_tensor[seq_index1[test]], seq_tensor[seq_index2[test]]], class_labels[test]), callbacks=[checkpoint, reduce_lr])\n",
        "\n",
        "      print(f'==================End training {training_time}========================')\n",
        "      # # Create test\n",
        "      # test_dataset = tf.data.Dataset.from_generator(generator_pair, args=[test], output_types=({\"seq1\": DTYPE, \"seq2\": DTYPE}, DTYPE), output_shapes = ({\"seq1\": (seq_size, dim), \"seq2\": (seq_size, dim)}, (2,)) )\n",
        "      # test_dataset = test_dataset.batch(hparams[HP_BATCH_SIZE])\n",
        "      # res = merge_model.evaluate(test_dataset)\n",
        "      # Create pred\n",
        "      pred_dataset = tf.data.Dataset.from_generator(generator_pair_predict, args=[test], output_types=({\"seq1\": DTYPE, \"seq2\": DTYPE}), output_shapes = ({\"seq1\": (seq_size, dim), \"seq2\": (seq_size, dim)}) )\n",
        "      pred_dataset = pred_dataset.batch(BATCH_SIZE)\n",
        "      pred_dataset = pred_dataset.prefetch(tf.data.AUTOTUNE)\n",
        "      pred = merge_model.predict(pred_dataset)\n",
        "\n",
        "      # Performance metrics\n",
        "      for i in range(len(class_labels[test])):\n",
        "          num_total += 1\n",
        "          if np.argmax(class_labels[test][i]) == np.argmax(pred[i]):\n",
        "              num_hit += 1\n",
        "          if class_labels[test][i][0] > 0.:\n",
        "              num_pos += 1.\n",
        "              if pred[i][0] > pred[i][1]:\n",
        "                  num_true_pos += 1\n",
        "              else:\n",
        "                  num_false_neg += 1\n",
        "          else:\n",
        "              if pred[i][0] > pred[i][1]:\n",
        "                  num_false_pos += 1\n",
        "              else:\n",
        "                  num_true_neg += 1\n",
        "      accuracy = num_hit / num_total\n",
        "      prec = num_true_pos / (num_true_pos + num_false_pos)\n",
        "      recall = num_true_pos / num_pos\n",
        "      spec = num_true_neg / (num_true_neg + num_false_neg)\n",
        "      f1 = 2. * prec * recall / (prec + recall)\n",
        "      # mcc = (num_true_pos * num_true_neg - num_false_pos * num_false_neg) / ((num_true_pos + num_true_neg) * (num_true_pos + num_false_neg) * (num_false_pos + num_true_neg) * (num_false_pos + num_false_neg)) ** 0.5\n",
        "      mcc = (num_true_pos * num_true_neg - num_false_pos * num_false_neg) / ((num_true_pos + num_false_pos) * (num_true_pos + num_false_neg) * (num_true_neg + num_false_pos) * (num_true_neg + num_false_neg)) ** 0.5\n",
        "      training_time += 1\n",
        "      print (f'accuracy: {accuracy}, precision: {prec}, recall: {recall}, specificity: {spec}, mcc: {mcc} ,f1-score: {f1}')\n",
        "  return accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hUrIncXN1vp"
      },
      "source": [
        "### Log configurations and accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "0Frknl0rEDtm"
      },
      "outputs": [],
      "source": [
        "def run(run_dir, hparams):\n",
        "  with tf.summary.create_file_writer(run_dir).as_default():\n",
        "    hp.hparams(hparams)  # record the values used in this trial\n",
        "    accuracy = train_test_model(hparams)\n",
        "    tf.summary.scalar(METRIC_ACCURACY, accuracy, step=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ONZlF6POBUX"
      },
      "source": [
        "### Train the neural network through all configurations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 195,
      "metadata": {
        "id": "_Rz0FeXKSNbK"
      },
      "outputs": [],
      "source": [
        "# from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
        "# for i in range(1441):\n",
        "#   scaler = StandardScaler().fit(seq_tensor[i])\n",
        "\n",
        "#   # scaler = RobustScaler().fit(X)\n",
        "#   seq_tensor[i]= scaler.transform(seq_tensor[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63YbvCPBGBNB",
        "outputId": "7a217065-d0d6-40cb-dcb1-ae2bcb0d07c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting trial: run-0\n",
            "{'epsilon': 1e-06, 'learning_rate': 0.001, 'first_dense': 100, 'kernel_size': 3, 'pooling_kernel': 3, 'conv_hidden_dim': 50, 'rnn_hidden_dim': 50, 'activation': 'leaky_relu', 'activation_conv': 'linear', 'regularizer': 0, 'conv_padding': 'valid', 'dropout': 0.2, 'batch_size': 32, 'leaky_relu': 0.3}\n",
            "==================== Training time  1 =====================\n",
            "Epoch 1/80\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.6703 - accuracy: 0.6072 - precision: 0.6072 - recall: 0.6072 - mcc: 0.2190 - f1-score: 0.6007 - auc: 0.6413 - prc: 0.6239\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.61921, saving model to my_best_model_1.hdf5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  layer_config = serialize_layer_fn(layer)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "72/72 [==============================] - 72s 263ms/step - loss: 0.6703 - accuracy: 0.6072 - precision: 0.6072 - recall: 0.6072 - mcc: 0.2190 - f1-score: 0.6007 - auc: 0.6413 - prc: 0.6239 - val_loss: 0.6470 - val_accuracy: 0.6192 - val_precision: 0.6192 - val_recall: 0.6192 - val_mcc: 0.2367 - val_f1-score: 0.6173 - val_auc: 0.6789 - val_prc: 0.6678 - lr: 0.0010\n",
            "Epoch 2/80\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.6630 - accuracy: 0.6241 - precision: 0.6241 - recall: 0.6241 - mcc: 0.2525 - f1-score: 0.6196 - auc: 0.6501 - prc: 0.6359\n",
            "Epoch 00002: val_accuracy improved from 0.61921 to 0.63293, saving model to my_best_model_1.hdf5\n",
            "72/72 [==============================] - 10s 144ms/step - loss: 0.6630 - accuracy: 0.6241 - precision: 0.6241 - recall: 0.6241 - mcc: 0.2525 - f1-score: 0.6196 - auc: 0.6501 - prc: 0.6359 - val_loss: 0.6508 - val_accuracy: 0.6329 - val_precision: 0.6329 - val_recall: 0.6329 - val_mcc: 0.2898 - val_f1-score: 0.6253 - val_auc: 0.6675 - val_prc: 0.6575 - lr: 0.0010\n",
            "Epoch 3/80\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.6446 - accuracy: 0.6584 - precision: 0.6584 - recall: 0.6584 - mcc: 0.3199 - f1-score: 0.6555 - auc: 0.6897 - prc: 0.6695\n",
            "Epoch 00003: val_accuracy improved from 0.63293 to 0.64837, saving model to my_best_model_1.hdf5\n",
            "72/72 [==============================] - 10s 141ms/step - loss: 0.6446 - accuracy: 0.6584 - precision: 0.6584 - recall: 0.6584 - mcc: 0.3199 - f1-score: 0.6555 - auc: 0.6897 - prc: 0.6695 - val_loss: 0.6286 - val_accuracy: 0.6484 - val_precision: 0.6484 - val_recall: 0.6484 - val_mcc: 0.3060 - val_f1-score: 0.6467 - val_auc: 0.6987 - val_prc: 0.6909 - lr: 0.0010\n",
            "Epoch 4/80\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.6128 - accuracy: 0.6684 - precision: 0.6684 - recall: 0.6684 - mcc: 0.3388 - f1-score: 0.6670 - auc: 0.7324 - prc: 0.7182\n",
            "Epoch 00004: val_accuracy improved from 0.64837 to 0.70669, saving model to my_best_model_1.hdf5\n",
            "72/72 [==============================] - 11s 153ms/step - loss: 0.6128 - accuracy: 0.6684 - precision: 0.6684 - recall: 0.6684 - mcc: 0.3388 - f1-score: 0.6670 - auc: 0.7324 - prc: 0.7182 - val_loss: 0.5890 - val_accuracy: 0.7067 - val_precision: 0.7067 - val_recall: 0.7067 - val_mcc: 0.4132 - val_f1-score: 0.7066 - val_auc: 0.7622 - val_prc: 0.7482 - lr: 0.0010\n",
            "Epoch 5/80\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.5699 - accuracy: 0.7144 - precision: 0.7144 - recall: 0.7144 - mcc: 0.4300 - f1-score: 0.7128 - auc: 0.7810 - prc: 0.7704\n",
            "Epoch 00005: val_accuracy did not improve from 0.70669\n",
            "72/72 [==============================] - 10s 138ms/step - loss: 0.5699 - accuracy: 0.7144 - precision: 0.7144 - recall: 0.7144 - mcc: 0.4300 - f1-score: 0.7128 - auc: 0.7810 - prc: 0.7704 - val_loss: 0.6505 - val_accuracy: 0.6707 - val_precision: 0.6707 - val_recall: 0.6707 - val_mcc: 0.4068 - val_f1-score: 0.6526 - val_auc: 0.7354 - val_prc: 0.7345 - lr: 0.0010\n",
            "Epoch 6/80\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.4930 - accuracy: 0.7739 - precision: 0.7739 - recall: 0.7739 - mcc: 0.5477 - f1-score: 0.7739 - auc: 0.8445 - prc: 0.8397\n",
            "Epoch 00006: val_accuracy improved from 0.70669 to 0.82161, saving model to my_best_model_1.hdf5\n",
            "72/72 [==============================] - 10s 142ms/step - loss: 0.4930 - accuracy: 0.7739 - precision: 0.7739 - recall: 0.7739 - mcc: 0.5477 - f1-score: 0.7739 - auc: 0.8445 - prc: 0.8397 - val_loss: 0.4213 - val_accuracy: 0.8216 - val_precision: 0.8216 - val_recall: 0.8216 - val_mcc: 0.6428 - val_f1-score: 0.8214 - val_auc: 0.8925 - val_prc: 0.8888 - lr: 0.0010\n",
            "Epoch 7/80\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.3881 - accuracy: 0.8290 - precision: 0.8290 - recall: 0.8290 - mcc: 0.6580 - f1-score: 0.8289 - auc: 0.9077 - prc: 0.9065\n",
            "Epoch 00007: val_accuracy did not improve from 0.82161\n",
            "72/72 [==============================] - 10s 138ms/step - loss: 0.3881 - accuracy: 0.8290 - precision: 0.8290 - recall: 0.8290 - mcc: 0.6580 - f1-score: 0.8289 - auc: 0.9077 - prc: 0.9065 - val_loss: 0.4303 - val_accuracy: 0.8199 - val_precision: 0.8199 - val_recall: 0.8199 - val_mcc: 0.6488 - val_f1-score: 0.8195 - val_auc: 0.8850 - val_prc: 0.8812 - lr: 0.0010\n",
            "Epoch 8/80\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.3494 - accuracy: 0.8555 - precision: 0.8555 - recall: 0.8555 - mcc: 0.7109 - f1-score: 0.8555 - auc: 0.9259 - prc: 0.9250\n",
            "Epoch 00008: val_accuracy did not improve from 0.82161\n",
            "72/72 [==============================] - 10s 137ms/step - loss: 0.3494 - accuracy: 0.8555 - precision: 0.8555 - recall: 0.8555 - mcc: 0.7109 - f1-score: 0.8555 - auc: 0.9259 - prc: 0.9250 - val_loss: 0.4564 - val_accuracy: 0.8027 - val_precision: 0.8027 - val_recall: 0.8027 - val_mcc: 0.6213 - val_f1-score: 0.7981 - val_auc: 0.8737 - val_prc: 0.8723 - lr: 0.0010\n",
            "Epoch 9/80\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.3045 - accuracy: 0.8724 - precision: 0.8724 - recall: 0.8724 - mcc: 0.7447 - f1-score: 0.8724 - auc: 0.9451 - prc: 0.9427\n",
            "Epoch 00009: val_accuracy did not improve from 0.82161\n",
            "72/72 [==============================] - 10s 136ms/step - loss: 0.3045 - accuracy: 0.8724 - precision: 0.8724 - recall: 0.8724 - mcc: 0.7447 - f1-score: 0.8724 - auc: 0.9451 - prc: 0.9427 - val_loss: 0.4361 - val_accuracy: 0.8216 - val_precision: 0.8216 - val_recall: 0.8216 - val_mcc: 0.6469 - val_f1-score: 0.8215 - val_auc: 0.8938 - val_prc: 0.8851 - lr: 0.0010\n",
            "Epoch 10/80\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.2856 - accuracy: 0.8798 - precision: 0.8798 - recall: 0.8798 - mcc: 0.7597 - f1-score: 0.8798 - auc: 0.9504 - prc: 0.9501\n",
            "Epoch 00010: val_accuracy did not improve from 0.82161\n",
            "72/72 [==============================] - 10s 137ms/step - loss: 0.2856 - accuracy: 0.8798 - precision: 0.8798 - recall: 0.8798 - mcc: 0.7597 - f1-score: 0.8798 - auc: 0.9504 - prc: 0.9501 - val_loss: 0.5258 - val_accuracy: 0.8010 - val_precision: 0.8010 - val_recall: 0.8010 - val_mcc: 0.6162 - val_f1-score: 0.8000 - val_auc: 0.8768 - val_prc: 0.8734 - lr: 0.0010\n",
            "Epoch 11/80\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.2575 - accuracy: 0.8919 - precision: 0.8919 - recall: 0.8919 - mcc: 0.7841 - f1-score: 0.8919 - auc: 0.9598 - prc: 0.9597\n",
            "Epoch 00011: val_accuracy improved from 0.82161 to 0.82847, saving model to my_best_model_1.hdf5\n",
            "Restoring model weights from the end of the best epoch: 6.\n",
            "72/72 [==============================] - 10s 139ms/step - loss: 0.2575 - accuracy: 0.8919 - precision: 0.8919 - recall: 0.8919 - mcc: 0.7841 - f1-score: 0.8919 - auc: 0.9598 - prc: 0.9597 - val_loss: 0.4375 - val_accuracy: 0.8285 - val_precision: 0.8285 - val_recall: 0.8285 - val_mcc: 0.6588 - val_f1-score: 0.8274 - val_auc: 0.9011 - val_prc: 0.8927 - lr: 0.0010\n",
            "Epoch 00011: early stopping\n",
            "==================End training 1========================\n",
            "accuracy: 0.8216123499142367, precision: 0.8223684210526315, recall: 0.8333333333333334, specificity: 0.8207885304659498, mcc: 0.6428387032637988 ,f1-score: 0.8278145695364237\n",
            "==================== Training time  2 =====================\n",
            "Epoch 1/80\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.6913 - accuracy: 0.5958 - precision: 0.5958 - recall: 0.5958 - mcc: 0.1942 - f1-score: 0.5920 - auc: 0.6615 - prc: 0.6749\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.63293, saving model to my_best_model_2.hdf5\n",
            "72/72 [==============================] - 56s 250ms/step - loss: 0.6913 - accuracy: 0.5958 - precision: 0.5958 - recall: 0.5958 - mcc: 0.1942 - f1-score: 0.5920 - auc: 0.6615 - prc: 0.6749 - val_loss: 0.6548 - val_accuracy: 0.6329 - val_precision: 0.6329 - val_recall: 0.6329 - val_mcc: 0.2672 - val_f1-score: 0.6325 - val_auc: 0.6747 - val_prc: 0.6718 - lr: 0.0010\n",
            "Epoch 2/80\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.6751 - accuracy: 0.5881 - precision: 0.5881 - recall: 0.5881 - mcc: 0.1870 - f1-score: 0.5763 - auc: 0.6209 - prc: 0.6060\n",
            "Epoch 00002: val_accuracy improved from 0.63293 to 0.64666, saving model to my_best_model_2.hdf5\n",
            "72/72 [==============================] - 10s 139ms/step - loss: 0.6751 - accuracy: 0.5881 - precision: 0.5881 - recall: 0.5881 - mcc: 0.1870 - f1-score: 0.5763 - auc: 0.6209 - prc: 0.6060 - val_loss: 0.6452 - val_accuracy: 0.6467 - val_precision: 0.6467 - val_recall: 0.6467 - val_mcc: 0.3278 - val_f1-score: 0.6307 - val_auc: 0.6870 - val_prc: 0.6740 - lr: 0.0010\n",
            "Epoch 3/80\n",
            "72/72 [==============================] - ETA: 0s - loss: 0.6510 - accuracy: 0.6363 - precision: 0.6363 - recall: 0.6363 - mcc: 0.2731 - f1-score: 0.6356 - auc: 0.6712 - prc: 0.6566\n",
            "Epoch 00003: val_accuracy did not improve from 0.64666\n",
            "72/72 [==============================] - 10s 137ms/step - loss: 0.6510 - accuracy: 0.6363 - precision: 0.6363 - recall: 0.6363 - mcc: 0.2731 - f1-score: 0.6356 - auc: 0.6712 - prc: 0.6566 - val_loss: 0.6335 - val_accuracy: 0.6467 - val_precision: 0.6467 - val_recall: 0.6467 - val_mcc: 0.2934 - val_f1-score: 0.6462 - val_auc: 0.7013 - val_prc: 0.6979 - lr: 0.0010\n",
            "Epoch 4/80\n",
            "65/72 [==========================>...] - ETA: 0s - loss: 0.6375 - accuracy: 0.6596 - precision: 0.6596 - recall: 0.6596 - mcc: 0.3238 - f1-score: 0.6566 - auc: 0.6975 - prc: 0.6800"
          ]
        }
      ],
      "source": [
        "# Remove log directory from last run\n",
        "%%time\n",
        "!rm -rf ./logs\n",
        "session_num = 0\n",
        "seq_size=seq_tensor.shape[1]\n",
        "dim=seq_tensor.shape[2]\n",
        "for epsilon in HP_EPSILON.domain.values:\n",
        "  for learning_rate in HP_LEARNING_RATE.domain.values:\n",
        "    for first_dense in HP_FIRST_DENSE.domain.values:\n",
        "      for kernel_size in HP_KERNEL_SIZE.domain.values:\n",
        "        for pooling_kernel in HP_POOLING_KERNEL.domain.values:\n",
        "          for conv_hidden_dim in HP_CONV_HIDDEN_DIM.domain.values:\n",
        "            for rnn_hidden_dim in HP_RNN_HIDDEN_DIM.domain.values:\n",
        "              for activation in HP_ACTIVATION.domain.values:\n",
        "                for activation_conv in HP_ACTIVATION_CONV.domain.values:\n",
        "                  for regularizer in HP_REGULARIZER.domain.values:\n",
        "                    for conv_padding in HP_CONV_PADDING.domain.values:\n",
        "                      for dropout in HP_DROPOUT.domain.values:\n",
        "                        for batch_size in HP_BATCH_SIZE.domain.values:\n",
        "                          for leaky_relu in HP_LEAKY_RELU.domain.values:\n",
        "                            hparams = {\n",
        "                                HP_EPSILON: epsilon,\n",
        "                                HP_LEARNING_RATE: learning_rate,\n",
        "                                HP_FIRST_DENSE: first_dense,\n",
        "                                HP_KERNEL_SIZE: kernel_size,\n",
        "                                HP_POOLING_KERNEL: pooling_kernel,\n",
        "                                HP_CONV_HIDDEN_DIM: conv_hidden_dim,\n",
        "                                HP_RNN_HIDDEN_DIM: rnn_hidden_dim,\n",
        "                                HP_ACTIVATION: activation,\n",
        "                                HP_ACTIVATION_CONV: activation_conv,\n",
        "                                HP_REGULARIZER: regularizer,\n",
        "                                HP_CONV_PADDING: conv_padding,\n",
        "                                HP_DROPOUT: dropout,\n",
        "                                HP_BATCH_SIZE: batch_size,\n",
        "                                HP_LEAKY_RELU: leaky_relu\n",
        "                            }\n",
        "                            run_name = \"run-%d\" % session_num\n",
        "                            print('--- Starting trial: %s' % run_name)\n",
        "                            print({h.name: hparams[h] for h in hparams})\n",
        "                            run('logs/hparam_tuning/' + run_name, hparams)\n",
        "                            session_num += 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jonPYQx6-PGj"
      },
      "source": [
        "### Extract last hidden layer for input of LGBM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 197,
      "metadata": {
        "id": "W6JYKHUj7UsD"
      },
      "outputs": [],
      "source": [
        "################################Intermediate Layer prediction (Abstraction features extraction)######################################\n",
        "model = tf.keras.models.load_model('/content/my_best_model_1.hdf5')\n",
        "intermediate_layer_model = Model(inputs=model.input,outputs=model.get_layer(model.layers[-3].name).output)\n",
        "\n",
        "# Use intermediate layer to transform pairs matrix\n",
        "intermediate_output_p1 = intermediate_layer_model.predict([seq_tensor[seq_index1],seq_tensor[seq_index2]])  \n",
        "\n",
        "\n",
        "p_merge=pd.DataFrame(intermediate_output_p1)    \n",
        "Trainlabels = np.zeros((len(class_labels),))\n",
        "for i in range(len(class_labels)):\n",
        "  if np.allclose(np.array([1., 0.]), class_labels[i]):\n",
        "    Trainlabels[i] = 1.\n",
        "# create dataframe use transformed pairs matrix outputs and labels\n",
        "X_train_feat=pd.concat((p_merge,pd.DataFrame(pd.DataFrame(Trainlabels))),axis=1,ignore_index=True)\n",
        "\n",
        "# write to file dataframe of transformed pairs matrix and labels\n",
        "X_train_feat.to_csv('X_train.csv',header=False, index=False)\n",
        "\n",
        "# read dataframe of transformed pairs matrix and labels\n",
        "Train=pd.read_csv(\"X_train.csv\",header=None)\n",
        "Train=Train.sample(frac=1)\n",
        "\n",
        "X=Train.iloc[:,0:28].values\n",
        "y=Train.iloc[:,28:].values\n",
        "\n",
        "extracted_df=X_train_feat\n",
        "\n",
        "scaler=RobustScaler()\n",
        "# scaler=MinMaxScaler()\n",
        "# scaler = StandardScaler()\n",
        "X=scaler.fit_transform(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Bqa_mrke8dP"
      },
      "source": [
        "### Cross-validation with PIPR-LGBM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 199,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJFn3Uk0Z83o",
        "outputId": "3f030346-bfb6-431b-eb42-384868f608f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========================== START TRAINING TIME 1 ===========================\n",
            "========================== PERFOMANCE METRICS AT TRAINING TIME 1 ===========================\n",
            "accuracy: 0.9725557461406518, specificity: 0.9640522875816994, sensitivity: 0.9819494584837545, precision= 0.9611307420494699, recall: 0.9819494584837545 \n",
            "========================== START TRAINING TIME 2 ===========================\n",
            "========================== PERFOMANCE METRICS AT TRAINING TIME 2 ===========================\n",
            "accuracy: 0.9536878216123499, specificity: 0.9560439560439561, sensitivity: 0.9516129032258065, precision= 0.9609120521172638, recall: 0.9516129032258065 \n",
            "========================== START TRAINING TIME 3 ===========================\n",
            "========================== PERFOMANCE METRICS AT TRAINING TIME 3 ===========================\n",
            "accuracy: 0.9725557461406518, specificity: 0.9715302491103203, sensitivity: 0.9735099337748344, precision= 0.9735099337748344, recall: 0.9735099337748344 \n",
            "========================== START TRAINING TIME 4 ===========================\n",
            "========================== PERFOMANCE METRICS AT TRAINING TIME 4 ===========================\n",
            "accuracy: 0.9708404802744426, specificity: 0.976897689768977, sensitivity: 0.9642857142857143, precision= 0.9747292418772563, recall: 0.9642857142857143 \n",
            "========================== START TRAINING TIME 5 ===========================\n",
            "========================== PERFOMANCE METRICS AT TRAINING TIME 5 ===========================\n",
            "accuracy: 0.9622641509433962, specificity: 0.9593220338983051, sensitivity: 0.9652777777777778, precision= 0.9586206896551724, recall: 0.9652777777777778 \n",
            "====================================== END CROSS VALIDATION ===============================\n",
            "\n",
            "Accuracy:0.9663807890222984 Â± 0.007405500221242852\n",
            "Accuracy_Var:0.9663807890222984 Â± 5.484143352682793e-05\n",
            "Specificity:0.9655692432806514 Â± 0.0076932069638247945\n",
            "Sensitivity:0.9673271575095775 Â± 0.010123502654612695\n",
            "Precison:0.9657805318947995 Â± 0.0068761769264550025\n",
            "Recall:0.9673271575095775 Â± 0.010123502654612695\n",
            "MCC:0.9327089831643922 Â± 0.014900827490324512\n",
            "AUC:0.9806414584906804\n"
          ]
        }
      ],
      "source": [
        "##################################### Five-fold Cross-Validation ##########################################################\n",
        "kf=KFold(n_splits=5, random_state=42, shuffle=True)\n",
        "\n",
        "\n",
        "accuracy = []\n",
        "specificity = []\n",
        "sensitivity = []\n",
        "precision=[]\n",
        "recall=[]\n",
        "m_coef=[]\n",
        "\n",
        "auc_list=[]\n",
        "xgb_fpr_list=[]\n",
        "xgb_tpr_list=[]\n",
        "o=0\n",
        "max_accuracy=float(\"-inf\")\n",
        "xgb_fpr=None\n",
        "xgb_tpr=None\n",
        "\n",
        "y = y.reshape(-1, )\n",
        "training_time = 1\n",
        "for train, test in kf.split(X,y):\n",
        "    o=o+1\n",
        "    model_=LGBMClassifier(learning_rate=.2, gamma=0, max_depth=10, n_estimators=1000)\n",
        "    # model_=XGBClassifier(n_estimators=200)\n",
        "    print(f\"========================== START TRAINING TIME {training_time} ===========================\")\n",
        "    hist=model_.fit(X[train], y[train],eval_set=[(X[test], y[test])],verbose=False)\n",
        "    # hist = lgb.train(param, train_data, num_round, valid_sets=[validation_data])\n",
        "    y_score=model_.predict_proba(X[test])\n",
        "    y_test=tf.keras.utils.to_categorical(y[test]) \n",
        "    \n",
        "    # fpr, tpr, _ = roc_curve(y_test[:,0].ravel(), y_score[:,0].ravel())\n",
        "    auc = metrics.roc_auc_score(y_test, y_score)\n",
        "    auc_list.append(auc)\n",
        "    coef=matthews_corrcoef(y_test.argmax(axis=1), y_score.argmax(axis=1), sample_weight=None)\n",
        "    m_coef.append(coef)\n",
        "    \n",
        "    cm1=confusion_matrix(y_test.argmax(axis=1), y_score.argmax(axis=1))\n",
        "    acc = (cm1[0,0]+cm1[1,1])/(cm1[0,0]+cm1[0,1]+cm1[1,0]+cm1[1,1])\n",
        "    spec= (cm1[0,0])/(cm1[0,0]+cm1[0,1])\n",
        "    sens = (cm1[1,1])/(cm1[1,0]+cm1[1,1])\n",
        "    prec=cm1[1,1]/(cm1[1,1]+cm1[0,1])\n",
        "    rec=cm1[1,1]/(cm1[1,1]+cm1[1,0])\n",
        "    sensitivity.append(sens)\n",
        "    specificity.append(spec)\n",
        "    accuracy.append(acc)\n",
        "    precision.append(prec)\n",
        "    recall.append(rec)\n",
        "    # xgb_fpr_list.append(fpr)\n",
        "    # xgb_tpr_list.append(tpr)\n",
        "    print(f\"========================== PERFOMANCE METRICS AT TRAINING TIME {training_time} ===========================\")\n",
        "    print(f\"accuracy: {acc}, specificity: {spec}, sensitivity: {sens}, precision= {prec}, recall: {rec} \")\n",
        "    # if max_accuracy<acc:\n",
        "    #     max_accuracy=acc\n",
        "    #     xgb_fpr=fpr\n",
        "    #     xgb_tpr=tpr\n",
        "    training_time += 1\n",
        "        \n",
        "print(\"====================================== END CROSS VALIDATION ===============================\\n\")\n",
        "xgb_fpr=pd.DataFrame(xgb_fpr)\n",
        "xgb_tpr=pd.DataFrame(xgb_tpr)\n",
        "\n",
        "xgb_fpr.to_csv('fprdnn_xgb.csv',header=False, index=False)\n",
        "xgb_tpr.to_csv('tprdnn_xgb.csv',header=False, index=False)   \n",
        " \n",
        "mean_acc=np.mean(accuracy)\n",
        "std_acc=np.std(accuracy)\n",
        "var_acc=np.var(accuracy)\n",
        "print(\"Accuracy:\"+str(mean_acc)+\" Â± \"+str(std_acc))\n",
        "print(\"Accuracy_Var:\"+str(mean_acc)+\" Â± \"+str(var_acc))\n",
        "mean_spec=np.mean(specificity)\n",
        "std_spec=np.std(specificity)\n",
        "print(\"Specificity:\"+str(mean_spec)+\" Â± \"+str(std_spec))\n",
        "mean_sens=np.mean(sensitivity)\n",
        "std_sens=np.std(sensitivity)\n",
        "print(\"Sensitivity:\"+str(mean_sens)+\" Â± \"+str(std_sens))\n",
        "mean_prec=np.mean(precision)\n",
        "std_prec=np.std(precision)\n",
        "print(\"Precison:\"+str(mean_prec)+\" Â± \"+str(std_prec))\n",
        "mean_rec=np.mean(recall)\n",
        "std_rec=np.std(recall)\n",
        "print(\"Recall:\"+str(mean_rec)+\" Â± \"+str(std_rec))\n",
        "mean_coef=np.mean(m_coef)\n",
        "std_coef=np.std(m_coef)\n",
        "print(\"MCC:\"+str(mean_coef)+\" Â± \"+str(std_coef))\n",
        "\n",
        "print(\"AUC:\"+str(np.mean(auc_list)))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-dslaN3fD32"
      },
      "source": [
        "### t-sne visualization of last hiden layer extracted by PIPR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_3asJh5ZtZ5"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "def TSNE_extracted():\n",
        "    \n",
        "    pos=extracted_df[extracted_df.iloc[:,-1]==1]\n",
        "    neg=extracted_df[extracted_df.iloc[:,-1]==0]\n",
        "    X_feat=pd.concat([pos,neg])\n",
        "    X_feat=X_feat.iloc[:,:-1]\n",
        "    t=TSNE(n_components=2).fit_transform(X_feat)\n",
        "    pos_t=t[:int(len(t)/2),:]\n",
        "    neg_t=t[int(len(t)/2):,:]\n",
        "    plt.scatter(pos_t[:,0],pos_t[:,1],label=\"Positive\",s=4)\n",
        "    plt.scatter(neg_t[:,0],neg_t[:,1],label=\"Negative\",s=4)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "TSNE_extracted()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhjyFk1XOHNJ"
      },
      "source": [
        "### Tensorboard monitor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sjolcDW-w5jY"
      },
      "outputs": [],
      "source": [
        "# %reload_ext tensorboard\n",
        "# %tensorboard --logdir=/content/logs"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "PIPR-LGBM.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}