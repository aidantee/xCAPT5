{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HH-e5IC_hGa"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-FZrex4_i2z",
        "outputId": "3461e7b3-b47b-4556-a772-60d36a4fb14c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.8/dist-packages (2.2.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from lightgbm) (1.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from lightgbm) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from lightgbm) (1.7.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->lightgbm) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->lightgbm) (3.1.0)\n",
            "--2022-12-21 16:29:36--  https://raw.githubusercontent.com/anhvt00/PIPR/master/embeddings/seq2tensor.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1104 (1.1K) [text/plain]\n",
            "Saving to: ‘seq2tensor.py’\n",
            "\n",
            "seq2tensor.py       100%[===================>]   1.08K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-12-21 16:29:36 (51.9 MB/s) - ‘seq2tensor.py’ saved [1104/1104]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!pip install lightgbm\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "!wget https://raw.githubusercontent.com/anhvt00/PIPR/master/embeddings/seq2tensor.py\n",
        "from seq2tensor import s2t\n",
        "import os\n",
        "from tensorflow.keras.layers import Dense, Input, Dropout\n",
        "from tensorflow.keras.layers import concatenate, multiply\n",
        "from tensorflow.keras.optimizers import SGD, Adam\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.regularizers import l2, l1_l2\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_curve, precision_recall_curve\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import sklearn.metrics as metrics\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import matthews_corrcoef,accuracy_score, precision_score,recall_score\n",
        "from sklearn.manifold import TSNE\n",
        "from lightgbm import LGBMClassifier\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "import lightgbm as lgb\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kiTiIFTn8Uyz"
      },
      "source": [
        "### Conjoint Triad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w1QbOrTYesLt"
      },
      "outputs": [],
      "source": [
        "\"\"\"Implementation of CT coding method\n",
        "\"\"\"\n",
        "\n",
        "__all__ = ['ct_code_of']\n",
        "\n",
        "# AAC: Classification of amino acids.\n",
        "AAC = {\n",
        "    '1': ['A', 'G', 'V'],\n",
        "    '2': ['I', 'L', 'F', 'P'],\n",
        "    '3': ['Y', 'M', 'T', 'S'],\n",
        "    '4': ['H', 'N', 'Q', 'W'],\n",
        "    '5': ['R', 'K'],\n",
        "    '6': ['D', 'E'],\n",
        "    '7': ['C']\n",
        "}\n",
        "\n",
        "# AAC_R: Reverse of AAC.\n",
        "AAC_R = {}\n",
        "for C, AAS in AAC.items():\n",
        "    for AA in AAS:\n",
        "        AAC_R[AA] = C\n",
        "\n",
        "def classification_of(AA):\n",
        "    \"\"\"Get classification of amino acids.\"\"\"\n",
        "    return AAC_R[AA]\n",
        "\n",
        "def classification_sequence_of(PS):\n",
        "    \"\"\"Make classification sequence from protein sequence.\"\"\"\n",
        "    CS = ''\n",
        "    for I, CH in enumerate(PS):\n",
        "        if CH == 'U':\n",
        "          continue\n",
        "        CS = CS + classification_of(CH)\n",
        "    return CS\n",
        "\n",
        "def ct_code_of(PS):\n",
        "    \"\"\"Get CT Code of protein sequence.\"\"\"\n",
        "    CT_Code = [0]*343\n",
        "    CS = classification_sequence_of(PS)\n",
        "    for I in range(len(CS)-2):\n",
        "        SubCS = CS[I:I+3]\n",
        "        CT_Code_Index = int(SubCS[0]) + (int(SubCS[1])-1)*7 + (int(SubCS[2])-1)*7*7\n",
        "        CT_Code[CT_Code_Index-1] = CT_Code[CT_Code_Index-1] + 1\n",
        "    SUM = sum(CT_Code)\n",
        "    CT_Code = [N*1.0/SUM for N in CT_Code]\n",
        "    # Normalizing CT_Code\n",
        "    # MIN_CODE = min(CT_Code)\n",
        "    # MAX_CODE = max(CT_Code)\n",
        "    # CT_Code = [(N-MIN_CODE)*1.0/(MAX_CODE-MIN_CODE) for N in CT_Code]\n",
        "    return CT_Code\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEzG9WEi8g88"
      },
      "source": [
        "### Local Descriptor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "39B9qBKQLWxk"
      },
      "outputs": [],
      "source": [
        "\"\"\"Implementation of LD coding method\n",
        "\"\"\"\n",
        "\n",
        "__all__ = ['ld_code_of']\n",
        "\n",
        "# AAC: Classification of amino acids.\n",
        "AAC = {\n",
        "    # '0': ['X'],\n",
        "    '1': ['A', 'G', 'V'],\n",
        "    '2': ['I', 'L', 'F', 'P'],\n",
        "    '3': ['Y', 'M', 'T', 'S'],\n",
        "    '4': ['H', 'N', 'Q', 'W'],\n",
        "    '5': ['R', 'K'],\n",
        "    '6': ['D', 'E'],\n",
        "    '7': ['C']\n",
        "}\n",
        "\n",
        "# AAC_R: Reverse of AAC.\n",
        "AAC_R = {}\n",
        "for C, AAS in AAC.items():\n",
        "    for AA in AAS:\n",
        "        AAC_R[AA] = C\n",
        "\n",
        "def classification_of(AA):\n",
        "    \"\"\"Get classification of amino acids.\"\"\"\n",
        "    return AAC_R[AA]\n",
        "\n",
        "def classification_sequence_of(PS):\n",
        "    \"\"\"Make classification sequence from protein sequence.\"\"\"\n",
        "    CS = ''\n",
        "    for I, CH in enumerate(PS):\n",
        "        if CH == 'X':\n",
        "          CS = CS\n",
        "        elif CH=='U':\n",
        "          continue\n",
        "        else:\n",
        "          CS = CS + classification_of(CH)\n",
        "    return CS\n",
        "\n",
        "def ld_info_of(CS):\n",
        "    L = len(CS)\n",
        "    C = {}\n",
        "    T = {}\n",
        "    for I, CH in enumerate(CS):\n",
        "        if CH not in C:\n",
        "            C[CH] = []\n",
        "        C[CH].append(I+1)\n",
        "        if I > 0:\n",
        "            PCH = CS[I-1]\n",
        "            if PCH != CH:\n",
        "                if int(PCH)<int(CH):\n",
        "                    TIndex = PCH + CH\n",
        "                else:\n",
        "                    TIndex = CH + PCH\n",
        "                if TIndex not in T:\n",
        "                    T[TIndex] = 0\n",
        "                T[TIndex] = T[TIndex]+1\n",
        "    return L, C, T\n",
        "\n",
        "def ld_code_of_0(CS):\n",
        "    RC = [0]*7\n",
        "    RT = [0]*21\n",
        "    RD = [0]*35\n",
        "    L, C, T = ld_info_of(CS)\n",
        "    for Class, Indexs in C.items():\n",
        "        Len = len(Indexs)\n",
        "        RC[int(Class)-1]=Len*1.0/L\n",
        "        Residues = [1, int(Len*0.25), int(Len*0.5), int(Len*0.75), Len]\n",
        "        # Residues = list(map(lambda x:x*1.0/L, Residues))\n",
        "        Residues = list(map(lambda x:Indexs[x-1]*1.0/L, Residues))\n",
        "        RD[(int(Class)-1)*5:int(Class)*5] = Residues\n",
        "    for Trans, Frequency in T.items():\n",
        "        PI, I = int(Trans[0])-1, int(Trans[1])-1\n",
        "        Index = int((21-(6-PI)*(6-PI+1)/2)+(I-PI-1))\n",
        "        RT[Index] = Frequency*1.0/(L-1)\n",
        "    # return RC, RT, RD\n",
        "    return RC+RT+RD\n",
        "\n",
        "def ld_code_of(PS):\n",
        "    \"\"\"Get LD Code of protein sequence.\"\"\"\n",
        "    CS = classification_sequence_of(PS)\n",
        "    L = len(CS)\n",
        "    A = ld_code_of_0(CS[          0:int(L*0.25)])\n",
        "    B = ld_code_of_0(CS[int(L*0.25):int(L*0.50)])\n",
        "    C = ld_code_of_0(CS[int(L*0.50):int(L*0.75)])\n",
        "    D = ld_code_of_0(CS[int(L*0.75):L          ])\n",
        "    E = ld_code_of_0(CS[          0:int(L*0.50)])\n",
        "    F = ld_code_of_0(CS[int(L*0.50):L          ])\n",
        "    G = ld_code_of_0(CS[int(L*0.25):int(L*0.75)])\n",
        "    H = ld_code_of_0(CS[          0:int(L*0.75)])\n",
        "    I = ld_code_of_0(CS[int(L*0.25):L          ])\n",
        "    J = ld_code_of_0(CS[int(L*0.125):int(L*0.875)])\n",
        "    return A+B+C+D+E+F+G+H+I+J\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ihzqb1mB8j6B"
      },
      "source": [
        "### Auto Covariance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXJI0Myaf0Rj"
      },
      "outputs": [],
      "source": [
        "'''Implementation of AC coding method'''\n",
        "\n",
        "__all__ = ['ac_code_of']\n",
        "\n",
        "# PCPNS: Physicochemical property names\n",
        "PCPNS = ['H1', 'H2', 'NCI', 'P1', 'P2', 'SASA', 'V']\n",
        "\n",
        "# AAPCPVS: Physicochemical property values of amino acid\n",
        "AAPCPVS = {\n",
        "    'A': { 'H1': 0.62, 'H2':-0.5, 'NCI': 0.007187, 'P1': 8.1, 'P2':0.046, 'SASA':1.181, 'V': 27.5 },\n",
        "    'C': { 'H1': 0.29, 'H2':-1.0, 'NCI':-0.036610, 'P1': 5.5, 'P2':0.128, 'SASA':1.461, 'V': 44.6 },\n",
        "    'D': { 'H1':-0.90, 'H2': 3.0, 'NCI':-0.023820, 'P1':13.0, 'P2':0.105, 'SASA':1.587, 'V': 40.0 },\n",
        "    'E': { 'H1': 0.74, 'H2': 3.0, 'NCI': 0.006802, 'P1':12.3, 'P2':0.151, 'SASA':1.862, 'V': 62.0 },\n",
        "    'F': { 'H1': 1.19, 'H2':-2.5, 'NCI': 0.037552, 'P1': 5.2, 'P2':0.290, 'SASA':2.228, 'V':115.5 },\n",
        "    'G': { 'H1': 0.48, 'H2': 0.0, 'NCI': 0.179052, 'P1': 9.0, 'P2':0.000, 'SASA':0.881, 'V':  0.0 },\n",
        "    'H': { 'H1':-0.40, 'H2':-0.5, 'NCI':-0.010690, 'P1':10.4, 'P2':0.230, 'SASA':2.025, 'V': 79.0 },\n",
        "    'I': { 'H1': 1.38, 'H2':-1.8, 'NCI': 0.021631, 'P1': 5.2, 'P2':0.186, 'SASA':1.810, 'V': 93.5 },\n",
        "    'K': { 'H1':-1.50, 'H2': 3.0, 'NCI': 0.017708, 'P1':11.3, 'P2':0.219, 'SASA':2.258, 'V':100.0 },\n",
        "    'L': { 'H1': 1.06, 'H2':-1.8, 'NCI': 0.051672, 'P1': 4.9, 'P2':0.186, 'SASA':1.931, 'V': 93.5 },\n",
        "    'M': { 'H1': 0.64, 'H2':-1.3, 'NCI': 0.002683, 'P1': 5.7, 'P2':0.221, 'SASA':2.034, 'V': 94.1 },\n",
        "    'N': { 'H1':-0.78, 'H2': 2.0, 'NCI': 0.005392, 'P1':11.6, 'P2':0.134, 'SASA':1.655, 'V': 58.7 },\n",
        "    'P': { 'H1': 0.12, 'H2': 0.0, 'NCI': 0.239531, 'P1': 8.0, 'P2':0.131, 'SASA':1.468, 'V': 41.9 },\n",
        "    'Q': { 'H1':-0.85, 'H2': 0.2, 'NCI': 0.049211, 'P1':10.5, 'P2':0.180, 'SASA':1.932, 'V': 80.7 },\n",
        "    'R': { 'H1':-2.53, 'H2': 3.0, 'NCI': 0.043587, 'P1':10.5, 'P2':0.291, 'SASA':2.560, 'V':105.0 },\n",
        "    'S': { 'H1':-0.18, 'H2': 0.3, 'NCI': 0.004627, 'P1': 9.2, 'P2':0.062, 'SASA':1.298, 'V': 29.3 },\n",
        "    'T': { 'H1':-0.05, 'H2':-0.4, 'NCI': 0.003352, 'P1': 8.6, 'P2':0.108, 'SASA':1.525, 'V': 51.3 },\n",
        "    'V': { 'H1': 1.08, 'H2':-1.5, 'NCI': 0.057004, 'P1': 5.9, 'P2':0.140, 'SASA':1.645, 'V': 71.5 },\n",
        "    'W': { 'H1': 0.81, 'H2':-3.4, 'NCI': 0.037977, 'P1': 5.4, 'P2':0.409, 'SASA':2.663, 'V':145.5 },\n",
        "    'Y': { 'H1': 0.26, 'H2':-2.3, 'NCI': 117.3000, 'P1': 6.2, 'P2':0.298, 'SASA':2.368, 'V':  0.023599 },\n",
        "}\n",
        "\n",
        "import math\n",
        "\n",
        "def avg_sd(NUMBERS):\n",
        "    AVG = sum(NUMBERS)/len(NUMBERS)\n",
        "    TEM = [pow(NUMBER-AVG, 2) for NUMBER in NUMBERS]\n",
        "    DEV = sum(TEM)/len(TEM)\n",
        "    SD = math.sqrt(DEV)\n",
        "    return (AVG, SD)\n",
        "\n",
        "# PCPVS: Physicochemical property values\n",
        "PCPVS = {'H1':[], 'H2':[], 'NCI':[], 'P1':[], 'P2':[], 'SASA':[], 'V':[]}\n",
        "for AA, PCPS in AAPCPVS.items():\n",
        "    for PCPN in PCPNS:\n",
        "        PCPVS[PCPN].append(PCPS[PCPN])\n",
        "\n",
        "# PCPASDS: Physicochemical property avg and sds\n",
        "PCPASDS = {}\n",
        "for PCP, VS in PCPVS.items():\n",
        "    PCPASDS[PCP] = avg_sd(VS)\n",
        "\n",
        "# NORMALIZED_AAPCPVS\n",
        "NORMALIZED_AAPCPVS = {}\n",
        "for AA, PCPS in AAPCPVS.items():\n",
        "    NORMALIZED_PCPVS = {}\n",
        "    for PCP, V in PCPS.items():\n",
        "        NORMALIZED_PCPVS[PCP] = (V-PCPASDS[PCP][0])/PCPASDS[PCP][1]\n",
        "    NORMALIZED_AAPCPVS[AA] = NORMALIZED_PCPVS\n",
        "\n",
        "def pcp_value_of(AA, PCP):\n",
        "    \"\"\"Get physicochemical properties value of amino acid.\"\"\"\n",
        "    return NORMALIZED_AAPCPVS[AA][PCP];\n",
        "\n",
        "def pcp_sequence_of(PS, PCP):\n",
        "    \"\"\"Make physicochemical properties sequence of protein sequence.\"\"\"\n",
        "    PCPS = []\n",
        "    for I, CH in enumerate(PS):\n",
        "        if CH == 'X':\n",
        "          continue\n",
        "        PCPS.append(pcp_value_of(CH, PCP))\n",
        "    # Centralization\n",
        "    AVG = sum(PCPS)/len(PCPS)\n",
        "    for I, PCP in enumerate(PCPS):\n",
        "        PCPS[I] = PCP - AVG\n",
        "    return PCPS\n",
        "\n",
        "def ac_values_of(PS, PCP, LAG):\n",
        "    \"\"\"Get ac values of protein sequence.\"\"\"\n",
        "    AVS = []\n",
        "    PCPS = pcp_sequence_of(PS, PCP)\n",
        "    for LG in range(1, LAG+1):\n",
        "        SUM = 0\n",
        "        for I in range(len(PCPS)-LG):\n",
        "            SUM = SUM + PCPS[I]*PCPS[I+LG]\n",
        "        SUM = SUM / (len(PCPS)-LG)\n",
        "        AVS.append(SUM)\n",
        "    return AVS\n",
        "\n",
        "def all_ac_values_of(PS, LAG):\n",
        "    \"\"\"Get all ac values of protein sequence.\"\"\"\n",
        "    AAVS = []\n",
        "    for PCP in PCPS:\n",
        "        AVS = ac_values_of(PS, PCP, LAG)\n",
        "        AAVS = AAVS + AVS\n",
        "    return AAVS\n",
        "\n",
        "def ac_code_of(PS):\n",
        "    \"\"\"Get ac code of protein sequence.\"\"\"\n",
        "    AC_Code = all_ac_values_of(PS, 30)\n",
        "    # Normalizing AC_Code\n",
        "    # MIN_CODE = min(AC_Code)\n",
        "    # MAX_CODE = max(AC_Code)\n",
        "    # AC_Code = [(N-MIN_CODE)*1.0/(MAX_CODE-MIN_CODE) for N in AC_Code]\n",
        "    return AC_Code\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8ncX7ZW8ma7"
      },
      "source": [
        "### Pseudo amino acid composition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CHIsFXbi07ja"
      },
      "outputs": [],
      "source": [
        "def paac(str_, lambda_=0):\n",
        "  # str_=\"ATTRCDEQGGGMFSTQW\"\n",
        "  # lambda_ = 3\n",
        "  len_=len(str_)\n",
        "  tt=['A', 'R', 'N', 'D', 'C', 'E', 'Q', 'G', 'H', 'I',  'L', 'K', 'M', 'F', 'P', 'S', 'T', 'W', 'Y', 'V']\n",
        "  A = [0.62,  -0.5, 15]\n",
        "  R = [-2.53,   3, 101]\n",
        "  N = [-0.78,  0.2, 58]\n",
        "  D = [-0.9,    3, 59]\n",
        "  C = [0.29,    -1, 47]\n",
        "  E = [-0.74,   3, 73]\n",
        "  Q = [-0.85,  0.2, 72]\n",
        "  G =[0.48,    0, 1]\n",
        "  H = [-0.4,  -0.5, 82]\n",
        "  I = [1.38, -1.8, 57]\n",
        "  L = [1.06,  -1.8, 57]\n",
        "  K = [-1.5,    3, 73]\n",
        "  M = [0.64,  -1.3, 75]\n",
        "  F =[1.19, -2.5, 91]\n",
        "  P = [0.12,     0, 42]\n",
        "  S = [-0.18, 0.3, 31]\n",
        "  T = [-0.05, -0.4, 45]\n",
        "  W = [0.81, -3.4, 130] \n",
        "  Y = [0.26,  -2.3, 107]\n",
        "  V = [1.08, -1.5, 43]\n",
        "  X = [0, 0, 0]\n",
        "  H1=[A[0],R[0],N[0],D[0],C[0],E[0],Q[0],G[0],H[0],I[0],L[0],K[0],M[0],F[0],P[0],S[0],T[0],W[0],Y[0],V[0]]\n",
        "  H2=[A[1],R[1],N[1],D[1],C[1],E[1],Q[1],G[1],H[1],I[1],L[1],K[1],M[1],F[1],P[1],S[1],T[1],W[1],Y[1],V[1]]\n",
        "  M=[A[2],R[2],N[2],D[2],C[2],E[2],Q[2],G[2],H[2],I[2],L[2],K[2],M[2],F[2],P[2],S[2],T[2],W[2],Y[2],V[2]]\n",
        "  # Normalization\n",
        "  mean_H1=np.mean(H1)\n",
        "  std_H1=np.std(H1)\n",
        "  H1=(H1-mean_H1)/(std_H1)\n",
        "\n",
        "\n",
        "\n",
        "  mean_H2=np.mean(H2)\n",
        "  std_H2=np.std(H2)\n",
        "  H2=(H2-mean_H2)/(std_H2)\n",
        "\n",
        "  mean_M=np.mean(M)\n",
        "  std_M=np.std(M)\n",
        "  M=(M-mean_M)/(std_M)\n",
        "  data=np.zeros((1,len_))\n",
        "  f=np.zeros((1,20))\n",
        "\n",
        "  for j in range(len_):\n",
        "      for k in range(20):\n",
        "          # if strcmp(str(j),tt(k))==1\n",
        "          if str_[j] == tt[k]:\n",
        "              # print(j, k)\n",
        "              data[:,j]=int(k)+1\n",
        "              f[:,k]=f[:,k]+1\n",
        "  data = data.astype('int32')\n",
        "  Theta=np.zeros((lambda_,len_))\n",
        "  H=np.hstack((H1,H2,M))\n",
        "  H=H.reshape(3,-1)\n",
        "  for i in range(lambda_):\n",
        "      # for j=1:len-i\n",
        "      for j in range(len_-i):\n",
        "          if j+i+1<len_:\n",
        "              Theta[i,j]=np.mean(np.mean((H[:, data[:,j]-1]-H[:, data[:,j+i+1]-1])**2))\n",
        "\n",
        "  theta=np.zeros((1,lambda_))\n",
        "  for j in range(lambda_):\n",
        "      theta[:,j]=np.mean(Theta[j,:(len_-j-1)])\n",
        "\n",
        "  f=f/len_\n",
        "  XC=f/(1+0.05*np.sum(theta))\n",
        "  XC2=(0.05*theta)/(1+0.05*np.sum(theta))\n",
        "\n",
        "  paac = np.hstack((XC, XC2))\n",
        "  paac = paac.reshape(-1,).tolist()\n",
        "  return paac\n",
        "\n",
        "\n",
        "# 23 dimension paac vector\n",
        "# paac(seq, 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGEuw7JBLpcY"
      },
      "source": [
        "### Amino acid composition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oGMJF17fN9vn"
      },
      "outputs": [],
      "source": [
        "def aac(seq):\n",
        "  aa_list = ['A', 'R', 'N', 'D', 'C', 'E', 'Q', 'G', 'H', 'I',  'L', 'K', 'M', 'F', 'P', 'S', 'T', 'W', 'Y', 'V']\n",
        "  sum_freq = 0\n",
        "  for i in range(20):\n",
        "    sum_freq += seq.count(aa_list[i])\n",
        "\n",
        "  aa_freq = []\n",
        "  for i in range(20):\n",
        "    aa_freq.append(seq.count(aa_list[i])/sum_freq)\n",
        "  return aa_freq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EdaN4-dO-WQ"
      },
      "source": [
        "### Concatenate features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ih153Zn6RTxj"
      },
      "outputs": [],
      "source": [
        "def encode_seq(seq):\n",
        "  encoding = paac(seq) + ct_code_of(seq)\n",
        "  encoding = np.array(encoding)\n",
        "  encoding = encoding.reshape(-1, )\n",
        "  return encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrsmOfEvwNR8"
      },
      "source": [
        "### Download datasets and embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6X_2PtoHQAfm",
        "outputId": "fdb0300e-ec33-4e80-cd8d-ced020cc7ab6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-12-21 16:29:38--  https://raw.githubusercontent.com/anhvt00/S-HNBM/master/data/Golden-standard-datasets/Pan-2010/pan_pairs.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 990256 (967K) [text/plain]\n",
            "Saving to: ‘pan_pairs.tsv’\n",
            "\n",
            "pan_pairs.tsv       100%[===================>] 967.05K  --.-KB/s    in 0.06s   \n",
            "\n",
            "2022-12-21 16:29:38 (16.4 MB/s) - ‘pan_pairs.tsv’ saved [990256/990256]\n",
            "\n",
            "--2022-12-21 16:29:39--  https://raw.githubusercontent.com/anhvt00/S-HNBM/master/data/Golden-standard-datasets/Pan-2010/pan_dict.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5012967 (4.8M) [text/plain]\n",
            "Saving to: ‘pan_dict.tsv’\n",
            "\n",
            "pan_dict.tsv        100%[===================>]   4.78M  --.-KB/s    in 0.09s   \n",
            "\n",
            "2022-12-21 16:29:40 (52.6 MB/s) - ‘pan_dict.tsv’ saved [5012967/5012967]\n",
            "\n",
            "--2022-12-21 16:29:40--  https://raw.githubusercontent.com/anhvt00/PIPR/master/embeddings/seq2tensor.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1104 (1.1K) [text/plain]\n",
            "Saving to: ‘seq2tensor.py.1’\n",
            "\n",
            "seq2tensor.py.1     100%[===================>]   1.08K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-12-21 16:29:40 (95.6 MB/s) - ‘seq2tensor.py.1’ saved [1104/1104]\n",
            "\n",
            "--2022-12-21 16:29:40--  https://raw.githubusercontent.com/anhvt00/S-HNBM/master/data/Dscript-data/pairs/human_train.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 18558848 (18M) [text/plain]\n",
            "Saving to: ‘human_train.tsv’\n",
            "\n",
            "human_train.tsv     100%[===================>]  17.70M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2022-12-21 16:29:42 (125 MB/s) - ‘human_train.tsv’ saved [18558848/18558848]\n",
            "\n",
            "--2022-12-21 16:29:42--  https://raw.githubusercontent.com/anhvt00/S-HNBM/master/data/Dscript-data/seqs/human_dict.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 30858114 (29M) [text/plain]\n",
            "Saving to: ‘human_dict.tsv’\n",
            "\n",
            "human_dict.tsv      100%[===================>]  29.43M   153MB/s    in 0.2s    \n",
            "\n",
            "2022-12-21 16:29:45 (153 MB/s) - ‘human_dict.tsv’ saved [30858114/30858114]\n",
            "\n",
            "--2022-12-21 16:29:45--  https://raw.githubusercontent.com/anhvt00/PIPR/master/embeddings/ac5_aph.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2335 (2.3K) [text/plain]\n",
            "Saving to: ‘ac5_aph.txt’\n",
            "\n",
            "ac5_aph.txt         100%[===================>]   2.28K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-12-21 16:29:46 (60.0 MB/s) - ‘ac5_aph.txt’ saved [2335/2335]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/anhvt00/S-HNBM/master/data/Golden-standard-datasets/Pan-2010/pan_pairs.tsv\n",
        "!wget https://raw.githubusercontent.com/anhvt00/S-HNBM/master/data/Golden-standard-datasets/Pan-2010/pan_dict.tsv\n",
        "\n",
        "# Download file seq2tensor.py for converting protein sequences to tensors\n",
        "!wget https://raw.githubusercontent.com/anhvt00/PIPR/master/embeddings/seq2tensor.py\n",
        "\n",
        "!wget https://raw.githubusercontent.com/anhvt00/S-HNBM/master/data/Dscript-data/pairs/human_train.tsv\n",
        "!wget https://raw.githubusercontent.com/anhvt00/S-HNBM/master/data/Dscript-data/seqs/human_dict.tsv\n",
        "\n",
        "# Download file ac5_aph.txt for ac5_aph embedding \n",
        "!wget https://raw.githubusercontent.com/anhvt00/PIPR/master/embeddings/ac5_aph.txt\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paWVRDN40lw4"
      },
      "source": [
        "### Architecture of FSNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cIWepo66wCiT"
      },
      "outputs": [],
      "source": [
        "def fsnn():\n",
        "\n",
        "    d1 = Dense(256, activation='relu', kernel_initializer='glorot_normal', name='ProA_feature_1', kernel_regularizer=l2(0.01))\n",
        "    d2 = Dense(256, activation='relu', kernel_initializer='glorot_normal', name='ProA_feature_1_cos', kernel_regularizer=l2(0.01))\n",
        "    d3 = Dense(256, activation='relu', kernel_initializer='glorot_normal', name='ProA_feature_1_sin', kernel_regularizer=l2(0.01))\n",
        "\n",
        "    ########################################################\"Channel-1\" ########################################################\n",
        "    input_1 = Input(shape=(FEATURE_NUM, ), name='Protein_a')\n",
        "    p1 = d1(input_1)\n",
        "    p1 = Dropout(.2)(p1)\n",
        "    p1_cos = d2(tf.math.cos(math.pi*input_1))\n",
        "    p1_cos = Dropout(.2)(p1_cos)\n",
        "    p1_sin = d3(tf.math.sin(math.pi*input_1))\n",
        "    p1_sin = Dropout(.2)(p1_sin)\n",
        "    p1 = p1 + p1_cos + p1_sin\n",
        "    p1 = Dense(128, activation='relu', kernel_initializer='glorot_normal', name='ProA_feature_12', kernel_regularizer=l2(0.01))(p1)\n",
        "    p1 = Dropout(.2)(p1)\n",
        "    \n",
        "    ########################################################\"Channel-2\" ########################################################\n",
        "    \n",
        "   \n",
        "    input_2 = Input(shape=(FEATURE_NUM, ), name='Protein_b')\n",
        "    p2 = d1(input_2)\n",
        "    p2 = Dropout(.2)(p2)\n",
        "    p2_cos = d2(tf.math.cos(math.pi*input_2))\n",
        "    p2_cos = Dropout(.2)(p2_cos)\n",
        "    p2_sin = d3(tf.math.sin(math.pi*input_2))\n",
        "    p2_sin = Dropout(.2)(p2_sin)\n",
        "    p2 = p2 + p2_sin + p2_cos\n",
        "    p2 = Dense(128, activation='relu', kernel_initializer='glorot_normal', name='ProB_feature_12', kernel_regularizer=l2(0.01))(p2)\n",
        "    p2 = Dropout(.2)(p2)\n",
        "    \n",
        "\n",
        "\n",
        "    ##################################### Merge Abstraction features ##################################################\n",
        "    \n",
        "    # Hadamard multiplication\n",
        "    merged = tf.keras.layers.multiply([p1,p2], name='merged_protein1_2')\n",
        " \n",
        "    # Min-max scaling\n",
        "    merged = tf.divide(\n",
        "   tf.subtract(\n",
        "      merged, \n",
        "      tf.reduce_min(merged)\n",
        "   ), \n",
        "   tf.subtract(\n",
        "      tf.reduce_max(merged), \n",
        "      tf.reduce_min(merged)\n",
        "   )\n",
        ")\n",
        "    ##################################### Prediction Module ##########################################################\n",
        "\n",
        "    \n",
        "    pre_output = Dense(64, activation='relu', kernel_initializer='glorot_normal', name='Merged_feature_1')(merged)\n",
        "    pre_output=Dropout(0.2)(pre_output)\n",
        "\n",
        "    output = Dense(1, activation='sigmoid', name='output')(pre_output)\n",
        "    model = Model(inputs=[input_1, input_2], outputs=output)\n",
        "   \n",
        "    sgd = SGD(learning_rate=0.01, momentum=0.9, decay=0.001)\n",
        "    model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "\n",
        "def dnn():\n",
        "    \n",
        "    ########################################################\"Channel-1\" ########################################################\n",
        "    \n",
        "    input_1 = Input(shape=(FEATURE_NUM, ), name='Protein_a')\n",
        "    p11 = Dense(512, activation='relu', kernel_initializer='glorot_normal', name='ProA_feature_1', kernel_regularizer=l2(0.01))(input_1)\n",
        "    p11 = Dropout(0.2)(p11)\n",
        "    \n",
        "    p12 = Dense(256, activation='relu', kernel_initializer='glorot_normal', name='ProA_feature_2', kernel_regularizer=l2(0.01))(p11)\n",
        "    p12 = Dropout(0.2)(p12)\n",
        "    \n",
        "    p13= Dense(128, activation='relu', kernel_initializer='glorot_normal', name='ProA_feature_3', kernel_regularizer=l2(0.01))(p12)\n",
        "    p13 = Dropout(0.2)(p13)\n",
        "    \n",
        "    p14= Dense(64, activation='relu', kernel_initializer='glorot_normal', name='ProA_feature_4', kernel_regularizer=l2(0.01))(p13)\n",
        "    p14 = Dropout(0.2)(p14)\n",
        "    \n",
        "    ########################################################\"Channel-2\" ########################################################\n",
        "    \n",
        "    input_2 = Input(shape=(FEATURE_NUM, ), name='Protein_b')\n",
        "    p21 = Dense(512, activation='relu', kernel_initializer='glorot_normal', name='ProB_feature_1', kernel_regularizer=l2(0.01))(input_2)\n",
        "    p21 = Dropout(0.2)(p21)\n",
        "    \n",
        "    p22 = Dense(256, activation='relu', kernel_initializer='glorot_normal', name='ProB_feature_2', kernel_regularizer=l2(0.01))(p21)\n",
        "    p22 = Dropout(0.2)(p22)\n",
        "    \n",
        "    p23= Dense(128, activation='relu', kernel_initializer='glorot_normal', name='ProB_feature_3', kernel_regularizer=l2(0.01))(p22)\n",
        "    p23 = Dropout(0.2)(p23)\n",
        "    \n",
        "    p24= Dense(64, activation='relu', kernel_initializer='glorot_normal', name='ProB_feature_4', kernel_regularizer=l2(0.01))(p23)\n",
        "    p24 = Dropout(0.2)(p24)\n",
        "   \n",
        "\n",
        "\n",
        "    ##################################### Merge Abstraction features ##################################################\n",
        "    \n",
        "    merged = concatenate([p14,p24], name='merged_protein1_2')\n",
        "    \n",
        "    ##################################### Prediction Module ##########################################################\n",
        "    \n",
        "    pre_output = Dense(64, activation='relu', kernel_initializer='glorot_normal', name='Merged_feature_1')(merged)\n",
        "    pre_output = Dense(32, activation='relu', kernel_initializer='glorot_normal', name='Merged_feature_2')(pre_output)\n",
        "    pre_output = Dense(16, activation='relu', kernel_initializer='glorot_normal', name='Merged_feature_3')(pre_output)\n",
        "\n",
        "\n",
        "    \n",
        "    pre_output=Dropout(0.2)(pre_output)\n",
        "\n",
        "    output = Dense(1, activation='sigmoid', name='output')(pre_output)\n",
        "    model = Model(inputs=[input_1, input_2], outputs=output)\n",
        "   \n",
        "    sgd = SGD(learning_rate=0.01, momentum=0.9, decay=0.001)\n",
        "    model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZEZwCFdv7fQ"
      },
      "source": [
        "### Training phase "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsMrfzq5zD8Q"
      },
      "source": [
        "#### Read Pan dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0joX24ZwzD8Q",
        "outputId": "ef5eca78-1b06-47b6-c1c4-7e37f9f38cf9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "421792it [00:01, 294052.93it/s]\n",
            "100%|██████████| 15816/15816 [00:40<00:00, 393.64it/s]\n",
            "100%|██████████| 421792/421792 [00:00<00:00, 1977832.69it/s]\n",
            "100%|██████████| 421792/421792 [00:00<00:00, 2025575.97it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([0., 0., 0., ..., 0., 0., 0.])"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "import pdb\n",
        "\n",
        "# Hyperparameter for embedding file\n",
        "ds_file = 'human_train.tsv'\n",
        "label_index = 2\n",
        "id2seq_file = 'human_dict.tsv'\n",
        "id2index = {}\n",
        "seqs = []\n",
        "index = 0\n",
        "sid1_index = 0\n",
        "sid2_index = 1\n",
        "\n",
        "# Create line variable as a list of protein sequences with index is the number of protein sequences\n",
        "# id2index is a dictionary of protein id and incremental index number \n",
        "for line in open(id2seq_file):\n",
        "    line = line.strip().split('\\t')\n",
        "    id2index[line[0]] = index\n",
        "    seqs.append(line[1])\n",
        "    index += 1\n",
        "\n",
        "seq_array = []\n",
        "id2_aid = {}\n",
        "sid = 0\n",
        "max_data = -1\n",
        "limit_data = max_data > 0\n",
        "raw_data = []\n",
        "skip_head = False\n",
        "x = None\n",
        "count = 0\n",
        "\n",
        "# Create sequence array as a list of protein strings\n",
        "# Create raw data as list of pairs and label\n",
        "for line in tqdm(open(ds_file)):\n",
        "    # pdb.set_trace()\n",
        "    if skip_head:\n",
        "        skip_head = False\n",
        "        continue\n",
        "    line = line.rstrip('\\n').rstrip('\\r').split('\\t')\n",
        "    if id2index.get(line[sid1_index]) is None or id2index.get(line[sid2_index]) is None:\n",
        "        continue\n",
        "    if id2_aid.get(line[sid1_index]) is None:\n",
        "        id2_aid[line[sid1_index]] = sid\n",
        "        sid += 1\n",
        "        seq_array.append(seqs[id2index[line[sid1_index]]])\n",
        "    line[sid1_index] = id2_aid[line[sid1_index]]\n",
        "    if id2_aid.get(line[sid2_index]) is None:\n",
        "        id2_aid[line[sid2_index]] = sid\n",
        "        sid += 1\n",
        "        seq_array.append(seqs[id2index[line[sid2_index]]])\n",
        "    line[sid2_index] = id2_aid[line[sid2_index]]\n",
        "    raw_data.append(line)\n",
        "    if limit_data:\n",
        "        count += 1\n",
        "        if count >= max_data:\n",
        "            break\n",
        "\n",
        "len_m_seq = np.array([len(line.split()) for line in seq_array])\n",
        "avg_m_seq = int(np.average(len_m_seq)) + 1\n",
        "max_m_seq = max(len_m_seq)\n",
        "\n",
        "# Random for distribution of class labels\n",
        "np.random.seed(42)\n",
        "np.random.shuffle(raw_data)\n",
        "seq_tensor = np.array([encode_seq(line) for line in tqdm(seq_array)])\n",
        "\n",
        "# Extract index of 1st and 2nd sequences in pairs\n",
        "seq_index1 = np.array([line[sid1_index] for line in tqdm(raw_data)])\n",
        "seq_index2 = np.array([line[sid2_index] for line in tqdm(raw_data)])\n",
        "\n",
        "# Create class labels\n",
        "class_labels = np.zeros((len(raw_data,)))\n",
        "for i in range(len(raw_data)):\n",
        "  class_labels[i] = float(raw_data[i][label_index])\n",
        "class_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YO7uqFZyzD8U"
      },
      "source": [
        "#### Train FSNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2oeEwlXzD8U",
        "outputId": "02bf8e38-5484-4797-ffe4-41caf2aa034a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "6591/6591 [==============================] - 36s 5ms/step - loss: 0.8835 - accuracy: 0.9268\n",
            "Epoch 2/50\n",
            "6591/6591 [==============================] - 32s 5ms/step - loss: 0.2107 - accuracy: 0.9472\n",
            "Epoch 3/50\n",
            "6591/6591 [==============================] - 31s 5ms/step - loss: 0.1869 - accuracy: 0.9544\n",
            "Epoch 4/50\n",
            "6591/6591 [==============================] - 31s 5ms/step - loss: 0.1714 - accuracy: 0.9591\n",
            "Epoch 5/50\n",
            "6591/6591 [==============================] - 30s 5ms/step - loss: 0.1590 - accuracy: 0.9624\n",
            "Epoch 6/50\n",
            "6591/6591 [==============================] - 31s 5ms/step - loss: 0.1497 - accuracy: 0.9644\n",
            "Epoch 7/50\n",
            "6591/6591 [==============================] - 30s 5ms/step - loss: 0.1421 - accuracy: 0.9665\n",
            "Epoch 8/50\n",
            "6591/6591 [==============================] - 32s 5ms/step - loss: 0.1360 - accuracy: 0.9677\n",
            "Epoch 9/50\n",
            "6591/6591 [==============================] - 31s 5ms/step - loss: 0.1311 - accuracy: 0.9685\n",
            "Epoch 10/50\n",
            "6591/6591 [==============================] - 31s 5ms/step - loss: 0.1260 - accuracy: 0.9696\n",
            "Epoch 11/50\n",
            "6591/6591 [==============================] - 30s 5ms/step - loss: 0.1223 - accuracy: 0.9704\n",
            "Epoch 12/50\n",
            "6591/6591 [==============================] - 31s 5ms/step - loss: 0.1188 - accuracy: 0.9711\n",
            "Epoch 13/50\n",
            "6591/6591 [==============================] - 31s 5ms/step - loss: 0.1162 - accuracy: 0.9714\n",
            "Epoch 14/50\n",
            "6591/6591 [==============================] - 30s 5ms/step - loss: 0.1133 - accuracy: 0.9719\n",
            "Epoch 15/50\n",
            "6591/6591 [==============================] - 30s 5ms/step - loss: 0.1103 - accuracy: 0.9729\n",
            "Epoch 16/50\n",
            "6591/6591 [==============================] - 30s 5ms/step - loss: 0.1084 - accuracy: 0.9731\n",
            "Epoch 17/50\n",
            "6591/6591 [==============================] - 30s 5ms/step - loss: 0.1055 - accuracy: 0.9735\n",
            "Epoch 18/50\n",
            "6591/6591 [==============================] - 30s 5ms/step - loss: 0.1040 - accuracy: 0.9737\n",
            "Epoch 19/50\n",
            "6591/6591 [==============================] - 30s 5ms/step - loss: 0.1021 - accuracy: 0.9743\n",
            "Epoch 20/50\n",
            "6591/6591 [==============================] - 30s 5ms/step - loss: 0.1006 - accuracy: 0.9745\n",
            "Epoch 21/50\n",
            "6591/6591 [==============================] - 30s 5ms/step - loss: 0.0986 - accuracy: 0.9750\n",
            "Epoch 22/50\n",
            "6591/6591 [==============================] - 30s 5ms/step - loss: 0.0970 - accuracy: 0.9754\n",
            "Epoch 23/50\n",
            "6591/6591 [==============================] - 31s 5ms/step - loss: 0.0964 - accuracy: 0.9752\n",
            "Epoch 24/50\n",
            "6591/6591 [==============================] - 30s 5ms/step - loss: 0.0944 - accuracy: 0.9757\n",
            "Epoch 25/50\n",
            "6591/6591 [==============================] - 30s 5ms/step - loss: 0.0931 - accuracy: 0.9763\n",
            "Epoch 26/50\n",
            "6591/6591 [==============================] - 30s 5ms/step - loss: 0.0923 - accuracy: 0.9763\n",
            "Epoch 27/50\n",
            "6591/6591 [==============================] - 30s 5ms/step - loss: 0.0914 - accuracy: 0.9763\n",
            "Epoch 28/50\n",
            "6591/6591 [==============================] - 30s 5ms/step - loss: 0.0901 - accuracy: 0.9766\n",
            "Epoch 29/50\n",
            "6591/6591 [==============================] - 30s 5ms/step - loss: 0.0894 - accuracy: 0.9766\n",
            "Epoch 30/50\n",
            "6591/6591 [==============================] - 30s 5ms/step - loss: 0.0879 - accuracy: 0.9771\n",
            "Epoch 31/50\n",
            "6591/6591 [==============================] - 30s 5ms/step - loss: 0.0865 - accuracy: 0.9772\n",
            "Epoch 32/50\n",
            "6591/6591 [==============================] - 30s 5ms/step - loss: 0.0861 - accuracy: 0.9774\n",
            "Epoch 33/50\n",
            "6591/6591 [==============================] - 31s 5ms/step - loss: 0.0850 - accuracy: 0.9776\n",
            "Epoch 34/50\n",
            "6591/6591 [==============================] - 30s 5ms/step - loss: 0.0846 - accuracy: 0.9777\n",
            "Epoch 35/50\n",
            "6591/6591 [==============================] - 30s 5ms/step - loss: 0.0836 - accuracy: 0.9779\n",
            "Epoch 36/50\n",
            "6591/6591 [==============================] - 30s 5ms/step - loss: 0.0826 - accuracy: 0.9782\n",
            "Epoch 37/50\n",
            "6591/6591 [==============================] - 30s 5ms/step - loss: 0.0824 - accuracy: 0.9782\n",
            "Epoch 38/50\n",
            "6591/6591 [==============================] - 30s 5ms/step - loss: 0.0811 - accuracy: 0.9784\n",
            "Epoch 39/50\n",
            "6591/6591 [==============================] - 30s 5ms/step - loss: 0.0803 - accuracy: 0.9787\n",
            "Epoch 40/50\n",
            "6591/6591 [==============================] - 30s 5ms/step - loss: 0.0796 - accuracy: 0.9790\n",
            "Epoch 41/50\n",
            "6591/6591 [==============================] - 30s 5ms/step - loss: 0.0794 - accuracy: 0.9789\n",
            "Epoch 42/50\n",
            "6591/6591 [==============================] - 30s 5ms/step - loss: 0.0782 - accuracy: 0.9789\n",
            "Epoch 43/50\n",
            "6591/6591 [==============================] - 31s 5ms/step - loss: 0.0781 - accuracy: 0.9791\n",
            "Epoch 44/50\n",
            "6591/6591 [==============================] - 30s 5ms/step - loss: 0.0771 - accuracy: 0.9793\n",
            "Epoch 45/50\n",
            "6591/6591 [==============================] - 30s 5ms/step - loss: 0.0770 - accuracy: 0.9793\n",
            "Epoch 46/50\n",
            "6591/6591 [==============================] - 31s 5ms/step - loss: 0.0760 - accuracy: 0.9793\n",
            "Epoch 47/50\n",
            "6591/6591 [==============================] - 30s 5ms/step - loss: 0.0755 - accuracy: 0.9795\n",
            "Epoch 48/50\n",
            "6591/6591 [==============================] - 30s 5ms/step - loss: 0.0748 - accuracy: 0.9796\n",
            "Epoch 49/50\n",
            "6591/6591 [==============================] - 30s 5ms/step - loss: 0.0741 - accuracy: 0.9799\n",
            "Epoch 50/50\n",
            "6591/6591 [==============================] - 30s 5ms/step - loss: 0.0736 - accuracy: 0.9801\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f8a0f2d5f40>"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "##################################### Load Positive and Negative Dataset ##########################################################\n",
        "\n",
        "FEATURE_NUM = seq_tensor.shape[1]\n",
        "# Create data frame of pairs and labels\n",
        "df = np.hstack([seq_tensor[seq_index1], seq_tensor[seq_index2]])\n",
        "df = np.hstack([df, class_labels.reshape(-1,1)])\n",
        "df = pd.DataFrame(df)\n",
        "df = df.sample(frac=1)\n",
        "\n",
        "# Create pairs matrix and label vector\n",
        "X = df.iloc[:,0:FEATURE_NUM*2].values\n",
        "y_train = df.iloc[:,FEATURE_NUM*2:].values\n",
        "\n",
        "# standard scaler\n",
        "standard_scaler = StandardScaler()\n",
        "X = standard_scaler.fit_transform(X)\n",
        "\n",
        "\n",
        "X1_train = X[:, :FEATURE_NUM]\n",
        "X2_train = X[:, FEATURE_NUM:]\n",
        "\n",
        "\n",
        "model=fsnn()\n",
        "model.fit([X1_train, X2_train],y_train, epochs=50,batch_size=64,verbose=1)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vh7aiTCM6cgh",
        "outputId": "55913c32-311f-4390-efa0-24ead78c7711"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================= INFER BY TRAINED NEURAL NETWORK ON TRAINING SET ==============================\n",
            "13181/13181 [==============================] - 25s 2ms/step\n",
            "============= INFERENCE BY NEURAL NETWORK ===============\n",
            "accuracy: 0.9851751574235642, precision: 0.9439694529758446, recall: 0.8897350302524515, specificity: 0.9947189710208425, f1-score: 0.9160502114519702, mcc: 0.9084057698303128, auroc: 0.9976192139095166, auprc: 0.979436682225439 \n",
            "0.9851751574235642\t0.9439694529758446\t0.8897350302524515\t0.9947189710208425\t0.9160502114519702\t0.9084057698303128\t0.9976192139095166\t0.979436682225439\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"============================= INFER BY TRAINED NEURAL NETWORK ON TRAINING SET ==============================\")\n",
        "y_pred = model.predict([X1_train, X2_train])\n",
        "y_true = y_train\n",
        "cm1=confusion_matrix(y_true, np.round(y_pred))\n",
        "acc = (cm1[0,0]+cm1[1,1])/(cm1[0,0]+cm1[0,1]+cm1[1,0]+cm1[1,1])\n",
        "spec= (cm1[0,0])/(cm1[0,0]+cm1[0,1])\n",
        "sens = (cm1[1,1])/(cm1[1,0]+cm1[1,1])\n",
        "prec=cm1[1,1]/(cm1[1,1]+cm1[0,1])\n",
        "rec=cm1[1,1]/(cm1[1,1]+cm1[1,0])\n",
        "f1 = 2 * (prec * rec) / (prec + rec)\n",
        "mcc = matthews_corrcoef(y_true, np.round(y_pred))\n",
        "\n",
        "prc = metrics.average_precision_score(y_true, y_pred)\n",
        "\n",
        "print(\"============= INFERENCE BY NEURAL NETWORK ===============\")\n",
        "try:\n",
        "  auc = metrics.roc_auc_score(y_true, y_pred)\n",
        "  print(f'accuracy: {acc}, precision: {prec}, recall: {rec}, specificity: {spec}, f1-score: {f1}, mcc: {mcc}, auroc: {auc}, auprc: {prc} ')\n",
        "  print(str(acc) + \"\\t\" + str(prec) + \"\\t\" + str(rec) + \"\\t\" + str(spec) + \"\\t\" + str(f1) + \"\\t\" + str(mcc)+\"\\t\" + str(auc)  + \"\\t\" + str(prc) + \"\\n\")\n",
        "except ValueError:\n",
        "  print(f'accuracy: {acc}, precision: {prec}, recall: {rec}, specificity: {spec}, f1-score: {f1}, mcc: {mcc}, auroc: nan, auprc: {prc} ')\n",
        "  print(str(acc) + \"\\t\" + str(prec) + \"\\t\" + str(rec) + \"\\t\" + str(spec) + \"\\t\" + str(f1) + \"\\t\" + str(mcc)+\"\\t nan\"  + \"\\t\" + str(prc) + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32c-jPrr0eFk"
      },
      "source": [
        "#### Extract hidden layer of FSNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ERM1r-YYyvE",
        "outputId": "9aa59310-cacf-4d1c-a732-5e625db4a770"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "13181/13181 [==============================] - 23s 2ms/step\n"
          ]
        }
      ],
      "source": [
        "################################Intermediate Layer prediction (Abstraction features extraction)######################################\n",
        "\n",
        "intermediate_layer_model = Model(inputs=model.input,outputs=model.get_layer('Merged_feature_1').output)\n",
        "\n",
        "# Use intermediate layer to transform pairs matrix\n",
        "intermediate_output_p1 = intermediate_layer_model.predict([X1_train,X2_train])  \n",
        "p_merge=pd.DataFrame(intermediate_output_p1)    \n",
        "\n",
        "# create dataframe use transformed pairs matrix outputs and labels\n",
        "X_train_feat=pd.concat((p_merge,pd.DataFrame(pd.DataFrame(y_train))),axis=1,ignore_index=True)\n",
        "\n",
        "# write to file dataframe of transformed pairs matrix and labels\n",
        "X_train_feat.to_csv('X_train.csv',header=False, index=False)\n",
        "\n",
        "# read dataframe of transformed pairs matrix and labels\n",
        "Train=pd.read_csv(\"X_train.csv\",header=None)\n",
        "Train=Train.sample(frac=1)\n",
        "\n",
        "X=Train.iloc[:,0:64].values\n",
        "y=Train.iloc[:,64:].values.ravel()\n",
        "\n",
        "extracted_df=X_train_feat\n",
        "\n",
        "robust_scaler=RobustScaler()\n",
        "# scaler=MinMaxScaler()\n",
        "# scaler = StandardScaler()\n",
        "X=robust_scaler.fit_transform(X)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2Ov_iwg9OiC"
      },
      "source": [
        "#### Train LGBM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iED9z0tr9gDy",
        "outputId": "71302181-3a1d-4550-87f2-3ea33aea5c0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================= INFER BY TRAINED HYBRID MODEL ON TRAINING SET ==============================\n",
            "accuracy: 1.0, precision: 1.0, recall: 1.0, specificity: 1.0, f1-score: 1.0, mcc: 1.0, auroc: 1.0, auprc: 1.0 \n",
            "1.0\t1.0\t1.0\t1.0\t1.0\t1.0\t1.0\t1.0\n",
            "\n"
          ]
        }
      ],
      "source": [
        "model_=LGBMClassifier(learning_rate=.2, gamma=0, max_depth=10, n_estimators=1000)\n",
        "model_.fit(X, y)\n",
        "print(\"============================= INFER BY TRAINED HYBRID MODEL ON TRAINING SET ==============================\")\n",
        "y_true=y\n",
        "y_pred = model_.predict(X)\n",
        "cm1=confusion_matrix(y_true, np.round(y_pred))\n",
        "acc = (cm1[0,0]+cm1[1,1])/(cm1[0,0]+cm1[0,1]+cm1[1,0]+cm1[1,1])\n",
        "spec= (cm1[0,0])/(cm1[0,0]+cm1[0,1])\n",
        "sens = (cm1[1,1])/(cm1[1,0]+cm1[1,1])\n",
        "prec=cm1[1,1]/(cm1[1,1]+cm1[0,1])\n",
        "rec=cm1[1,1]/(cm1[1,1]+cm1[1,0])\n",
        "f1 = 2 * (prec * rec) / (prec + rec)\n",
        "mcc = matthews_corrcoef(y_true, np.round(y_pred))\n",
        "\n",
        "prc = metrics.average_precision_score(y_true, y_pred)\n",
        "\n",
        "try:\n",
        "  auc = metrics.roc_auc_score(y_true, y_pred)\n",
        "  print(f'accuracy: {acc}, precision: {prec}, recall: {rec}, specificity: {spec}, f1-score: {f1}, mcc: {mcc}, auroc: {auc}, auprc: {prc} ')\n",
        "  print(str(acc) + \"\\t\" + str(prec) + \"\\t\" + str(rec) + \"\\t\" + str(spec) + \"\\t\" + str(f1) + \"\\t\" + str(mcc)+\"\\t\" + str(auc)  + \"\\t\" + str(prc) + \"\\n\")\n",
        "except ValueError:\n",
        "  print(f'accuracy: {acc}, precision: {prec}, recall: {rec}, specificity: {spec}, f1-score: {f1}, mcc: {mcc}, auroc: nan, auprc: {prc} ')\n",
        "  print(str(acc) + \"\\t\" + str(prec) + \"\\t\" + str(rec) + \"\\t\" + str(spec) + \"\\t\" + str(f1) + \"\\t\" + str(mcc)+\"\\t nan\"  + \"\\t\" + str(prc) + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxnJBQk0CDMp"
      },
      "source": [
        "## Inference phase"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkFVJTMEUzpH"
      },
      "source": [
        "## Evaluation on intra-species datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmIjcOdrD7iR"
      },
      "source": [
        "### HPRD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zal6oSwdFEh3"
      },
      "source": [
        "#### Read dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9gcHxCgFDAK",
        "outputId": "16b73591-66a3-4e2e-ce4e-c46d68fb0581"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-12-17 17:08:33--  https://raw.githubusercontent.com/anhvt00/MCAPS/master/data/Independent-testsets/Human-sets/HPRD/hprd_pairs.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 56256 (55K) [text/plain]\n",
            "Saving to: ‘hprd_pairs.tsv’\n",
            "\n",
            "hprd_pairs.tsv      100%[===================>]  54.94K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2022-12-17 17:08:34 (70.7 MB/s) - ‘hprd_pairs.tsv’ saved [56256/56256]\n",
            "\n",
            "--2022-12-17 17:08:34--  https://raw.githubusercontent.com/anhvt00/MCAPS/master/data/Independent-testsets/Human-sets/HPRD/hprd_dict.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2428937 (2.3M) [text/plain]\n",
            "Saving to: ‘hprd_dict.tsv’\n",
            "\n",
            "hprd_dict.tsv       100%[===================>]   2.32M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2022-12-17 17:08:34 (242 MB/s) - ‘hprd_dict.tsv’ saved [2428937/2428937]\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "3516it [00:00, 380474.02it/s]\n",
            "100%|██████████| 2747/2747 [00:08<00:00, 330.81it/s]\n",
            "100%|██████████| 3516/3516 [00:00<00:00, 1734349.39it/s]\n",
            "100%|██████████| 3516/3516 [00:00<00:00, 1806144.87it/s]\n"
          ]
        }
      ],
      "source": [
        "id2seq_file = 'hprd_dict.tsv'\n",
        "id2index = {}\n",
        "seqs = []\n",
        "index = 0\n",
        "sid1_index = 0\n",
        "sid2_index = 1\n",
        "ds_file = 'hprd_pairs.tsv'\n",
        "label_index = 2\n",
        "use_emb = 'ac5_aph.txt'\n",
        "\n",
        "if not os.path.isfile(ds_file) or not os.path.isfile(id2seq_file):\n",
        "  !wget https://raw.githubusercontent.com/anhvt00/MCAPS/master/data/Independent-testsets/Human-sets/HPRD/hprd_pairs.tsv\n",
        "  !wget https://raw.githubusercontent.com/anhvt00/MCAPS/master/data/Independent-testsets/Human-sets/HPRD/hprd_dict.tsv\n",
        "\n",
        "\n",
        "# Create line variable as a list of protein sequences with index is the number of protein sequences\n",
        "# id2index is a dictionary of protein id and incremental index number \n",
        "for line in open(id2seq_file):\n",
        "    line = line.strip().split('\\t')\n",
        "    id2index[line[0]] = index\n",
        "    seqs.append(line[1])\n",
        "    index += 1\n",
        "\n",
        "seq_array = []\n",
        "id2_aid = {}\n",
        "sid = 0\n",
        "\n",
        "seq2t = s2t(use_emb)\n",
        "\n",
        "max_data = -1\n",
        "limit_data = max_data > 0\n",
        "raw_data = []\n",
        "skip_head = False\n",
        "x = None\n",
        "count = 0\n",
        "\n",
        "# Create sequence array as a list of protein strings\n",
        "for line in tqdm(open(ds_file)):\n",
        "    if skip_head:\n",
        "        skip_head = False\n",
        "        continue\n",
        "    line = line.rstrip('\\n').rstrip('\\r').split('\\t')\n",
        "    if id2index.get(line[sid1_index]) is None or id2index.get(line[sid2_index]) is None:\n",
        "        continue\n",
        "    if id2_aid.get(line[sid1_index]) is None:\n",
        "        id2_aid[line[sid1_index]] = sid\n",
        "        sid += 1\n",
        "        seq_array.append(seqs[id2index[line[sid1_index]]])\n",
        "    line[sid1_index] = id2_aid[line[sid1_index]]\n",
        "    if id2_aid.get(line[sid2_index]) is None:\n",
        "        id2_aid[line[sid2_index]] = sid\n",
        "        sid += 1\n",
        "        seq_array.append(seqs[id2index[line[sid2_index]]])\n",
        "    line[sid2_index] = id2_aid[line[sid2_index]]\n",
        "    raw_data.append(line)\n",
        "    if limit_data:\n",
        "        count += 1\n",
        "        if count >= max_data:\n",
        "            break\n",
        "\n",
        "len_m_seq = np.array([len(line.split()) for line in seq_array])\n",
        "avg_m_seq = int(np.average(len_m_seq)) + 1\n",
        "max_m_seq = max(len_m_seq)\n",
        "dim = seq2t.dim\n",
        "\n",
        "# seq_tensor is tensor representation of dataset having shape of (number_of_sequences, padding_length, embedding_dim_of_aa)\n",
        "# Random for distribution of class labels\n",
        "np.random.seed(42)\n",
        "np.random.shuffle(raw_data)\n",
        "seq_tensor = np.array([encode_seq(line) for line in tqdm(seq_array)])\n",
        "\n",
        "# Extract index of 1st and 2nd sequences in pairs\n",
        "seq_index1 = np.array([line[sid1_index] for line in tqdm(raw_data)])\n",
        "seq_index2 = np.array([line[sid2_index] for line in tqdm(raw_data)])\n",
        "\n",
        "# Assign labels for pairs of sequences\n",
        "class_map = {'0':1,'1':0}\n",
        "class_labels = np.zeros((len(raw_data,)))\n",
        "for i in range(len(raw_data)):\n",
        "  class_labels[i] = float(raw_data[i][label_index])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3ugg5teYSjv",
        "outputId": "fa3001bc-da33-454f-feb8-1e9fb24f0e60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================= INFER BY TRAINED NEURAL NETWORK ON TRAINING SET ==============================\n",
            "110/110 [==============================] - 0s 2ms/step\n",
            "============= INFERENCE BY NEURAL NETWORK ===============\n",
            "accuracy: 0.36888509670079633, precision: 1.0, recall: 0.36888509670079633, specificity: nan, f1-score: 0.5389569914814045, mcc: 0.0, auroc: nan, auprc: 1.0 \n",
            "0.36888509670079633\t1.0\t0.36888509670079633\tnan\t0.5389569914814045\t0.0\t nan\t1.0\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-16-01eae5801102>:31: RuntimeWarning: invalid value encountered in long_scalars\n",
            "  spec= (cm1[0,0])/(cm1[0,0]+cm1[0,1])\n"
          ]
        }
      ],
      "source": [
        "##################################### Load Positive and Negative Dataset ##########################################################\n",
        "\n",
        "FEATURE_NUM = seq_tensor.shape[1]\n",
        "# Create data frame of pairs and labels\n",
        "df = np.hstack([seq_tensor[seq_index1], seq_tensor[seq_index2]])\n",
        "df = np.hstack([df, class_labels.reshape(-1,1)])\n",
        "df = pd.DataFrame(df)\n",
        "df = df.sample(frac=1)\n",
        "\n",
        "# Create pairs matrix and label vector\n",
        "X = df.iloc[:,0:FEATURE_NUM*2].values\n",
        "y = df.iloc[:,FEATURE_NUM*2:].values\n",
        "\n",
        "\n",
        "# standard scaler\n",
        "X = standard_scaler.transform(X)\n",
        "\n",
        "\n",
        "X1_train = X[:, :FEATURE_NUM]\n",
        "X2_train = X[:, FEATURE_NUM:]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"============================= INFER BY TRAINED NEURAL NETWORK ON TRAINING SET ==============================\")\n",
        "y_true = y\n",
        "y_pred = model.predict([X1_train, X2_train])\n",
        "cm1=confusion_matrix(y_true, np.round(y_pred))\n",
        "acc = (cm1[0,0]+cm1[1,1])/(cm1[0,0]+cm1[0,1]+cm1[1,0]+cm1[1,1])\n",
        "spec= (cm1[0,0])/(cm1[0,0]+cm1[0,1])\n",
        "sens = (cm1[1,1])/(cm1[1,0]+cm1[1,1])\n",
        "prec=cm1[1,1]/(cm1[1,1]+cm1[0,1])\n",
        "rec=cm1[1,1]/(cm1[1,1]+cm1[1,0])\n",
        "f1 = 2 * (prec * rec) / (prec + rec)\n",
        "mcc = matthews_corrcoef(y_true, np.round(y_pred))\n",
        "\n",
        "prc = metrics.average_precision_score(y_true, y_pred)\n",
        "\n",
        "print(\"============= INFERENCE BY NEURAL NETWORK ===============\")\n",
        "try:\n",
        "  auc = metrics.roc_auc_score(y_true, y_pred)\n",
        "  print(f'accuracy: {acc}, precision: {prec}, recall: {rec}, specificity: {spec}, f1-score: {f1}, mcc: {mcc}, auroc: {auc}, auprc: {prc} ')\n",
        "  print(str(acc) + \"\\t\" + str(prec) + \"\\t\" + str(rec) + \"\\t\" + str(spec) + \"\\t\" + str(f1) + \"\\t\" + str(mcc)+\"\\t\" + str(auc)  + \"\\t\" + str(prc) + \"\\n\")\n",
        "except ValueError:\n",
        "  print(f'accuracy: {acc}, precision: {prec}, recall: {rec}, specificity: {spec}, f1-score: {f1}, mcc: {mcc}, auroc: nan, auprc: {prc} ')\n",
        "  print(str(acc) + \"\\t\" + str(prec) + \"\\t\" + str(rec) + \"\\t\" + str(spec) + \"\\t\" + str(f1) + \"\\t\" + str(mcc)+\"\\t nan\"  + \"\\t\" + str(prc) + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5R6OvagdCM_"
      },
      "source": [
        "#### Evaluate with trained FSNN-LGBM prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LqcWlzJHz2B",
        "outputId": "a6e8d699-8f3a-4e0a-c3d7-db89ac587cd0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "110/110 [==============================] - 0s 2ms/step\n",
            "accuracy: 0.41439135381114905, precision: 1.0, recall: 0.41439135381114905, specificity: nan, f1-score: 0.5859642067162679, mcc: 0.0, auroc: nan, auprc: 1.0 \n",
            "0.41439135381114905\t1.0\t0.41439135381114905\tnan\t0.5859642067162679\t0.0\t nan\t1.0\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-17-ea27a8f05e5a>:57: RuntimeWarning: invalid value encountered in long_scalars\n",
            "  spec= (cm1[0,0])/(cm1[0,0]+cm1[0,1])\n"
          ]
        }
      ],
      "source": [
        "# Read new data\n",
        "\n",
        "FEATURE_NUM = seq_tensor.shape[1]\n",
        "# Create data frame of pairs and labels\n",
        "df = np.hstack([seq_tensor[seq_index1], seq_tensor[seq_index2]])\n",
        "df = np.hstack([df, class_labels.reshape(-1,1)])\n",
        "df = pd.DataFrame(df)\n",
        "df = df.sample(frac=1)\n",
        "\n",
        "# Create pairs matrix and label vector\n",
        "X = df.iloc[:,0:FEATURE_NUM*2].values\n",
        "y = df.iloc[:,FEATURE_NUM*2:].values\n",
        "\n",
        "Trainlabels=y\n",
        "\n",
        "# standard scaler\n",
        "# scaler = StandardScaler().fit(X)\n",
        "# scaler = MinMaxScaler().fit(X)\n",
        "\n",
        "# scaler = RobustScaler().fit(X)\n",
        "X = standard_scaler.transform(X)\n",
        "\n",
        "\n",
        "X1_train = X[:, :FEATURE_NUM]\n",
        "X2_train = X[:, FEATURE_NUM:]\n",
        "\n",
        "\n",
        "# Predict representation from trained neural network\n",
        "\n",
        "intermediate_layer_model = Model(inputs=model.input,outputs=model.get_layer('Merged_feature_1').output)\n",
        "# Use intermediate layer to transform pairs matrix\n",
        "intermediate_output_p1 = intermediate_layer_model.predict([X1_train,X2_train])  \n",
        "p_merge=pd.DataFrame(intermediate_output_p1)    \n",
        "\n",
        "# create dataframe use transformed pairs matrix outputs and labels\n",
        "X_train_feat=pd.concat((p_merge,pd.DataFrame(pd.DataFrame(Trainlabels))),axis=1,ignore_index=True)\n",
        "\n",
        "# write to file dataframe of transformed pairs matrix and labels\n",
        "X_train_feat.to_csv('X_train.csv',header=False, index=False)\n",
        "\n",
        "# read dataframe of transformed pairs matrix and labels\n",
        "Train=pd.read_csv(\"X_train.csv\",header=None)\n",
        "Train=Train.sample(frac=1)\n",
        "\n",
        "X=Train.iloc[:,0:64].values\n",
        "y=Train.iloc[:,64:].values.ravel()\n",
        "\n",
        "extracted_df=X_train_feat\n",
        "\n",
        "X=robust_scaler.transform(X)\n",
        "\n",
        "# Predict probability from neural network output for new data\n",
        "y_true=y\n",
        "y_pred = model_.predict(X)\n",
        "cm1=confusion_matrix(y_true, np.round(y_pred))\n",
        "acc = (cm1[0,0]+cm1[1,1])/(cm1[0,0]+cm1[0,1]+cm1[1,0]+cm1[1,1])\n",
        "spec= (cm1[0,0])/(cm1[0,0]+cm1[0,1])\n",
        "sens = (cm1[1,1])/(cm1[1,0]+cm1[1,1])\n",
        "prec=cm1[1,1]/(cm1[1,1]+cm1[0,1])\n",
        "rec=cm1[1,1]/(cm1[1,1]+cm1[1,0])\n",
        "f1 = 2 * (prec * rec) / (prec + rec)\n",
        "mcc = matthews_corrcoef(y_true, np.round(y_pred))\n",
        "\n",
        "prc = metrics.average_precision_score(y_true, y_pred)\n",
        "\n",
        "try:\n",
        "  auc = metrics.roc_auc_score(y_true, y_pred)\n",
        "  print(f'accuracy: {acc}, precision: {prec}, recall: {rec}, specificity: {spec}, f1-score: {f1}, mcc: {mcc}, auroc: {auc}, auprc: {prc} ')\n",
        "  print(str(acc) + \"\\t\" + str(prec) + \"\\t\" + str(rec) + \"\\t\" + str(spec) + \"\\t\" + str(f1) + \"\\t\" + str(mcc)+\"\\t\" + str(auc)  + \"\\t\" + str(prc) + \"\\n\")\n",
        "except ValueError:\n",
        "  print(f'accuracy: {acc}, precision: {prec}, recall: {rec}, specificity: {spec}, f1-score: {f1}, mcc: {mcc}, auroc: nan, auprc: {prc} ')\n",
        "  print(str(acc) + \"\\t\" + str(prec) + \"\\t\" + str(rec) + \"\\t\" + str(spec) + \"\\t\" + str(f1) + \"\\t\" + str(mcc)+\"\\t nan\"  + \"\\t\" + str(prc) + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8U_wH2gos8dp"
      },
      "source": [
        "### DIP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyH_oDSts8dp"
      },
      "source": [
        "#### Read dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDczzw8os8dq",
        "outputId": "5f65c747-e384-40f0-c48a-5a91ad300cfa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-12-17 17:08:44--  https://raw.githubusercontent.com/anhvt00/S-HNBM/master/data/Independent-testsets/Human-sets/DIP/dip_pairs.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 23488 (23K) [text/plain]\n",
            "Saving to: ‘dip_pairs.tsv’\n",
            "\n",
            "dip_pairs.tsv       100%[===================>]  22.94K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-12-17 17:08:44 (88.2 MB/s) - ‘dip_pairs.tsv’ saved [23488/23488]\n",
            "\n",
            "--2022-12-17 17:08:45--  https://raw.githubusercontent.com/anhvt00/S-HNBM/master/data/Independent-testsets/Human-sets/DIP/dip_dict.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1262993 (1.2M) [text/plain]\n",
            "Saving to: ‘dip_dict.tsv’\n",
            "\n",
            "dip_dict.tsv        100%[===================>]   1.20M  --.-KB/s    in 0.007s  \n",
            "\n",
            "2022-12-17 17:08:45 (181 MB/s) - ‘dip_dict.tsv’ saved [1262993/1262993]\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1468it [00:00, 199327.88it/s]\n",
            "100%|██████████| 1312/1312 [00:04<00:00, 321.62it/s]\n",
            "100%|██████████| 1468/1468 [00:00<00:00, 1072315.97it/s]\n",
            "100%|██████████| 1468/1468 [00:00<00:00, 1159992.14it/s]\n"
          ]
        }
      ],
      "source": [
        "id2seq_file = 'dip_dict.tsv'\n",
        "id2index = {}\n",
        "seqs = []\n",
        "index = 0\n",
        "sid1_index = 0\n",
        "sid2_index = 1\n",
        "ds_file = 'dip_pairs.tsv'\n",
        "label_index = 2\n",
        "use_emb = 'ac5_aph.txt'\n",
        "\n",
        "if not os.path.isfile(ds_file) or not os.path.isfile(id2seq_file):\n",
        "  !wget https://raw.githubusercontent.com/anhvt00/S-HNBM/master/data/Independent-testsets/Human-sets/DIP/dip_pairs.tsv\n",
        "  !wget https://raw.githubusercontent.com/anhvt00/S-HNBM/master/data/Independent-testsets/Human-sets/DIP/dip_dict.tsv\n",
        "\n",
        "\n",
        "\n",
        "# Create line variable as a list of protein sequences with index is the number of protein sequences\n",
        "# id2index is a dictionary of protein id and incremental index number \n",
        "for line in open(id2seq_file):\n",
        "    line = line.strip().split('\\t')\n",
        "    id2index[line[0]] = index\n",
        "    seqs.append(line[1])\n",
        "    index += 1\n",
        "\n",
        "seq_array = []\n",
        "id2_aid = {}\n",
        "sid = 0\n",
        "\n",
        "seq2t = s2t(use_emb)\n",
        "\n",
        "max_data = -1\n",
        "limit_data = max_data > 0\n",
        "raw_data = []\n",
        "skip_head = False\n",
        "x = None\n",
        "count = 0\n",
        "\n",
        "# Create sequence array as a list of protein strings\n",
        "for line in tqdm(open(ds_file)):\n",
        "    if skip_head:\n",
        "        skip_head = False\n",
        "        continue\n",
        "    line = line.rstrip('\\n').rstrip('\\r').split('\\t')\n",
        "    if id2index.get(line[sid1_index]) is None or id2index.get(line[sid2_index]) is None:\n",
        "        continue\n",
        "    if id2_aid.get(line[sid1_index]) is None:\n",
        "        id2_aid[line[sid1_index]] = sid\n",
        "        sid += 1\n",
        "        seq_array.append(seqs[id2index[line[sid1_index]]])\n",
        "    line[sid1_index] = id2_aid[line[sid1_index]]\n",
        "    if id2_aid.get(line[sid2_index]) is None:\n",
        "        id2_aid[line[sid2_index]] = sid\n",
        "        sid += 1\n",
        "        seq_array.append(seqs[id2index[line[sid2_index]]])\n",
        "    line[sid2_index] = id2_aid[line[sid2_index]]\n",
        "    raw_data.append(line)\n",
        "    if limit_data:\n",
        "        count += 1\n",
        "        if count >= max_data:\n",
        "            break\n",
        "\n",
        "len_m_seq = np.array([len(line.split()) for line in seq_array])\n",
        "avg_m_seq = int(np.average(len_m_seq)) + 1\n",
        "max_m_seq = max(len_m_seq)\n",
        "dim = seq2t.dim\n",
        "\n",
        "# seq_tensor is tensor representation of dataset having shape of (number_of_sequences, padding_length, embedding_dim_of_aa)\n",
        "# Random for distribution of class labels\n",
        "np.random.seed(42)\n",
        "np.random.shuffle(raw_data)\n",
        "seq_tensor = np.array([encode_seq(line) for line in tqdm(seq_array)])\n",
        "\n",
        "# Extract index of 1st and 2nd sequences in pairs\n",
        "seq_index1 = np.array([line[sid1_index] for line in tqdm(raw_data)])\n",
        "seq_index2 = np.array([line[sid2_index] for line in tqdm(raw_data)])\n",
        "\n",
        "# Assign labels for pairs of sequences\n",
        "class_map = {'0':1,'1':0}\n",
        "class_labels = np.zeros((len(raw_data,)))\n",
        "for i in range(len(raw_data)):\n",
        "  class_labels[i] = float(raw_data[i][label_index])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ndV4RNGs8dq",
        "outputId": "0e2ded63-8643-4864-b94b-c3d024080d42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================= INFER BY TRAINED NEURAL NETWORK ON TRAINING SET ==============================\n",
            "46/46 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-19-01eae5801102>:31: RuntimeWarning: invalid value encountered in long_scalars\n",
            "  spec= (cm1[0,0])/(cm1[0,0]+cm1[0,1])\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============= INFERENCE BY NEURAL NETWORK ===============\n",
            "accuracy: 0.44618528610354224, precision: 1.0, recall: 0.44618528610354224, specificity: nan, f1-score: 0.6170513424399435, mcc: 0.0, auroc: nan, auprc: 1.0 \n",
            "0.44618528610354224\t1.0\t0.44618528610354224\tnan\t0.6170513424399435\t0.0\t nan\t1.0\n",
            "\n"
          ]
        }
      ],
      "source": [
        "##################################### Load Positive and Negative Dataset ##########################################################\n",
        "\n",
        "FEATURE_NUM = seq_tensor.shape[1]\n",
        "# Create data frame of pairs and labels\n",
        "df = np.hstack([seq_tensor[seq_index1], seq_tensor[seq_index2]])\n",
        "df = np.hstack([df, class_labels.reshape(-1,1)])\n",
        "df = pd.DataFrame(df)\n",
        "df = df.sample(frac=1)\n",
        "\n",
        "# Create pairs matrix and label vector\n",
        "X = df.iloc[:,0:FEATURE_NUM*2].values\n",
        "y = df.iloc[:,FEATURE_NUM*2:].values\n",
        "\n",
        "\n",
        "# standard scaler\n",
        "X = standard_scaler.transform(X)\n",
        "\n",
        "\n",
        "X1_train = X[:, :FEATURE_NUM]\n",
        "X2_train = X[:, FEATURE_NUM:]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"============================= INFER BY TRAINED NEURAL NETWORK ON TRAINING SET ==============================\")\n",
        "y_true = y\n",
        "y_pred = model.predict([X1_train, X2_train])\n",
        "cm1=confusion_matrix(y_true, np.round(y_pred))\n",
        "acc = (cm1[0,0]+cm1[1,1])/(cm1[0,0]+cm1[0,1]+cm1[1,0]+cm1[1,1])\n",
        "spec= (cm1[0,0])/(cm1[0,0]+cm1[0,1])\n",
        "sens = (cm1[1,1])/(cm1[1,0]+cm1[1,1])\n",
        "prec=cm1[1,1]/(cm1[1,1]+cm1[0,1])\n",
        "rec=cm1[1,1]/(cm1[1,1]+cm1[1,0])\n",
        "f1 = 2 * (prec * rec) / (prec + rec)\n",
        "mcc = matthews_corrcoef(y_true, np.round(y_pred))\n",
        "\n",
        "prc = metrics.average_precision_score(y_true, y_pred)\n",
        "\n",
        "print(\"============= INFERENCE BY NEURAL NETWORK ===============\")\n",
        "try:\n",
        "  auc = metrics.roc_auc_score(y_true, y_pred)\n",
        "  print(f'accuracy: {acc}, precision: {prec}, recall: {rec}, specificity: {spec}, f1-score: {f1}, mcc: {mcc}, auroc: {auc}, auprc: {prc} ')\n",
        "  print(str(acc) + \"\\t\" + str(prec) + \"\\t\" + str(rec) + \"\\t\" + str(spec) + \"\\t\" + str(f1) + \"\\t\" + str(mcc)+\"\\t\" + str(auc)  + \"\\t\" + str(prc) + \"\\n\")\n",
        "except ValueError:\n",
        "  print(f'accuracy: {acc}, precision: {prec}, recall: {rec}, specificity: {spec}, f1-score: {f1}, mcc: {mcc}, auroc: nan, auprc: {prc} ')\n",
        "  print(str(acc) + \"\\t\" + str(prec) + \"\\t\" + str(rec) + \"\\t\" + str(spec) + \"\\t\" + str(f1) + \"\\t\" + str(mcc)+\"\\t nan\"  + \"\\t\" + str(prc) + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Lpyyw7Ps8dq"
      },
      "source": [
        "#### Evaluate with trained FSNN-LGBM prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGbRnkLss8dq",
        "outputId": "14156d87-1483-423e-fd05-0ca1ba1fe636"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "46/46 [==============================] - 0s 2ms/step\n",
            "accuracy: 0.4925068119891008, precision: 1.0, recall: 0.4925068119891008, specificity: nan, f1-score: 0.6599726152441807, mcc: 0.0, auroc: nan, auprc: 1.0 \n",
            "0.4925068119891008\t1.0\t0.4925068119891008\tnan\t0.6599726152441807\t0.0\t nan\t1.0\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-20-ea27a8f05e5a>:57: RuntimeWarning: invalid value encountered in long_scalars\n",
            "  spec= (cm1[0,0])/(cm1[0,0]+cm1[0,1])\n"
          ]
        }
      ],
      "source": [
        "# Read new data\n",
        "\n",
        "FEATURE_NUM = seq_tensor.shape[1]\n",
        "# Create data frame of pairs and labels\n",
        "df = np.hstack([seq_tensor[seq_index1], seq_tensor[seq_index2]])\n",
        "df = np.hstack([df, class_labels.reshape(-1,1)])\n",
        "df = pd.DataFrame(df)\n",
        "df = df.sample(frac=1)\n",
        "\n",
        "# Create pairs matrix and label vector\n",
        "X = df.iloc[:,0:FEATURE_NUM*2].values\n",
        "y = df.iloc[:,FEATURE_NUM*2:].values\n",
        "\n",
        "Trainlabels=y\n",
        "\n",
        "# standard scaler\n",
        "# scaler = StandardScaler().fit(X)\n",
        "# scaler = MinMaxScaler().fit(X)\n",
        "\n",
        "# scaler = RobustScaler().fit(X)\n",
        "X = standard_scaler.transform(X)\n",
        "\n",
        "\n",
        "X1_train = X[:, :FEATURE_NUM]\n",
        "X2_train = X[:, FEATURE_NUM:]\n",
        "\n",
        "\n",
        "# Predict representation from trained neural network\n",
        "\n",
        "intermediate_layer_model = Model(inputs=model.input,outputs=model.get_layer('Merged_feature_1').output)\n",
        "# Use intermediate layer to transform pairs matrix\n",
        "intermediate_output_p1 = intermediate_layer_model.predict([X1_train,X2_train])  \n",
        "p_merge=pd.DataFrame(intermediate_output_p1)    \n",
        "\n",
        "# create dataframe use transformed pairs matrix outputs and labels\n",
        "X_train_feat=pd.concat((p_merge,pd.DataFrame(pd.DataFrame(Trainlabels))),axis=1,ignore_index=True)\n",
        "\n",
        "# write to file dataframe of transformed pairs matrix and labels\n",
        "X_train_feat.to_csv('X_train.csv',header=False, index=False)\n",
        "\n",
        "# read dataframe of transformed pairs matrix and labels\n",
        "Train=pd.read_csv(\"X_train.csv\",header=None)\n",
        "Train=Train.sample(frac=1)\n",
        "\n",
        "X=Train.iloc[:,0:64].values\n",
        "y=Train.iloc[:,64:].values.ravel()\n",
        "\n",
        "extracted_df=X_train_feat\n",
        "\n",
        "X=robust_scaler.transform(X)\n",
        "\n",
        "# Predict probability from neural network output for new data\n",
        "y_true=y\n",
        "y_pred = model_.predict(X)\n",
        "cm1=confusion_matrix(y_true, np.round(y_pred))\n",
        "acc = (cm1[0,0]+cm1[1,1])/(cm1[0,0]+cm1[0,1]+cm1[1,0]+cm1[1,1])\n",
        "spec= (cm1[0,0])/(cm1[0,0]+cm1[0,1])\n",
        "sens = (cm1[1,1])/(cm1[1,0]+cm1[1,1])\n",
        "prec=cm1[1,1]/(cm1[1,1]+cm1[0,1])\n",
        "rec=cm1[1,1]/(cm1[1,1]+cm1[1,0])\n",
        "f1 = 2 * (prec * rec) / (prec + rec)\n",
        "mcc = matthews_corrcoef(y_true, np.round(y_pred))\n",
        "\n",
        "prc = metrics.average_precision_score(y_true, y_pred)\n",
        "\n",
        "try:\n",
        "  auc = metrics.roc_auc_score(y_true, y_pred)\n",
        "  print(f'accuracy: {acc}, precision: {prec}, recall: {rec}, specificity: {spec}, f1-score: {f1}, mcc: {mcc}, auroc: {auc}, auprc: {prc} ')\n",
        "  print(str(acc) + \"\\t\" + str(prec) + \"\\t\" + str(rec) + \"\\t\" + str(spec) + \"\\t\" + str(f1) + \"\\t\" + str(mcc)+\"\\t\" + str(auc)  + \"\\t\" + str(prc) + \"\\n\")\n",
        "except ValueError:\n",
        "  print(f'accuracy: {acc}, precision: {prec}, recall: {rec}, specificity: {spec}, f1-score: {f1}, mcc: {mcc}, auroc: nan, auprc: {prc} ')\n",
        "  print(str(acc) + \"\\t\" + str(prec) + \"\\t\" + str(rec) + \"\\t\" + str(spec) + \"\\t\" + str(f1) + \"\\t\" + str(mcc)+\"\\t nan\"  + \"\\t\" + str(prc) + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4ItmWYys8ra"
      },
      "source": [
        "### HIPPIE HQ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZeUd5BNs8ra"
      },
      "source": [
        "#### Read dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7QijyNvs8ra",
        "outputId": "40a48e83-9e5e-46dc-99fb-d40769a8419f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-12-17 17:08:50--  https://raw.githubusercontent.com/anhvt00/S-HNBM/master/data/Independent-testsets/Human-sets/HIPPIE/HQ/hiphq_dict.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4151340 (4.0M) [text/plain]\n",
            "Saving to: ‘hiphq_dict.tsv’\n",
            "\n",
            "hiphq_dict.tsv      100%[===================>]   3.96M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2022-12-17 17:08:50 (303 MB/s) - ‘hiphq_dict.tsv’ saved [4151340/4151340]\n",
            "\n",
            "--2022-12-17 17:08:50--  https://raw.githubusercontent.com/anhvt00/S-HNBM/master/data/Independent-testsets/Human-sets/HIPPIE/HQ/hiphq_pairs.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 247824 (242K) [text/plain]\n",
            "Saving to: ‘hiphq_pairs.tsv’\n",
            "\n",
            "hiphq_pairs.tsv     100%[===================>] 242.02K  --.-KB/s    in 0.003s  \n",
            "\n",
            "2022-12-17 17:08:51 (76.8 MB/s) - ‘hiphq_pairs.tsv’ saved [247824/247824]\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "15489it [00:00, 419370.83it/s]\n",
            "100%|██████████| 5517/5517 [00:21<00:00, 253.76it/s]\n",
            "100%|██████████| 15489/15489 [00:00<00:00, 1826620.22it/s]\n",
            "100%|██████████| 15489/15489 [00:00<00:00, 2163571.94it/s]\n"
          ]
        }
      ],
      "source": [
        "id2seq_file = 'hiphq_dict.tsv'\n",
        "id2index = {}\n",
        "seqs = []\n",
        "index = 0\n",
        "sid1_index = 0\n",
        "sid2_index = 1\n",
        "ds_file = 'hiphq_pairs.tsv'\n",
        "label_index = 2\n",
        "use_emb = 'ac5_aph.txt'\n",
        "\n",
        "if not os.path.isfile(ds_file) or not os.path.isfile(id2seq_file):\n",
        "  !wget https://raw.githubusercontent.com/anhvt00/S-HNBM/master/data/Independent-testsets/Human-sets/HIPPIE/HQ/hiphq_dict.tsv\n",
        "  !wget https://raw.githubusercontent.com/anhvt00/S-HNBM/master/data/Independent-testsets/Human-sets/HIPPIE/HQ/hiphq_pairs.tsv\n",
        "\n",
        "\n",
        "# Create line variable as a list of protein sequences with index is the number of protein sequences\n",
        "# id2index is a dictionary of protein id and incremental index number \n",
        "for line in open(id2seq_file):\n",
        "    line = line.strip().split('\\t')\n",
        "    id2index[line[0]] = index\n",
        "    seqs.append(line[1])\n",
        "    index += 1\n",
        "\n",
        "seq_array = []\n",
        "id2_aid = {}\n",
        "sid = 0\n",
        "\n",
        "seq2t = s2t(use_emb)\n",
        "\n",
        "max_data = -1\n",
        "limit_data = max_data > 0\n",
        "raw_data = []\n",
        "skip_head = False\n",
        "x = None\n",
        "count = 0\n",
        "\n",
        "# Create sequence array as a list of protein strings\n",
        "for line in tqdm(open(ds_file)):\n",
        "    if skip_head:\n",
        "        skip_head = False\n",
        "        continue\n",
        "    line = line.rstrip('\\n').rstrip('\\r').split('\\t')\n",
        "    if id2index.get(line[sid1_index]) is None or id2index.get(line[sid2_index]) is None:\n",
        "        continue\n",
        "    if id2_aid.get(line[sid1_index]) is None:\n",
        "        id2_aid[line[sid1_index]] = sid\n",
        "        sid += 1\n",
        "        seq_array.append(seqs[id2index[line[sid1_index]]])\n",
        "    line[sid1_index] = id2_aid[line[sid1_index]]\n",
        "    if id2_aid.get(line[sid2_index]) is None:\n",
        "        id2_aid[line[sid2_index]] = sid\n",
        "        sid += 1\n",
        "        seq_array.append(seqs[id2index[line[sid2_index]]])\n",
        "    line[sid2_index] = id2_aid[line[sid2_index]]\n",
        "    raw_data.append(line)\n",
        "    if limit_data:\n",
        "        count += 1\n",
        "        if count >= max_data:\n",
        "            break\n",
        "\n",
        "len_m_seq = np.array([len(line.split()) for line in seq_array])\n",
        "avg_m_seq = int(np.average(len_m_seq)) + 1\n",
        "max_m_seq = max(len_m_seq)\n",
        "dim = seq2t.dim\n",
        "\n",
        "# seq_tensor is tensor representation of dataset having shape of (number_of_sequences, padding_length, embedding_dim_of_aa)\n",
        "# Random for distribution of class labels\n",
        "np.random.seed(42)\n",
        "np.random.shuffle(raw_data)\n",
        "seq_tensor = np.array([encode_seq(line) for line in tqdm(seq_array)])\n",
        "\n",
        "# Extract index of 1st and 2nd sequences in pairs\n",
        "seq_index1 = np.array([line[sid1_index] for line in tqdm(raw_data)])\n",
        "seq_index2 = np.array([line[sid2_index] for line in tqdm(raw_data)])\n",
        "\n",
        "# Assign labels for pairs of sequences\n",
        "class_map = {'0':1,'1':0}\n",
        "class_labels = np.zeros((len(raw_data,)))\n",
        "for i in range(len(raw_data)):\n",
        "  class_labels[i] = float(raw_data[i][label_index])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSo7cKYzs8rb",
        "outputId": "89b11301-c949-4e1a-a702-3f399571d600"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================= INFER BY TRAINED NEURAL NETWORK ON TRAINING SET ==============================\n",
            "485/485 [==============================] - 1s 2ms/step\n",
            "============= INFERENCE BY NEURAL NETWORK ===============\n",
            "accuracy: 0.4125508425334108, precision: 1.0, recall: 0.4125508425334108, specificity: nan, f1-score: 0.5841217605923488, mcc: 0.0, auroc: nan, auprc: 1.0 \n",
            "0.4125508425334108\t1.0\t0.4125508425334108\tnan\t0.5841217605923488\t0.0\t nan\t1.0\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-22-01eae5801102>:31: RuntimeWarning: invalid value encountered in long_scalars\n",
            "  spec= (cm1[0,0])/(cm1[0,0]+cm1[0,1])\n"
          ]
        }
      ],
      "source": [
        "##################################### Load Positive and Negative Dataset ##########################################################\n",
        "\n",
        "FEATURE_NUM = seq_tensor.shape[1]\n",
        "# Create data frame of pairs and labels\n",
        "df = np.hstack([seq_tensor[seq_index1], seq_tensor[seq_index2]])\n",
        "df = np.hstack([df, class_labels.reshape(-1,1)])\n",
        "df = pd.DataFrame(df)\n",
        "df = df.sample(frac=1)\n",
        "\n",
        "# Create pairs matrix and label vector\n",
        "X = df.iloc[:,0:FEATURE_NUM*2].values\n",
        "y = df.iloc[:,FEATURE_NUM*2:].values\n",
        "\n",
        "\n",
        "# standard scaler\n",
        "X = standard_scaler.transform(X)\n",
        "\n",
        "\n",
        "X1_train = X[:, :FEATURE_NUM]\n",
        "X2_train = X[:, FEATURE_NUM:]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"============================= INFER BY TRAINED NEURAL NETWORK ON TRAINING SET ==============================\")\n",
        "y_true = y\n",
        "y_pred = model.predict([X1_train, X2_train])\n",
        "cm1=confusion_matrix(y_true, np.round(y_pred))\n",
        "acc = (cm1[0,0]+cm1[1,1])/(cm1[0,0]+cm1[0,1]+cm1[1,0]+cm1[1,1])\n",
        "spec= (cm1[0,0])/(cm1[0,0]+cm1[0,1])\n",
        "sens = (cm1[1,1])/(cm1[1,0]+cm1[1,1])\n",
        "prec=cm1[1,1]/(cm1[1,1]+cm1[0,1])\n",
        "rec=cm1[1,1]/(cm1[1,1]+cm1[1,0])\n",
        "f1 = 2 * (prec * rec) / (prec + rec)\n",
        "mcc = matthews_corrcoef(y_true, np.round(y_pred))\n",
        "\n",
        "prc = metrics.average_precision_score(y_true, y_pred)\n",
        "\n",
        "print(\"============= INFERENCE BY NEURAL NETWORK ===============\")\n",
        "try:\n",
        "  auc = metrics.roc_auc_score(y_true, y_pred)\n",
        "  print(f'accuracy: {acc}, precision: {prec}, recall: {rec}, specificity: {spec}, f1-score: {f1}, mcc: {mcc}, auroc: {auc}, auprc: {prc} ')\n",
        "  print(str(acc) + \"\\t\" + str(prec) + \"\\t\" + str(rec) + \"\\t\" + str(spec) + \"\\t\" + str(f1) + \"\\t\" + str(mcc)+\"\\t\" + str(auc)  + \"\\t\" + str(prc) + \"\\n\")\n",
        "except ValueError:\n",
        "  print(f'accuracy: {acc}, precision: {prec}, recall: {rec}, specificity: {spec}, f1-score: {f1}, mcc: {mcc}, auroc: nan, auprc: {prc} ')\n",
        "  print(str(acc) + \"\\t\" + str(prec) + \"\\t\" + str(rec) + \"\\t\" + str(spec) + \"\\t\" + str(f1) + \"\\t\" + str(mcc)+\"\\t nan\"  + \"\\t\" + str(prc) + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9Jy12G1s8rc"
      },
      "source": [
        "#### Evaluate with trained FSNN-LGBM prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52L_KmqWs8rc",
        "outputId": "e3f14562-2b34-49be-ec46-dc5c3727a30d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "116/485 [======>.......................] - ETA: 0s"
          ]
        }
      ],
      "source": [
        "# Read new data\n",
        "\n",
        "FEATURE_NUM = seq_tensor.shape[1]\n",
        "# Create data frame of pairs and labels\n",
        "df = np.hstack([seq_tensor[seq_index1], seq_tensor[seq_index2]])\n",
        "df = np.hstack([df, class_labels.reshape(-1,1)])\n",
        "df = pd.DataFrame(df)\n",
        "df = df.sample(frac=1)\n",
        "\n",
        "# Create pairs matrix and label vector\n",
        "X = df.iloc[:,0:FEATURE_NUM*2].values\n",
        "y = df.iloc[:,FEATURE_NUM*2:].values\n",
        "\n",
        "Trainlabels=y\n",
        "\n",
        "# standard scaler\n",
        "# scaler = StandardScaler().fit(X)\n",
        "# scaler = MinMaxScaler().fit(X)\n",
        "\n",
        "# scaler = RobustScaler().fit(X)\n",
        "X = standard_scaler.transform(X)\n",
        "\n",
        "\n",
        "X1_train = X[:, :FEATURE_NUM]\n",
        "X2_train = X[:, FEATURE_NUM:]\n",
        "\n",
        "\n",
        "# Predict representation from trained neural network\n",
        "\n",
        "intermediate_layer_model = Model(inputs=model.input,outputs=model.get_layer('Merged_feature_1').output)\n",
        "# Use intermediate layer to transform pairs matrix\n",
        "intermediate_output_p1 = intermediate_layer_model.predict([X1_train,X2_train])  \n",
        "p_merge=pd.DataFrame(intermediate_output_p1)    \n",
        "\n",
        "# create dataframe use transformed pairs matrix outputs and labels\n",
        "X_train_feat=pd.concat((p_merge,pd.DataFrame(pd.DataFrame(Trainlabels))),axis=1,ignore_index=True)\n",
        "\n",
        "# write to file dataframe of transformed pairs matrix and labels\n",
        "X_train_feat.to_csv('X_train.csv',header=False, index=False)\n",
        "\n",
        "# read dataframe of transformed pairs matrix and labels\n",
        "Train=pd.read_csv(\"X_train.csv\",header=None)\n",
        "Train=Train.sample(frac=1)\n",
        "\n",
        "X=Train.iloc[:,0:64].values\n",
        "y=Train.iloc[:,64:].values.ravel()\n",
        "\n",
        "extracted_df=X_train_feat\n",
        "\n",
        "X=robust_scaler.transform(X)\n",
        "\n",
        "# Predict probability from neural network output for new data\n",
        "y_true=y\n",
        "y_pred = model_.predict(X)\n",
        "cm1=confusion_matrix(y_true, np.round(y_pred))\n",
        "acc = (cm1[0,0]+cm1[1,1])/(cm1[0,0]+cm1[0,1]+cm1[1,0]+cm1[1,1])\n",
        "spec= (cm1[0,0])/(cm1[0,0]+cm1[0,1])\n",
        "sens = (cm1[1,1])/(cm1[1,0]+cm1[1,1])\n",
        "prec=cm1[1,1]/(cm1[1,1]+cm1[0,1])\n",
        "rec=cm1[1,1]/(cm1[1,1]+cm1[1,0])\n",
        "f1 = 2 * (prec * rec) / (prec + rec)\n",
        "mcc = matthews_corrcoef(y_true, np.round(y_pred))\n",
        "\n",
        "prc = metrics.average_precision_score(y_true, y_pred)\n",
        "\n",
        "try:\n",
        "  auc = metrics.roc_auc_score(y_true, y_pred)\n",
        "  print(f'accuracy: {acc}, precision: {prec}, recall: {rec}, specificity: {spec}, f1-score: {f1}, mcc: {mcc}, auroc: {auc}, auprc: {prc} ')\n",
        "  print(str(acc) + \"\\t\" + str(prec) + \"\\t\" + str(rec) + \"\\t\" + str(spec) + \"\\t\" + str(f1) + \"\\t\" + str(mcc)+\"\\t\" + str(auc)  + \"\\t\" + str(prc) + \"\\n\")\n",
        "except ValueError:\n",
        "  print(f'accuracy: {acc}, precision: {prec}, recall: {rec}, specificity: {spec}, f1-score: {f1}, mcc: {mcc}, auroc: nan, auprc: {prc} ')\n",
        "  print(str(acc) + \"\\t\" + str(prec) + \"\\t\" + str(rec) + \"\\t\" + str(spec) + \"\\t\" + str(f1) + \"\\t\" + str(mcc)+\"\\t nan\"  + \"\\t\" + str(prc) + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3nOWPDZs86n"
      },
      "source": [
        "### HIPPIE LQ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTykcUv_s86o"
      },
      "source": [
        "#### Read dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "mACopDIbs86p",
        "outputId": "fac4e85c-eccb-4cfc-8124-c3b46a165aca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-12-17 17:09:16--  https://raw.githubusercontent.com/anhvt00/S-HNBM/master/data/Independent-testsets/Human-sets/HIPPIE/LQ/hiplq_dict.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6837915 (6.5M) [text/plain]\n",
            "Saving to: ‘hiplq_dict.tsv’\n",
            "\n",
            "hiplq_dict.tsv      100%[===================>]   6.52M  --.-KB/s    in 0.02s   \n",
            "\n",
            "2022-12-17 17:09:17 (377 MB/s) - ‘hiplq_dict.tsv’ saved [6837915/6837915]\n",
            "\n",
            "--2022-12-17 17:09:17--  https://raw.githubusercontent.com/anhvt00/S-HNBM/master/data/Independent-testsets/Human-sets/HIPPIE/LQ/hiplq_pairs.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1626980 (1.6M) [text/plain]\n",
            "Saving to: ‘hiplq_pairs.tsv’\n",
            "\n",
            "hiplq_pairs.tsv     100%[===================>]   1.55M  --.-KB/s    in 0.006s  \n",
            "\n",
            "2022-12-17 17:09:18 (260 MB/s) - ‘hiplq_pairs.tsv’ saved [1626980/1626980]\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "101684it [00:00, 210856.33it/s]\n",
            "100%|██████████| 10011/10011 [00:38<00:00, 257.39it/s]\n",
            "100%|██████████| 101684/101684 [00:00<00:00, 1743294.89it/s]\n",
            "100%|██████████| 101684/101684 [00:00<00:00, 1820288.55it/s]\n"
          ]
        }
      ],
      "source": [
        "id2seq_file = 'hiplq_dict.tsv'\n",
        "id2index = {}\n",
        "seqs = []\n",
        "index = 0\n",
        "sid1_index = 0\n",
        "sid2_index = 1\n",
        "ds_file = 'hiplq_pairs.tsv'\n",
        "label_index = 2\n",
        "use_emb = 'ac5_aph.txt'\n",
        "\n",
        "if not os.path.isfile(ds_file) or not os.path.isfile(id2seq_file):\n",
        "  !wget https://raw.githubusercontent.com/anhvt00/S-HNBM/master/data/Independent-testsets/Human-sets/HIPPIE/LQ/hiplq_dict.tsv\n",
        "  !wget https://raw.githubusercontent.com/anhvt00/S-HNBM/master/data/Independent-testsets/Human-sets/HIPPIE/LQ/hiplq_pairs.tsv\n",
        "\n",
        "\n",
        "\n",
        "# Create line variable as a list of protein sequences with index is the number of protein sequences\n",
        "# id2index is a dictionary of protein id and incremental index number \n",
        "for line in open(id2seq_file):\n",
        "    line = line.strip().split('\\t')\n",
        "    id2index[line[0]] = index\n",
        "    seqs.append(line[1])\n",
        "    index += 1\n",
        "\n",
        "seq_array = []\n",
        "id2_aid = {}\n",
        "sid = 0\n",
        "\n",
        "seq2t = s2t(use_emb)\n",
        "\n",
        "max_data = -1\n",
        "limit_data = max_data > 0\n",
        "raw_data = []\n",
        "skip_head = False\n",
        "x = None\n",
        "count = 0\n",
        "\n",
        "# Create sequence array as a list of protein strings\n",
        "for line in tqdm(open(ds_file)):\n",
        "    if skip_head:\n",
        "        skip_head = False\n",
        "        continue\n",
        "    line = line.rstrip('\\n').rstrip('\\r').split('\\t')\n",
        "    if id2index.get(line[sid1_index]) is None or id2index.get(line[sid2_index]) is None:\n",
        "        continue\n",
        "    if id2_aid.get(line[sid1_index]) is None:\n",
        "        id2_aid[line[sid1_index]] = sid\n",
        "        sid += 1\n",
        "        seq_array.append(seqs[id2index[line[sid1_index]]])\n",
        "    line[sid1_index] = id2_aid[line[sid1_index]]\n",
        "    if id2_aid.get(line[sid2_index]) is None:\n",
        "        id2_aid[line[sid2_index]] = sid\n",
        "        sid += 1\n",
        "        seq_array.append(seqs[id2index[line[sid2_index]]])\n",
        "    line[sid2_index] = id2_aid[line[sid2_index]]\n",
        "    raw_data.append(line)\n",
        "    if limit_data:\n",
        "        count += 1\n",
        "        if count >= max_data:\n",
        "            break\n",
        "\n",
        "len_m_seq = np.array([len(line.split()) for line in seq_array])\n",
        "avg_m_seq = int(np.average(len_m_seq)) + 1\n",
        "max_m_seq = max(len_m_seq)\n",
        "dim = seq2t.dim\n",
        "\n",
        "# seq_tensor is tensor representation of dataset having shape of (number_of_sequences, padding_length, embedding_dim_of_aa)\n",
        "# Random for distribution of class labels\n",
        "np.random.seed(42)\n",
        "np.random.shuffle(raw_data)\n",
        "seq_tensor = np.array([encode_seq(line) for line in tqdm(seq_array)])\n",
        "\n",
        "# Extract index of 1st and 2nd sequences in pairs\n",
        "seq_index1 = np.array([line[sid1_index] for line in tqdm(raw_data)])\n",
        "seq_index2 = np.array([line[sid2_index] for line in tqdm(raw_data)])\n",
        "\n",
        "# Assign labels for pairs of sequences\n",
        "class_map = {'0':1,'1':0}\n",
        "class_labels = np.zeros((len(raw_data,)))\n",
        "for i in range(len(raw_data)):\n",
        "  class_labels[i] = float(raw_data[i][label_index])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "EW46ViMcs86r",
        "outputId": "4c45ffb1-258a-499b-8d82-fce680176005"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================= INFER BY TRAINED NEURAL NETWORK ON TRAINING SET ==============================\n",
            "3178/3178 [==============================] - 6s 2ms/step\n",
            "============= INFERENCE BY NEURAL NETWORK ===============\n",
            "accuracy: 0.34825537941072343, precision: 1.0, recall: 0.34825537941072343, specificity: nan, f1-score: 0.5166015055143841, mcc: 0.0, auroc: nan, auprc: 1.0 \n",
            "0.34825537941072343\t1.0\t0.34825537941072343\tnan\t0.5166015055143841\t0.0\t nan\t1.0\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-25-01eae5801102>:31: RuntimeWarning: invalid value encountered in long_scalars\n",
            "  spec= (cm1[0,0])/(cm1[0,0]+cm1[0,1])\n"
          ]
        }
      ],
      "source": [
        "##################################### Load Positive and Negative Dataset ##########################################################\n",
        "\n",
        "FEATURE_NUM = seq_tensor.shape[1]\n",
        "# Create data frame of pairs and labels\n",
        "df = np.hstack([seq_tensor[seq_index1], seq_tensor[seq_index2]])\n",
        "df = np.hstack([df, class_labels.reshape(-1,1)])\n",
        "df = pd.DataFrame(df)\n",
        "df = df.sample(frac=1)\n",
        "\n",
        "# Create pairs matrix and label vector\n",
        "X = df.iloc[:,0:FEATURE_NUM*2].values\n",
        "y = df.iloc[:,FEATURE_NUM*2:].values\n",
        "\n",
        "\n",
        "# standard scaler\n",
        "X = standard_scaler.transform(X)\n",
        "\n",
        "\n",
        "X1_train = X[:, :FEATURE_NUM]\n",
        "X2_train = X[:, FEATURE_NUM:]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"============================= INFER BY TRAINED NEURAL NETWORK ON TRAINING SET ==============================\")\n",
        "y_true = y\n",
        "y_pred = model.predict([X1_train, X2_train])\n",
        "cm1=confusion_matrix(y_true, np.round(y_pred))\n",
        "acc = (cm1[0,0]+cm1[1,1])/(cm1[0,0]+cm1[0,1]+cm1[1,0]+cm1[1,1])\n",
        "spec= (cm1[0,0])/(cm1[0,0]+cm1[0,1])\n",
        "sens = (cm1[1,1])/(cm1[1,0]+cm1[1,1])\n",
        "prec=cm1[1,1]/(cm1[1,1]+cm1[0,1])\n",
        "rec=cm1[1,1]/(cm1[1,1]+cm1[1,0])\n",
        "f1 = 2 * (prec * rec) / (prec + rec)\n",
        "mcc = matthews_corrcoef(y_true, np.round(y_pred))\n",
        "\n",
        "prc = metrics.average_precision_score(y_true, y_pred)\n",
        "\n",
        "print(\"============= INFERENCE BY NEURAL NETWORK ===============\")\n",
        "try:\n",
        "  auc = metrics.roc_auc_score(y_true, y_pred)\n",
        "  print(f'accuracy: {acc}, precision: {prec}, recall: {rec}, specificity: {spec}, f1-score: {f1}, mcc: {mcc}, auroc: {auc}, auprc: {prc} ')\n",
        "  print(str(acc) + \"\\t\" + str(prec) + \"\\t\" + str(rec) + \"\\t\" + str(spec) + \"\\t\" + str(f1) + \"\\t\" + str(mcc)+\"\\t\" + str(auc)  + \"\\t\" + str(prc) + \"\\n\")\n",
        "except ValueError:\n",
        "  print(f'accuracy: {acc}, precision: {prec}, recall: {rec}, specificity: {spec}, f1-score: {f1}, mcc: {mcc}, auroc: nan, auprc: {prc} ')\n",
        "  print(str(acc) + \"\\t\" + str(prec) + \"\\t\" + str(rec) + \"\\t\" + str(spec) + \"\\t\" + str(f1) + \"\\t\" + str(mcc)+\"\\t nan\"  + \"\\t\" + str(prc) + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_XS77-7s86r"
      },
      "source": [
        "#### Evaluate with trained FSNN-LGBM prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Y6Pv-8rXs86s",
        "outputId": "d271124d-2525-4462-a835-8d4b996756c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3178/3178 [==============================] - 6s 2ms/step\n",
            "accuracy: 0.3803744935289721, precision: 1.0, recall: 0.3803744935289721, specificity: nan, f1-score: 0.551117823912455, mcc: 0.0, auroc: nan, auprc: 1.0 \n",
            "0.3803744935289721\t1.0\t0.3803744935289721\tnan\t0.551117823912455\t0.0\t nan\t1.0\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-26-ea27a8f05e5a>:57: RuntimeWarning: invalid value encountered in long_scalars\n",
            "  spec= (cm1[0,0])/(cm1[0,0]+cm1[0,1])\n"
          ]
        }
      ],
      "source": [
        "# Read new data\n",
        "\n",
        "FEATURE_NUM = seq_tensor.shape[1]\n",
        "# Create data frame of pairs and labels\n",
        "df = np.hstack([seq_tensor[seq_index1], seq_tensor[seq_index2]])\n",
        "df = np.hstack([df, class_labels.reshape(-1,1)])\n",
        "df = pd.DataFrame(df)\n",
        "df = df.sample(frac=1)\n",
        "\n",
        "# Create pairs matrix and label vector\n",
        "X = df.iloc[:,0:FEATURE_NUM*2].values\n",
        "y = df.iloc[:,FEATURE_NUM*2:].values\n",
        "\n",
        "Trainlabels=y\n",
        "\n",
        "# standard scaler\n",
        "# scaler = StandardScaler().fit(X)\n",
        "# scaler = MinMaxScaler().fit(X)\n",
        "\n",
        "# scaler = RobustScaler().fit(X)\n",
        "X = standard_scaler.transform(X)\n",
        "\n",
        "\n",
        "X1_train = X[:, :FEATURE_NUM]\n",
        "X2_train = X[:, FEATURE_NUM:]\n",
        "\n",
        "\n",
        "# Predict representation from trained neural network\n",
        "\n",
        "intermediate_layer_model = Model(inputs=model.input,outputs=model.get_layer('Merged_feature_1').output)\n",
        "# Use intermediate layer to transform pairs matrix\n",
        "intermediate_output_p1 = intermediate_layer_model.predict([X1_train,X2_train])  \n",
        "p_merge=pd.DataFrame(intermediate_output_p1)    \n",
        "\n",
        "# create dataframe use transformed pairs matrix outputs and labels\n",
        "X_train_feat=pd.concat((p_merge,pd.DataFrame(pd.DataFrame(Trainlabels))),axis=1,ignore_index=True)\n",
        "\n",
        "# write to file dataframe of transformed pairs matrix and labels\n",
        "X_train_feat.to_csv('X_train.csv',header=False, index=False)\n",
        "\n",
        "# read dataframe of transformed pairs matrix and labels\n",
        "Train=pd.read_csv(\"X_train.csv\",header=None)\n",
        "Train=Train.sample(frac=1)\n",
        "\n",
        "X=Train.iloc[:,0:64].values\n",
        "y=Train.iloc[:,64:].values.ravel()\n",
        "\n",
        "extracted_df=X_train_feat\n",
        "\n",
        "X=robust_scaler.transform(X)\n",
        "\n",
        "# Predict probability from neural network output for new data\n",
        "y_true=y\n",
        "y_pred = model_.predict(X)\n",
        "cm1=confusion_matrix(y_true, np.round(y_pred))\n",
        "acc = (cm1[0,0]+cm1[1,1])/(cm1[0,0]+cm1[0,1]+cm1[1,0]+cm1[1,1])\n",
        "spec= (cm1[0,0])/(cm1[0,0]+cm1[0,1])\n",
        "sens = (cm1[1,1])/(cm1[1,0]+cm1[1,1])\n",
        "prec=cm1[1,1]/(cm1[1,1]+cm1[0,1])\n",
        "rec=cm1[1,1]/(cm1[1,1]+cm1[1,0])\n",
        "f1 = 2 * (prec * rec) / (prec + rec)\n",
        "mcc = matthews_corrcoef(y_true, np.round(y_pred))\n",
        "\n",
        "prc = metrics.average_precision_score(y_true, y_pred)\n",
        "\n",
        "try:\n",
        "  auc = metrics.roc_auc_score(y_true, y_pred)\n",
        "  print(f'accuracy: {acc}, precision: {prec}, recall: {rec}, specificity: {spec}, f1-score: {f1}, mcc: {mcc}, auroc: {auc}, auprc: {prc} ')\n",
        "  print(str(acc) + \"\\t\" + str(prec) + \"\\t\" + str(rec) + \"\\t\" + str(spec) + \"\\t\" + str(f1) + \"\\t\" + str(mcc)+\"\\t\" + str(auc)  + \"\\t\" + str(prc) + \"\\n\")\n",
        "except ValueError:\n",
        "  print(f'accuracy: {acc}, precision: {prec}, recall: {rec}, specificity: {spec}, f1-score: {f1}, mcc: {mcc}, auroc: nan, auprc: {prc} ')\n",
        "  print(str(acc) + \"\\t\" + str(prec) + \"\\t\" + str(rec) + \"\\t\" + str(spec) + \"\\t\" + str(f1) + \"\\t\" + str(mcc)+\"\\t nan\"  + \"\\t\" + str(prc) + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObHQ2AUCtU8w"
      },
      "source": [
        "## Evaluation on cross-species datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXgqTUZ4tU8w"
      },
      "source": [
        "### E. coli"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7y3iLjctU8x"
      },
      "source": [
        "#### Read dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1d6JRt3ltU8x",
        "outputId": "25940863-8b41-445a-f59d-261909e9e178"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-12-17 17:10:20--  https://raw.githubusercontent.com/anhvt00/MCAPS/master/data/Independent-testsets/cross-species/pairs/ecoli_test.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 768468 (750K) [text/plain]\n",
            "Saving to: ‘ecoli_test.tsv’\n",
            "\n",
            "ecoli_test.tsv      100%[===================>] 750.46K  --.-KB/s    in 0.007s  \n",
            "\n",
            "2022-12-17 17:10:21 (111 MB/s) - ‘ecoli_test.tsv’ saved [768468/768468]\n",
            "\n",
            "--2022-12-17 17:10:21--  https://raw.githubusercontent.com/anhvt00/MCAPS/master/data/Independent-testsets/cross-species/seqs/ecoli_dict.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5395789 (5.1M) [text/plain]\n",
            "Saving to: ‘ecoli_dict.tsv’\n",
            "\n",
            "ecoli_dict.tsv      100%[===================>]   5.15M  --.-KB/s    in 0.02s   \n",
            "\n",
            "2022-12-17 17:10:21 (272 MB/s) - ‘ecoli_dict.tsv’ saved [5395789/5395789]\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "22000it [00:00, 404300.38it/s]\n",
            "100%|██████████| 7138/7138 [00:13<00:00, 535.10it/s]\n",
            "100%|██████████| 22000/22000 [00:00<00:00, 1751707.35it/s]\n",
            "100%|██████████| 22000/22000 [00:00<00:00, 1977893.98it/s]\n"
          ]
        }
      ],
      "source": [
        "id2seq_file = 'ecoli_dict.tsv'\n",
        "id2index = {}\n",
        "seqs = []\n",
        "index = 0\n",
        "sid1_index = 0\n",
        "sid2_index = 1\n",
        "ds_file = 'ecoli_test.tsv'\n",
        "label_index = 2\n",
        "use_emb = 'ac5_aph.txt'\n",
        "\n",
        "if not os.path.isfile(ds_file) or not os.path.isfile(id2seq_file):\n",
        "  !wget https://raw.githubusercontent.com/anhvt00/MCAPS/master/data/Independent-testsets/cross-species/pairs/ecoli_test.tsv\n",
        "  !wget https://raw.githubusercontent.com/anhvt00/MCAPS/master/data/Independent-testsets/cross-species/seqs/ecoli_dict.tsv\n",
        "\n",
        "\n",
        "# Create line variable as a list of protein sequences with index is the number of protein sequences\n",
        "# id2index is a dictionary of protein id and incremental index number \n",
        "for line in open(id2seq_file):\n",
        "    line = line.strip().split('\\t')\n",
        "    id2index[line[0]] = index\n",
        "    seqs.append(line[1])\n",
        "    index += 1\n",
        "\n",
        "seq_array = []\n",
        "id2_aid = {}\n",
        "sid = 0\n",
        "\n",
        "seq2t = s2t(use_emb)\n",
        "\n",
        "max_data = -1\n",
        "limit_data = max_data > 0\n",
        "raw_data = []\n",
        "skip_head = False\n",
        "x = None\n",
        "count = 0\n",
        "\n",
        "# Create sequence array as a list of protein strings\n",
        "for line in tqdm(open(ds_file)):\n",
        "    if skip_head:\n",
        "        skip_head = False\n",
        "        continue\n",
        "    line = line.rstrip('\\n').rstrip('\\r').split('\\t')\n",
        "    if id2index.get(line[sid1_index]) is None or id2index.get(line[sid2_index]) is None:\n",
        "        continue\n",
        "    if id2_aid.get(line[sid1_index]) is None:\n",
        "        id2_aid[line[sid1_index]] = sid\n",
        "        sid += 1\n",
        "        seq_array.append(seqs[id2index[line[sid1_index]]])\n",
        "    line[sid1_index] = id2_aid[line[sid1_index]]\n",
        "    if id2_aid.get(line[sid2_index]) is None:\n",
        "        id2_aid[line[sid2_index]] = sid\n",
        "        sid += 1\n",
        "        seq_array.append(seqs[id2index[line[sid2_index]]])\n",
        "    line[sid2_index] = id2_aid[line[sid2_index]]\n",
        "    raw_data.append(line)\n",
        "    if limit_data:\n",
        "        count += 1\n",
        "        if count >= max_data:\n",
        "            break\n",
        "\n",
        "len_m_seq = np.array([len(line.split()) for line in seq_array])\n",
        "avg_m_seq = int(np.average(len_m_seq)) + 1\n",
        "max_m_seq = max(len_m_seq)\n",
        "dim = seq2t.dim\n",
        "\n",
        "# seq_tensor is tensor representation of dataset having shape of (number_of_sequences, padding_length, embedding_dim_of_aa)\n",
        "# Random for distribution of class labels\n",
        "np.random.seed(42)\n",
        "np.random.shuffle(raw_data)\n",
        "seq_tensor = np.array([encode_seq(line) for line in tqdm(seq_array)])\n",
        "\n",
        "# Extract index of 1st and 2nd sequences in pairs\n",
        "seq_index1 = np.array([line[sid1_index] for line in tqdm(raw_data)])\n",
        "seq_index2 = np.array([line[sid2_index] for line in tqdm(raw_data)])\n",
        "\n",
        "# Assign labels for pairs of sequences\n",
        "class_map = {'0':1,'1':0}\n",
        "class_labels = np.zeros((len(raw_data,)))\n",
        "for i in range(len(raw_data)):\n",
        "  class_labels[i] = float(raw_data[i][label_index])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2PiFY-dytU8x",
        "outputId": "29a7b466-ff7a-41c1-8900-7f0196e8a029"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================= INFER BY TRAINED NEURAL NETWORK ON TRAINING SET ==============================\n",
            "688/688 [==============================] - 1s 2ms/step\n",
            "============= INFERENCE BY NEURAL NETWORK ===============\n",
            "accuracy: 0.8594545454545455, precision: 0.20613562970936491, recall: 0.1915, specificity: 0.92625, f1-score: 0.19854847071021253, mcc: 0.12173536460387874, auroc: 0.6732481000000001, auprc: 0.15631856627686794 \n",
            "0.8594545454545455\t0.20613562970936491\t0.1915\t0.92625\t0.19854847071021253\t0.12173536460387874\t0.6732481000000001\t0.15631856627686794\n",
            "\n"
          ]
        }
      ],
      "source": [
        "##################################### Load Positive and Negative Dataset ##########################################################\n",
        "\n",
        "FEATURE_NUM = seq_tensor.shape[1]\n",
        "# Create data frame of pairs and labels\n",
        "df = np.hstack([seq_tensor[seq_index1], seq_tensor[seq_index2]])\n",
        "df = np.hstack([df, class_labels.reshape(-1,1)])\n",
        "df = pd.DataFrame(df)\n",
        "df = df.sample(frac=1)\n",
        "\n",
        "# Create pairs matrix and label vector\n",
        "X = df.iloc[:,0:FEATURE_NUM*2].values\n",
        "y = df.iloc[:,FEATURE_NUM*2:].values\n",
        "\n",
        "\n",
        "# standard scaler\n",
        "X = standard_scaler.transform(X)\n",
        "\n",
        "\n",
        "X1_train = X[:, :FEATURE_NUM]\n",
        "X2_train = X[:, FEATURE_NUM:]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"============================= INFER BY TRAINED NEURAL NETWORK ON TRAINING SET ==============================\")\n",
        "y_true = y\n",
        "y_pred = model.predict([X1_train, X2_train])\n",
        "cm1=confusion_matrix(y_true, np.round(y_pred))\n",
        "acc = (cm1[0,0]+cm1[1,1])/(cm1[0,0]+cm1[0,1]+cm1[1,0]+cm1[1,1])\n",
        "spec= (cm1[0,0])/(cm1[0,0]+cm1[0,1])\n",
        "sens = (cm1[1,1])/(cm1[1,0]+cm1[1,1])\n",
        "prec=cm1[1,1]/(cm1[1,1]+cm1[0,1])\n",
        "rec=cm1[1,1]/(cm1[1,1]+cm1[1,0])\n",
        "f1 = 2 * (prec * rec) / (prec + rec)\n",
        "mcc = matthews_corrcoef(y_true, np.round(y_pred))\n",
        "\n",
        "prc = metrics.average_precision_score(y_true, y_pred)\n",
        "\n",
        "print(\"============= INFERENCE BY NEURAL NETWORK ===============\")\n",
        "try:\n",
        "  auc = metrics.roc_auc_score(y_true, y_pred)\n",
        "  print(f'accuracy: {acc}, precision: {prec}, recall: {rec}, specificity: {spec}, f1-score: {f1}, mcc: {mcc}, auroc: {auc}, auprc: {prc} ')\n",
        "  print(str(acc) + \"\\t\" + str(prec) + \"\\t\" + str(rec) + \"\\t\" + str(spec) + \"\\t\" + str(f1) + \"\\t\" + str(mcc)+\"\\t\" + str(auc)  + \"\\t\" + str(prc) + \"\\n\")\n",
        "except ValueError:\n",
        "  print(f'accuracy: {acc}, precision: {prec}, recall: {rec}, specificity: {spec}, f1-score: {f1}, mcc: {mcc}, auroc: nan, auprc: {prc} ')\n",
        "  print(str(acc) + \"\\t\" + str(prec) + \"\\t\" + str(rec) + \"\\t\" + str(spec) + \"\\t\" + str(f1) + \"\\t\" + str(mcc)+\"\\t nan\"  + \"\\t\" + str(prc) + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5YJn1GxtU8x"
      },
      "source": [
        "#### Evaluate with trained FSNN-LGBM prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "A8Re30RTtU8x",
        "outputId": "d20a9976-a66d-4153-d891-fdb4ebd19d5e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "688/688 [==============================] - 1s 2ms/step\n",
            "accuracy: 0.8146818181818182, precision: 0.15021892893230043, recall: 0.223, specificity: 0.87385, f1-score: 0.17951298047896957, mcc: 0.08148801096905704, auroc: 0.548425, auprc: 0.10413518478826664 \n",
            "0.8146818181818182\t0.15021892893230043\t0.223\t0.87385\t0.17951298047896957\t0.08148801096905704\t0.548425\t0.10413518478826664\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Read new data\n",
        "\n",
        "FEATURE_NUM = seq_tensor.shape[1]\n",
        "# Create data frame of pairs and labels\n",
        "df = np.hstack([seq_tensor[seq_index1], seq_tensor[seq_index2]])\n",
        "df = np.hstack([df, class_labels.reshape(-1,1)])\n",
        "df = pd.DataFrame(df)\n",
        "df = df.sample(frac=1)\n",
        "\n",
        "# Create pairs matrix and label vector\n",
        "X = df.iloc[:,0:FEATURE_NUM*2].values\n",
        "y = df.iloc[:,FEATURE_NUM*2:].values\n",
        "\n",
        "Trainlabels=y\n",
        "\n",
        "# standard scaler\n",
        "# scaler = StandardScaler().fit(X)\n",
        "# scaler = MinMaxScaler().fit(X)\n",
        "\n",
        "# scaler = RobustScaler().fit(X)\n",
        "X = standard_scaler.transform(X)\n",
        "\n",
        "\n",
        "X1_train = X[:, :FEATURE_NUM]\n",
        "X2_train = X[:, FEATURE_NUM:]\n",
        "\n",
        "\n",
        "# Predict representation from trained neural network\n",
        "\n",
        "intermediate_layer_model = Model(inputs=model.input,outputs=model.get_layer('Merged_feature_1').output)\n",
        "# Use intermediate layer to transform pairs matrix\n",
        "intermediate_output_p1 = intermediate_layer_model.predict([X1_train,X2_train])  \n",
        "p_merge=pd.DataFrame(intermediate_output_p1)    \n",
        "\n",
        "# create dataframe use transformed pairs matrix outputs and labels\n",
        "X_train_feat=pd.concat((p_merge,pd.DataFrame(pd.DataFrame(Trainlabels))),axis=1,ignore_index=True)\n",
        "\n",
        "# write to file dataframe of transformed pairs matrix and labels\n",
        "X_train_feat.to_csv('X_train.csv',header=False, index=False)\n",
        "\n",
        "# read dataframe of transformed pairs matrix and labels\n",
        "Train=pd.read_csv(\"X_train.csv\",header=None)\n",
        "Train=Train.sample(frac=1)\n",
        "\n",
        "X=Train.iloc[:,0:64].values\n",
        "y=Train.iloc[:,64:].values.ravel()\n",
        "\n",
        "extracted_df=X_train_feat\n",
        "\n",
        "X=robust_scaler.transform(X)\n",
        "\n",
        "# Predict probability from neural network output for new data\n",
        "y_true=y\n",
        "y_pred = model_.predict(X)\n",
        "cm1=confusion_matrix(y_true, np.round(y_pred))\n",
        "acc = (cm1[0,0]+cm1[1,1])/(cm1[0,0]+cm1[0,1]+cm1[1,0]+cm1[1,1])\n",
        "spec= (cm1[0,0])/(cm1[0,0]+cm1[0,1])\n",
        "sens = (cm1[1,1])/(cm1[1,0]+cm1[1,1])\n",
        "prec=cm1[1,1]/(cm1[1,1]+cm1[0,1])\n",
        "rec=cm1[1,1]/(cm1[1,1]+cm1[1,0])\n",
        "f1 = 2 * (prec * rec) / (prec + rec)\n",
        "mcc = matthews_corrcoef(y_true, np.round(y_pred))\n",
        "\n",
        "prc = metrics.average_precision_score(y_true, y_pred)\n",
        "\n",
        "try:\n",
        "  auc = metrics.roc_auc_score(y_true, y_pred)\n",
        "  print(f'accuracy: {acc}, precision: {prec}, recall: {rec}, specificity: {spec}, f1-score: {f1}, mcc: {mcc}, auroc: {auc}, auprc: {prc} ')\n",
        "  print(str(acc) + \"\\t\" + str(prec) + \"\\t\" + str(rec) + \"\\t\" + str(spec) + \"\\t\" + str(f1) + \"\\t\" + str(mcc)+\"\\t\" + str(auc)  + \"\\t\" + str(prc) + \"\\n\")\n",
        "except ValueError:\n",
        "  print(f'accuracy: {acc}, precision: {prec}, recall: {rec}, specificity: {spec}, f1-score: {f1}, mcc: {mcc}, auroc: nan, auprc: {prc} ')\n",
        "  print(str(acc) + \"\\t\" + str(prec) + \"\\t\" + str(rec) + \"\\t\" + str(spec) + \"\\t\" + str(f1) + \"\\t\" + str(mcc)+\"\\t nan\"  + \"\\t\" + str(prc) + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4dSd5wmtU8x"
      },
      "source": [
        "### Fly"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmd6gt1StU8x"
      },
      "source": [
        "#### Read dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Imv80ATitU8x",
        "outputId": "d99cdf9c-3e04-4e5b-feaa-26ffcfdbb4f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-12-17 17:10:40--  https://raw.githubusercontent.com/anhvt00/MCAPS/master/data/Independent-testsets/cross-species/pairs/fly_test.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1980000 (1.9M) [text/plain]\n",
            "Saving to: ‘fly_test.tsv’\n",
            "\n",
            "fly_test.tsv        100%[===================>]   1.89M  --.-KB/s    in 0.007s  \n",
            "\n",
            "2022-12-17 17:10:41 (276 MB/s) - ‘fly_test.tsv’ saved [1980000/1980000]\n",
            "\n",
            "--2022-12-17 17:10:41--  https://raw.githubusercontent.com/anhvt00/MCAPS/master/data/Independent-testsets/cross-species/seqs/fly_dict.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7839133 (7.5M) [text/plain]\n",
            "Saving to: ‘fly_dict.tsv’\n",
            "\n",
            "fly_dict.tsv        100%[===================>]   7.48M  --.-KB/s    in 0.02s   \n",
            "\n",
            "2022-12-17 17:10:42 (326 MB/s) - ‘fly_dict.tsv’ saved [7839133/7839133]\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "55000it [00:00, 357257.80it/s]\n",
            "100%|██████████| 19213/19213 [00:44<00:00, 427.85it/s]\n",
            "100%|██████████| 55000/55000 [00:00<00:00, 1847000.91it/s]\n",
            "100%|██████████| 55000/55000 [00:00<00:00, 1982440.77it/s]\n"
          ]
        }
      ],
      "source": [
        "id2seq_file = 'fly_dict.tsv'\n",
        "id2index = {}\n",
        "seqs = []\n",
        "index = 0\n",
        "sid1_index = 0\n",
        "sid2_index = 1\n",
        "ds_file = 'fly_test.tsv'\n",
        "label_index = 2\n",
        "use_emb = 'ac5_aph.txt'\n",
        "\n",
        "if not os.path.isfile(ds_file) or not os.path.isfile(id2seq_file):\n",
        "  !wget https://raw.githubusercontent.com/anhvt00/MCAPS/master/data/Independent-testsets/cross-species/pairs/fly_test.tsv\n",
        "  !wget https://raw.githubusercontent.com/anhvt00/MCAPS/master/data/Independent-testsets/cross-species/seqs/fly_dict.tsv\n",
        "\n",
        "\n",
        "# Create line variable as a list of protein sequences with index is the number of protein sequences\n",
        "# id2index is a dictionary of protein id and incremental index number \n",
        "for line in open(id2seq_file):\n",
        "    line = line.strip().split('\\t')\n",
        "    id2index[line[0]] = index\n",
        "    seqs.append(line[1])\n",
        "    index += 1\n",
        "\n",
        "seq_array = []\n",
        "id2_aid = {}\n",
        "sid = 0\n",
        "\n",
        "seq2t = s2t(use_emb)\n",
        "\n",
        "max_data = -1\n",
        "limit_data = max_data > 0\n",
        "raw_data = []\n",
        "skip_head = False\n",
        "x = None\n",
        "count = 0\n",
        "\n",
        "# Create sequence array as a list of protein strings\n",
        "for line in tqdm(open(ds_file)):\n",
        "    if skip_head:\n",
        "        skip_head = False\n",
        "        continue\n",
        "    line = line.rstrip('\\n').rstrip('\\r').split('\\t')\n",
        "    if id2index.get(line[sid1_index]) is None or id2index.get(line[sid2_index]) is None:\n",
        "        continue\n",
        "    if id2_aid.get(line[sid1_index]) is None:\n",
        "        id2_aid[line[sid1_index]] = sid\n",
        "        sid += 1\n",
        "        seq_array.append(seqs[id2index[line[sid1_index]]])\n",
        "    line[sid1_index] = id2_aid[line[sid1_index]]\n",
        "    if id2_aid.get(line[sid2_index]) is None:\n",
        "        id2_aid[line[sid2_index]] = sid\n",
        "        sid += 1\n",
        "        seq_array.append(seqs[id2index[line[sid2_index]]])\n",
        "    line[sid2_index] = id2_aid[line[sid2_index]]\n",
        "    raw_data.append(line)\n",
        "    if limit_data:\n",
        "        count += 1\n",
        "        if count >= max_data:\n",
        "            break\n",
        "\n",
        "len_m_seq = np.array([len(line.split()) for line in seq_array])\n",
        "avg_m_seq = int(np.average(len_m_seq)) + 1\n",
        "max_m_seq = max(len_m_seq)\n",
        "dim = seq2t.dim\n",
        "\n",
        "# seq_tensor is tensor representation of dataset having shape of (number_of_sequences, padding_length, embedding_dim_of_aa)\n",
        "# Random for distribution of class labels\n",
        "np.random.seed(42)\n",
        "np.random.shuffle(raw_data)\n",
        "seq_tensor = np.array([encode_seq(line) for line in tqdm(seq_array)])\n",
        "\n",
        "# Extract index of 1st and 2nd sequences in pairs\n",
        "seq_index1 = np.array([line[sid1_index] for line in tqdm(raw_data)])\n",
        "seq_index2 = np.array([line[sid2_index] for line in tqdm(raw_data)])\n",
        "\n",
        "# Assign labels for pairs of sequences\n",
        "class_map = {'0':1,'1':0}\n",
        "class_labels = np.zeros((len(raw_data,)))\n",
        "for i in range(len(raw_data)):\n",
        "  class_labels[i] = float(raw_data[i][label_index])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "blVA6y7xtU8y",
        "outputId": "d18112bd-26ae-4976-ba7d-c78bcb867f81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================= INFER BY TRAINED NEURAL NETWORK ON TRAINING SET ==============================\n",
            "1719/1719 [==============================] - 3s 2ms/step\n",
            "============= INFERENCE BY NEURAL NETWORK ===============\n",
            "accuracy: 0.8688363636363636, precision: 0.2612165660051769, recall: 0.2422, specificity: 0.9315, f1-score: 0.25134910751349104, mcc: 0.17973721227928793, auroc: 0.707428478, auprc: 0.20583744604505744 \n",
            "0.8688363636363636\t0.2612165660051769\t0.2422\t0.9315\t0.25134910751349104\t0.17973721227928793\t0.707428478\t0.20583744604505744\n",
            "\n"
          ]
        }
      ],
      "source": [
        "##################################### Load Positive and Negative Dataset ##########################################################\n",
        "\n",
        "FEATURE_NUM = seq_tensor.shape[1]\n",
        "# Create data frame of pairs and labels\n",
        "df = np.hstack([seq_tensor[seq_index1], seq_tensor[seq_index2]])\n",
        "df = np.hstack([df, class_labels.reshape(-1,1)])\n",
        "df = pd.DataFrame(df)\n",
        "df = df.sample(frac=1)\n",
        "\n",
        "# Create pairs matrix and label vector\n",
        "X = df.iloc[:,0:FEATURE_NUM*2].values\n",
        "y = df.iloc[:,FEATURE_NUM*2:].values\n",
        "\n",
        "\n",
        "# standard scaler\n",
        "X = standard_scaler.transform(X)\n",
        "\n",
        "\n",
        "X1_train = X[:, :FEATURE_NUM]\n",
        "X2_train = X[:, FEATURE_NUM:]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"============================= INFER BY TRAINED NEURAL NETWORK ON TRAINING SET ==============================\")\n",
        "y_true = y\n",
        "y_pred = model.predict([X1_train, X2_train])\n",
        "cm1=confusion_matrix(y_true, np.round(y_pred))\n",
        "acc = (cm1[0,0]+cm1[1,1])/(cm1[0,0]+cm1[0,1]+cm1[1,0]+cm1[1,1])\n",
        "spec= (cm1[0,0])/(cm1[0,0]+cm1[0,1])\n",
        "sens = (cm1[1,1])/(cm1[1,0]+cm1[1,1])\n",
        "prec=cm1[1,1]/(cm1[1,1]+cm1[0,1])\n",
        "rec=cm1[1,1]/(cm1[1,1]+cm1[1,0])\n",
        "f1 = 2 * (prec * rec) / (prec + rec)\n",
        "mcc = matthews_corrcoef(y_true, np.round(y_pred))\n",
        "\n",
        "prc = metrics.average_precision_score(y_true, y_pred)\n",
        "\n",
        "print(\"============= INFERENCE BY NEURAL NETWORK ===============\")\n",
        "try:\n",
        "  auc = metrics.roc_auc_score(y_true, y_pred)\n",
        "  print(f'accuracy: {acc}, precision: {prec}, recall: {rec}, specificity: {spec}, f1-score: {f1}, mcc: {mcc}, auroc: {auc}, auprc: {prc} ')\n",
        "  print(str(acc) + \"\\t\" + str(prec) + \"\\t\" + str(rec) + \"\\t\" + str(spec) + \"\\t\" + str(f1) + \"\\t\" + str(mcc)+\"\\t\" + str(auc)  + \"\\t\" + str(prc) + \"\\n\")\n",
        "except ValueError:\n",
        "  print(f'accuracy: {acc}, precision: {prec}, recall: {rec}, specificity: {spec}, f1-score: {f1}, mcc: {mcc}, auroc: nan, auprc: {prc} ')\n",
        "  print(str(acc) + \"\\t\" + str(prec) + \"\\t\" + str(rec) + \"\\t\" + str(spec) + \"\\t\" + str(f1) + \"\\t\" + str(mcc)+\"\\t nan\"  + \"\\t\" + str(prc) + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVgp81fwtU8y"
      },
      "source": [
        "#### Evaluate with trained FSNN-LGBM prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "QyE-wTyRtU8y",
        "outputId": "358507f1-9e78-4f3c-ffc0-151e511fa490"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1719/1719 [==============================] - 3s 2ms/step\n",
            "accuracy: 0.8284909090909091, precision: 0.20458483273357322, recall: 0.307, specificity: 0.88064, f1-score: 0.2455410701431656, mcc: 0.15716102969112408, auroc: 0.5938199999999999, auprc: 0.125807543649207 \n",
            "0.8284909090909091\t0.20458483273357322\t0.307\t0.88064\t0.2455410701431656\t0.15716102969112408\t0.5938199999999999\t0.125807543649207\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Read new data\n",
        "\n",
        "FEATURE_NUM = seq_tensor.shape[1]\n",
        "# Create data frame of pairs and labels\n",
        "df = np.hstack([seq_tensor[seq_index1], seq_tensor[seq_index2]])\n",
        "df = np.hstack([df, class_labels.reshape(-1,1)])\n",
        "df = pd.DataFrame(df)\n",
        "df = df.sample(frac=1)\n",
        "\n",
        "# Create pairs matrix and label vector\n",
        "X = df.iloc[:,0:FEATURE_NUM*2].values\n",
        "y = df.iloc[:,FEATURE_NUM*2:].values\n",
        "\n",
        "Trainlabels=y\n",
        "\n",
        "# standard scaler\n",
        "# scaler = StandardScaler().fit(X)\n",
        "# scaler = MinMaxScaler().fit(X)\n",
        "\n",
        "# scaler = RobustScaler().fit(X)\n",
        "X = standard_scaler.transform(X)\n",
        "\n",
        "\n",
        "X1_train = X[:, :FEATURE_NUM]\n",
        "X2_train = X[:, FEATURE_NUM:]\n",
        "\n",
        "\n",
        "# Predict representation from trained neural network\n",
        "\n",
        "intermediate_layer_model = Model(inputs=model.input,outputs=model.get_layer('Merged_feature_1').output)\n",
        "# Use intermediate layer to transform pairs matrix\n",
        "intermediate_output_p1 = intermediate_layer_model.predict([X1_train,X2_train])  \n",
        "p_merge=pd.DataFrame(intermediate_output_p1)    \n",
        "\n",
        "# create dataframe use transformed pairs matrix outputs and labels\n",
        "X_train_feat=pd.concat((p_merge,pd.DataFrame(pd.DataFrame(Trainlabels))),axis=1,ignore_index=True)\n",
        "\n",
        "# write to file dataframe of transformed pairs matrix and labels\n",
        "X_train_feat.to_csv('X_train.csv',header=False, index=False)\n",
        "\n",
        "# read dataframe of transformed pairs matrix and labels\n",
        "Train=pd.read_csv(\"X_train.csv\",header=None)\n",
        "Train=Train.sample(frac=1)\n",
        "\n",
        "X=Train.iloc[:,0:64].values\n",
        "y=Train.iloc[:,64:].values.ravel()\n",
        "\n",
        "extracted_df=X_train_feat\n",
        "\n",
        "X=robust_scaler.transform(X)\n",
        "\n",
        "# Predict probability from neural network output for new data\n",
        "y_true=y\n",
        "y_pred = model_.predict(X)\n",
        "cm1=confusion_matrix(y_true, np.round(y_pred))\n",
        "acc = (cm1[0,0]+cm1[1,1])/(cm1[0,0]+cm1[0,1]+cm1[1,0]+cm1[1,1])\n",
        "spec= (cm1[0,0])/(cm1[0,0]+cm1[0,1])\n",
        "sens = (cm1[1,1])/(cm1[1,0]+cm1[1,1])\n",
        "prec=cm1[1,1]/(cm1[1,1]+cm1[0,1])\n",
        "rec=cm1[1,1]/(cm1[1,1]+cm1[1,0])\n",
        "f1 = 2 * (prec * rec) / (prec + rec)\n",
        "mcc = matthews_corrcoef(y_true, np.round(y_pred))\n",
        "\n",
        "prc = metrics.average_precision_score(y_true, y_pred)\n",
        "\n",
        "try:\n",
        "  auc = metrics.roc_auc_score(y_true, y_pred)\n",
        "  print(f'accuracy: {acc}, precision: {prec}, recall: {rec}, specificity: {spec}, f1-score: {f1}, mcc: {mcc}, auroc: {auc}, auprc: {prc} ')\n",
        "  print(str(acc) + \"\\t\" + str(prec) + \"\\t\" + str(rec) + \"\\t\" + str(spec) + \"\\t\" + str(f1) + \"\\t\" + str(mcc)+\"\\t\" + str(auc)  + \"\\t\" + str(prc) + \"\\n\")\n",
        "except ValueError:\n",
        "  print(f'accuracy: {acc}, precision: {prec}, recall: {rec}, specificity: {spec}, f1-score: {f1}, mcc: {mcc}, auroc: nan, auprc: {prc} ')\n",
        "  print(str(acc) + \"\\t\" + str(prec) + \"\\t\" + str(rec) + \"\\t\" + str(spec) + \"\\t\" + str(f1) + \"\\t\" + str(mcc)+\"\\t nan\"  + \"\\t\" + str(prc) + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ym_X68DAtU8y"
      },
      "source": [
        "### Mouse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjhb9gWftU8y"
      },
      "source": [
        "#### Read dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "nFMItvoutU8y",
        "outputId": "e5effd06-be08-466e-effe-69edf0715a6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-12-17 17:11:39--  https://raw.githubusercontent.com/anhvt00/MCAPS/master/data/Independent-testsets/cross-species/pairs/mouse_test.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2860000 (2.7M) [text/plain]\n",
            "Saving to: ‘mouse_test.tsv’\n",
            "\n",
            "mouse_test.tsv      100%[===================>]   2.73M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2022-12-17 17:11:40 (252 MB/s) - ‘mouse_test.tsv’ saved [2860000/2860000]\n",
            "\n",
            "--2022-12-17 17:11:40--  https://raw.githubusercontent.com/anhvt00/MCAPS/master/data/Independent-testsets/cross-species/seqs/mouse_dict.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 16989035 (16M) [text/plain]\n",
            "Saving to: ‘mouse_dict.tsv’\n",
            "\n",
            "mouse_dict.tsv      100%[===================>]  16.20M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2022-12-17 17:11:41 (422 MB/s) - ‘mouse_dict.tsv’ saved [16989035/16989035]\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "55000it [00:00, 148263.96it/s]\n",
            "100%|██████████| 37497/37497 [01:28<00:00, 425.19it/s]\n",
            "100%|██████████| 55000/55000 [00:00<00:00, 1730790.79it/s]\n",
            "100%|██████████| 55000/55000 [00:00<00:00, 2011042.80it/s]\n"
          ]
        }
      ],
      "source": [
        "id2seq_file = 'mouse_dict.tsv'\n",
        "id2index = {}\n",
        "seqs = []\n",
        "index = 0\n",
        "sid1_index = 0\n",
        "sid2_index = 1\n",
        "ds_file = 'mouse_test.tsv'\n",
        "label_index = 2\n",
        "use_emb = 'ac5_aph.txt'\n",
        "\n",
        "if not os.path.isfile(ds_file) or not os.path.isfile(id2seq_file):\n",
        "  !wget https://raw.githubusercontent.com/anhvt00/MCAPS/master/data/Independent-testsets/cross-species/pairs/mouse_test.tsv\n",
        "  !wget https://raw.githubusercontent.com/anhvt00/MCAPS/master/data/Independent-testsets/cross-species/seqs/mouse_dict.tsv\n",
        "\n",
        "# Create line variable as a list of protein sequences with index is the number of protein sequences\n",
        "# id2index is a dictionary of protein id and incremental index number \n",
        "for line in open(id2seq_file):\n",
        "    line = line.strip().split('\\t')\n",
        "    id2index[line[0]] = index\n",
        "    seqs.append(line[1])\n",
        "    index += 1\n",
        "\n",
        "seq_array = []\n",
        "id2_aid = {}\n",
        "sid = 0\n",
        "\n",
        "seq2t = s2t(use_emb)\n",
        "\n",
        "max_data = -1\n",
        "limit_data = max_data > 0\n",
        "raw_data = []\n",
        "skip_head = False\n",
        "x = None\n",
        "count = 0\n",
        "\n",
        "# Create sequence array as a list of protein strings\n",
        "for line in tqdm(open(ds_file)):\n",
        "    if skip_head:\n",
        "        skip_head = False\n",
        "        continue\n",
        "    line = line.rstrip('\\n').rstrip('\\r').split('\\t')\n",
        "    if id2index.get(line[sid1_index]) is None or id2index.get(line[sid2_index]) is None:\n",
        "        continue\n",
        "    if id2_aid.get(line[sid1_index]) is None:\n",
        "        id2_aid[line[sid1_index]] = sid\n",
        "        sid += 1\n",
        "        seq_array.append(seqs[id2index[line[sid1_index]]])\n",
        "    line[sid1_index] = id2_aid[line[sid1_index]]\n",
        "    if id2_aid.get(line[sid2_index]) is None:\n",
        "        id2_aid[line[sid2_index]] = sid\n",
        "        sid += 1\n",
        "        seq_array.append(seqs[id2index[line[sid2_index]]])\n",
        "    line[sid2_index] = id2_aid[line[sid2_index]]\n",
        "    raw_data.append(line)\n",
        "    if limit_data:\n",
        "        count += 1\n",
        "        if count >= max_data:\n",
        "            break\n",
        "\n",
        "len_m_seq = np.array([len(line.split()) for line in seq_array])\n",
        "avg_m_seq = int(np.average(len_m_seq)) + 1\n",
        "max_m_seq = max(len_m_seq)\n",
        "dim = seq2t.dim\n",
        "\n",
        "# seq_tensor is tensor representation of dataset having shape of (number_of_sequences, padding_length, embedding_dim_of_aa)\n",
        "# Random for distribution of class labels\n",
        "np.random.seed(42)\n",
        "np.random.shuffle(raw_data)\n",
        "seq_tensor = np.array([encode_seq(line) for line in tqdm(seq_array)])\n",
        "\n",
        "# Extract index of 1st and 2nd sequences in pairs\n",
        "seq_index1 = np.array([line[sid1_index] for line in tqdm(raw_data)])\n",
        "seq_index2 = np.array([line[sid2_index] for line in tqdm(raw_data)])\n",
        "\n",
        "# Assign labels for pairs of sequences\n",
        "class_map = {'0':1,'1':0}\n",
        "class_labels = np.zeros((len(raw_data,)))\n",
        "for i in range(len(raw_data)):\n",
        "  class_labels[i] = float(raw_data[i][label_index])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "xetEYvGDtU8y",
        "outputId": "a79d672e-5ba1-4946-8e11-2972389b47ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================= INFER BY TRAINED NEURAL NETWORK ON TRAINING SET ==============================\n",
            "1719/1719 [==============================] - 3s 2ms/step\n",
            "============= INFERENCE BY NEURAL NETWORK ===============\n",
            "accuracy: 0.8713090909090909, precision: 0.346301775147929, recall: 0.4682, specificity: 0.91162, f1-score: 0.39812925170068025, mcc: 0.33256063154399146, auroc: 0.819519356, auprc: 0.3519240403914037 \n",
            "0.8713090909090909\t0.346301775147929\t0.4682\t0.91162\t0.39812925170068025\t0.33256063154399146\t0.819519356\t0.3519240403914037\n",
            "\n"
          ]
        }
      ],
      "source": [
        "##################################### Load Positive and Negative Dataset ##########################################################\n",
        "\n",
        "FEATURE_NUM = seq_tensor.shape[1]\n",
        "# Create data frame of pairs and labels\n",
        "df = np.hstack([seq_tensor[seq_index1], seq_tensor[seq_index2]])\n",
        "df = np.hstack([df, class_labels.reshape(-1,1)])\n",
        "df = pd.DataFrame(df)\n",
        "df = df.sample(frac=1)\n",
        "\n",
        "# Create pairs matrix and label vector\n",
        "X = df.iloc[:,0:FEATURE_NUM*2].values\n",
        "y = df.iloc[:,FEATURE_NUM*2:].values\n",
        "\n",
        "\n",
        "# standard scaler\n",
        "X = standard_scaler.transform(X)\n",
        "\n",
        "\n",
        "X1_train = X[:, :FEATURE_NUM]\n",
        "X2_train = X[:, FEATURE_NUM:]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"============================= INFER BY TRAINED NEURAL NETWORK ON TRAINING SET ==============================\")\n",
        "y_true = y\n",
        "y_pred = model.predict([X1_train, X2_train])\n",
        "cm1=confusion_matrix(y_true, np.round(y_pred))\n",
        "acc = (cm1[0,0]+cm1[1,1])/(cm1[0,0]+cm1[0,1]+cm1[1,0]+cm1[1,1])\n",
        "spec= (cm1[0,0])/(cm1[0,0]+cm1[0,1])\n",
        "sens = (cm1[1,1])/(cm1[1,0]+cm1[1,1])\n",
        "prec=cm1[1,1]/(cm1[1,1]+cm1[0,1])\n",
        "rec=cm1[1,1]/(cm1[1,1]+cm1[1,0])\n",
        "f1 = 2 * (prec * rec) / (prec + rec)\n",
        "mcc = matthews_corrcoef(y_true, np.round(y_pred))\n",
        "\n",
        "prc = metrics.average_precision_score(y_true, y_pred)\n",
        "\n",
        "print(\"============= INFERENCE BY NEURAL NETWORK ===============\")\n",
        "try:\n",
        "  auc = metrics.roc_auc_score(y_true, y_pred)\n",
        "  print(f'accuracy: {acc}, precision: {prec}, recall: {rec}, specificity: {spec}, f1-score: {f1}, mcc: {mcc}, auroc: {auc}, auprc: {prc} ')\n",
        "  print(str(acc) + \"\\t\" + str(prec) + \"\\t\" + str(rec) + \"\\t\" + str(spec) + \"\\t\" + str(f1) + \"\\t\" + str(mcc)+\"\\t\" + str(auc)  + \"\\t\" + str(prc) + \"\\n\")\n",
        "except ValueError:\n",
        "  print(f'accuracy: {acc}, precision: {prec}, recall: {rec}, specificity: {spec}, f1-score: {f1}, mcc: {mcc}, auroc: nan, auprc: {prc} ')\n",
        "  print(str(acc) + \"\\t\" + str(prec) + \"\\t\" + str(rec) + \"\\t\" + str(spec) + \"\\t\" + str(f1) + \"\\t\" + str(mcc)+\"\\t nan\"  + \"\\t\" + str(prc) + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlCnKukytU8z"
      },
      "source": [
        "#### Evaluate with trained FSNN-LGBM prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vvWuV7crtU8z",
        "outputId": "55b23760-815c-4469-bcee-71e8de3c2e83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1719/1719 [==============================] - 3s 2ms/step\n",
            "accuracy: 0.8395272727272727, precision: 0.2879627577033917, recall: 0.5196, specificity: 0.87152, f1-score: 0.3705605477107402, mcc: 0.3036362221199512, auroc: 0.69556, auprc: 0.1932981761754096 \n",
            "0.8395272727272727\t0.2879627577033917\t0.5196\t0.87152\t0.3705605477107402\t0.3036362221199512\t0.69556\t0.1932981761754096\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Read new data\n",
        "\n",
        "FEATURE_NUM = seq_tensor.shape[1]\n",
        "# Create data frame of pairs and labels\n",
        "df = np.hstack([seq_tensor[seq_index1], seq_tensor[seq_index2]])\n",
        "df = np.hstack([df, class_labels.reshape(-1,1)])\n",
        "df = pd.DataFrame(df)\n",
        "df = df.sample(frac=1)\n",
        "\n",
        "# Create pairs matrix and label vector\n",
        "X = df.iloc[:,0:FEATURE_NUM*2].values\n",
        "y = df.iloc[:,FEATURE_NUM*2:].values\n",
        "\n",
        "Trainlabels=y\n",
        "\n",
        "# standard scaler\n",
        "# scaler = StandardScaler().fit(X)\n",
        "# scaler = MinMaxScaler().fit(X)\n",
        "\n",
        "# scaler = RobustScaler().fit(X)\n",
        "X = standard_scaler.transform(X)\n",
        "\n",
        "\n",
        "X1_train = X[:, :FEATURE_NUM]\n",
        "X2_train = X[:, FEATURE_NUM:]\n",
        "\n",
        "\n",
        "# Predict representation from trained neural network\n",
        "\n",
        "intermediate_layer_model = Model(inputs=model.input,outputs=model.get_layer('Merged_feature_1').output)\n",
        "# Use intermediate layer to transform pairs matrix\n",
        "intermediate_output_p1 = intermediate_layer_model.predict([X1_train,X2_train])  \n",
        "p_merge=pd.DataFrame(intermediate_output_p1)    \n",
        "\n",
        "# create dataframe use transformed pairs matrix outputs and labels\n",
        "X_train_feat=pd.concat((p_merge,pd.DataFrame(pd.DataFrame(Trainlabels))),axis=1,ignore_index=True)\n",
        "\n",
        "# write to file dataframe of transformed pairs matrix and labels\n",
        "X_train_feat.to_csv('X_train.csv',header=False, index=False)\n",
        "\n",
        "# read dataframe of transformed pairs matrix and labels\n",
        "Train=pd.read_csv(\"X_train.csv\",header=None)\n",
        "Train=Train.sample(frac=1)\n",
        "\n",
        "X=Train.iloc[:,0:64].values\n",
        "y=Train.iloc[:,64:].values.ravel()\n",
        "\n",
        "extracted_df=X_train_feat\n",
        "\n",
        "X=robust_scaler.transform(X)\n",
        "\n",
        "# Predict probability from neural network output for new data\n",
        "y_true=y\n",
        "y_pred = model_.predict(X)\n",
        "cm1=confusion_matrix(y_true, np.round(y_pred))\n",
        "acc = (cm1[0,0]+cm1[1,1])/(cm1[0,0]+cm1[0,1]+cm1[1,0]+cm1[1,1])\n",
        "spec= (cm1[0,0])/(cm1[0,0]+cm1[0,1])\n",
        "sens = (cm1[1,1])/(cm1[1,0]+cm1[1,1])\n",
        "prec=cm1[1,1]/(cm1[1,1]+cm1[0,1])\n",
        "rec=cm1[1,1]/(cm1[1,1]+cm1[1,0])\n",
        "f1 = 2 * (prec * rec) / (prec + rec)\n",
        "mcc = matthews_corrcoef(y_true, np.round(y_pred))\n",
        "\n",
        "prc = metrics.average_precision_score(y_true, y_pred)\n",
        "\n",
        "try:\n",
        "  auc = metrics.roc_auc_score(y_true, y_pred)\n",
        "  print(f'accuracy: {acc}, precision: {prec}, recall: {rec}, specificity: {spec}, f1-score: {f1}, mcc: {mcc}, auroc: {auc}, auprc: {prc} ')\n",
        "  print(str(acc) + \"\\t\" + str(prec) + \"\\t\" + str(rec) + \"\\t\" + str(spec) + \"\\t\" + str(f1) + \"\\t\" + str(mcc)+\"\\t\" + str(auc)  + \"\\t\" + str(prc) + \"\\n\")\n",
        "except ValueError:\n",
        "  print(f'accuracy: {acc}, precision: {prec}, recall: {rec}, specificity: {spec}, f1-score: {f1}, mcc: {mcc}, auroc: nan, auprc: {prc} ')\n",
        "  print(str(acc) + \"\\t\" + str(prec) + \"\\t\" + str(rec) + \"\\t\" + str(spec) + \"\\t\" + str(f1) + \"\\t\" + str(mcc)+\"\\t nan\"  + \"\\t\" + str(prc) + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQziAWy7tU8z"
      },
      "source": [
        "### Worm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkgICX7_tU8z"
      },
      "source": [
        "#### Read dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "AQ46uHU6tU8z",
        "outputId": "e641fd02-cf54-4028-b8d2-f2e691e40de8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-12-17 17:13:22--  https://raw.githubusercontent.com/anhvt00/MCAPS/master/data/Independent-testsets/cross-species/pairs/worm_test.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1688849 (1.6M) [text/plain]\n",
            "Saving to: ‘worm_test.tsv’\n",
            "\n",
            "worm_test.tsv       100%[===================>]   1.61M  --.-KB/s    in 0.009s  \n",
            "\n",
            "2022-12-17 17:13:22 (187 MB/s) - ‘worm_test.tsv’ saved [1688849/1688849]\n",
            "\n",
            "--2022-12-17 17:13:22--  https://raw.githubusercontent.com/anhvt00/MCAPS/master/data/Independent-testsets/cross-species/seqs/worm_dict.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9519727 (9.1M) [text/plain]\n",
            "Saving to: ‘worm_dict.tsv’\n",
            "\n",
            "worm_dict.tsv       100%[===================>]   9.08M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2022-12-17 17:13:23 (346 MB/s) - ‘worm_dict.tsv’ saved [9519727/9519727]\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "55000it [00:00, 328470.38it/s]\n",
            "100%|██████████| 25429/25429 [00:54<00:00, 468.26it/s]\n",
            "100%|██████████| 55000/55000 [00:00<00:00, 1707765.86it/s]\n",
            "100%|██████████| 55000/55000 [00:00<00:00, 1904314.22it/s]\n"
          ]
        }
      ],
      "source": [
        "id2seq_file = 'worm_dict.tsv'\n",
        "id2index = {}\n",
        "seqs = []\n",
        "index = 0\n",
        "sid1_index = 0\n",
        "sid2_index = 1\n",
        "ds_file = 'worm_test.tsv'\n",
        "label_index = 2\n",
        "use_emb = 'ac5_aph.txt'\n",
        "\n",
        "if not os.path.isfile(ds_file) or not os.path.isfile(id2seq_file):\n",
        "  !wget https://raw.githubusercontent.com/anhvt00/MCAPS/master/data/Independent-testsets/cross-species/pairs/worm_test.tsv\n",
        "  !wget https://raw.githubusercontent.com/anhvt00/MCAPS/master/data/Independent-testsets/cross-species/seqs/worm_dict.tsv\n",
        "\n",
        "\n",
        "# Create line variable as a list of protein sequences with index is the number of protein sequences\n",
        "# id2index is a dictionary of protein id and incremental index number \n",
        "for line in open(id2seq_file):\n",
        "    line = line.strip().split('\\t')\n",
        "    id2index[line[0]] = index\n",
        "    seqs.append(line[1])\n",
        "    index += 1\n",
        "\n",
        "seq_array = []\n",
        "id2_aid = {}\n",
        "sid = 0\n",
        "\n",
        "seq2t = s2t(use_emb)\n",
        "\n",
        "max_data = -1\n",
        "limit_data = max_data > 0\n",
        "raw_data = []\n",
        "skip_head = False\n",
        "x = None\n",
        "count = 0\n",
        "\n",
        "# Create sequence array as a list of protein strings\n",
        "for line in tqdm(open(ds_file)):\n",
        "    if skip_head:\n",
        "        skip_head = False\n",
        "        continue\n",
        "    line = line.rstrip('\\n').rstrip('\\r').split('\\t')\n",
        "    if id2index.get(line[sid1_index]) is None or id2index.get(line[sid2_index]) is None:\n",
        "        continue\n",
        "    if id2_aid.get(line[sid1_index]) is None:\n",
        "        id2_aid[line[sid1_index]] = sid\n",
        "        sid += 1\n",
        "        seq_array.append(seqs[id2index[line[sid1_index]]])\n",
        "    line[sid1_index] = id2_aid[line[sid1_index]]\n",
        "    if id2_aid.get(line[sid2_index]) is None:\n",
        "        id2_aid[line[sid2_index]] = sid\n",
        "        sid += 1\n",
        "        seq_array.append(seqs[id2index[line[sid2_index]]])\n",
        "    line[sid2_index] = id2_aid[line[sid2_index]]\n",
        "    raw_data.append(line)\n",
        "    if limit_data:\n",
        "        count += 1\n",
        "        if count >= max_data:\n",
        "            break\n",
        "\n",
        "len_m_seq = np.array([len(line.split()) for line in seq_array])\n",
        "avg_m_seq = int(np.average(len_m_seq)) + 1\n",
        "max_m_seq = max(len_m_seq)\n",
        "dim = seq2t.dim\n",
        "\n",
        "# seq_tensor is tensor representation of dataset having shape of (number_of_sequences, padding_length, embedding_dim_of_aa)\n",
        "# Random for distribution of class labels\n",
        "np.random.seed(42)\n",
        "np.random.shuffle(raw_data)\n",
        "seq_tensor = np.array([encode_seq(line) for line in tqdm(seq_array)])\n",
        "\n",
        "# Extract index of 1st and 2nd sequences in pairs\n",
        "seq_index1 = np.array([line[sid1_index] for line in tqdm(raw_data)])\n",
        "seq_index2 = np.array([line[sid2_index] for line in tqdm(raw_data)])\n",
        "\n",
        "# Assign labels for pairs of sequences\n",
        "class_map = {'0':1,'1':0}\n",
        "class_labels = np.zeros((len(raw_data,)))\n",
        "for i in range(len(raw_data)):\n",
        "  class_labels[i] = float(raw_data[i][label_index])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "i6ZYl_cMtU8z",
        "outputId": "f613056a-331f-4057-fb2d-32755ed4cbd6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================= INFER BY TRAINED NEURAL NETWORK ON TRAINING SET ==============================\n",
            "1719/1719 [==============================] - 3s 2ms/step\n",
            "============= INFERENCE BY NEURAL NETWORK ===============\n",
            "accuracy: 0.8801272727272728, precision: 0.3017176997759522, recall: 0.2424, specificity: 0.9439, f1-score: 0.26882555173561046, mcc: 0.20583501234419635, auroc: 0.734948372, auprc: 0.21792906924331168 \n",
            "0.8801272727272728\t0.3017176997759522\t0.2424\t0.9439\t0.26882555173561046\t0.20583501234419635\t0.734948372\t0.21792906924331168\n",
            "\n"
          ]
        }
      ],
      "source": [
        "##################################### Load Positive and Negative Dataset ##########################################################\n",
        "\n",
        "FEATURE_NUM = seq_tensor.shape[1]\n",
        "# Create data frame of pairs and labels\n",
        "df = np.hstack([seq_tensor[seq_index1], seq_tensor[seq_index2]])\n",
        "df = np.hstack([df, class_labels.reshape(-1,1)])\n",
        "df = pd.DataFrame(df)\n",
        "df = df.sample(frac=1)\n",
        "\n",
        "# Create pairs matrix and label vector\n",
        "X = df.iloc[:,0:FEATURE_NUM*2].values\n",
        "y = df.iloc[:,FEATURE_NUM*2:].values\n",
        "\n",
        "\n",
        "# standard scaler\n",
        "X = standard_scaler.transform(X)\n",
        "\n",
        "\n",
        "X1_train = X[:, :FEATURE_NUM]\n",
        "X2_train = X[:, FEATURE_NUM:]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"============================= INFER BY TRAINED NEURAL NETWORK ON TRAINING SET ==============================\")\n",
        "y_true = y\n",
        "y_pred = model.predict([X1_train, X2_train])\n",
        "cm1=confusion_matrix(y_true, np.round(y_pred))\n",
        "acc = (cm1[0,0]+cm1[1,1])/(cm1[0,0]+cm1[0,1]+cm1[1,0]+cm1[1,1])\n",
        "spec= (cm1[0,0])/(cm1[0,0]+cm1[0,1])\n",
        "sens = (cm1[1,1])/(cm1[1,0]+cm1[1,1])\n",
        "prec=cm1[1,1]/(cm1[1,1]+cm1[0,1])\n",
        "rec=cm1[1,1]/(cm1[1,1]+cm1[1,0])\n",
        "f1 = 2 * (prec * rec) / (prec + rec)\n",
        "mcc = matthews_corrcoef(y_true, np.round(y_pred))\n",
        "\n",
        "prc = metrics.average_precision_score(y_true, y_pred)\n",
        "\n",
        "print(\"============= INFERENCE BY NEURAL NETWORK ===============\")\n",
        "try:\n",
        "  auc = metrics.roc_auc_score(y_true, y_pred)\n",
        "  print(f'accuracy: {acc}, precision: {prec}, recall: {rec}, specificity: {spec}, f1-score: {f1}, mcc: {mcc}, auroc: {auc}, auprc: {prc} ')\n",
        "  print(str(acc) + \"\\t\" + str(prec) + \"\\t\" + str(rec) + \"\\t\" + str(spec) + \"\\t\" + str(f1) + \"\\t\" + str(mcc)+\"\\t\" + str(auc)  + \"\\t\" + str(prc) + \"\\n\")\n",
        "except ValueError:\n",
        "  print(f'accuracy: {acc}, precision: {prec}, recall: {rec}, specificity: {spec}, f1-score: {f1}, mcc: {mcc}, auroc: nan, auprc: {prc} ')\n",
        "  print(str(acc) + \"\\t\" + str(prec) + \"\\t\" + str(rec) + \"\\t\" + str(spec) + \"\\t\" + str(f1) + \"\\t\" + str(mcc)+\"\\t nan\"  + \"\\t\" + str(prc) + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOOrKr-ctU8z"
      },
      "source": [
        "#### Evaluate with trained FSNN-LGBM prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Vbm_c2TItU8z",
        "outputId": "8bf56011-6a45-495a-9562-ef7e3c53d87e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1719/1719 [==============================] - 3s 2ms/step\n",
            "accuracy: 0.8452, precision: 0.2292758089368259, recall: 0.2976, specificity: 0.89996, f1-score: 0.2590078328981723, mcc: 0.17604797587956456, auroc: 0.59878, auprc: 0.13208702619414484 \n",
            "0.8452\t0.2292758089368259\t0.2976\t0.89996\t0.2590078328981723\t0.17604797587956456\t0.59878\t0.13208702619414484\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Read new data\n",
        "\n",
        "FEATURE_NUM = seq_tensor.shape[1]\n",
        "# Create data frame of pairs and labels\n",
        "df = np.hstack([seq_tensor[seq_index1], seq_tensor[seq_index2]])\n",
        "df = np.hstack([df, class_labels.reshape(-1,1)])\n",
        "df = pd.DataFrame(df)\n",
        "df = df.sample(frac=1)\n",
        "\n",
        "# Create pairs matrix and label vector\n",
        "X = df.iloc[:,0:FEATURE_NUM*2].values\n",
        "y = df.iloc[:,FEATURE_NUM*2:].values\n",
        "\n",
        "Trainlabels=y\n",
        "\n",
        "# standard scaler\n",
        "# scaler = StandardScaler().fit(X)\n",
        "# scaler = MinMaxScaler().fit(X)\n",
        "\n",
        "# scaler = RobustScaler().fit(X)\n",
        "X = standard_scaler.transform(X)\n",
        "\n",
        "\n",
        "X1_train = X[:, :FEATURE_NUM]\n",
        "X2_train = X[:, FEATURE_NUM:]\n",
        "\n",
        "\n",
        "# Predict representation from trained neural network\n",
        "\n",
        "intermediate_layer_model = Model(inputs=model.input,outputs=model.get_layer('Merged_feature_1').output)\n",
        "# Use intermediate layer to transform pairs matrix\n",
        "intermediate_output_p1 = intermediate_layer_model.predict([X1_train,X2_train])  \n",
        "p_merge=pd.DataFrame(intermediate_output_p1)    \n",
        "\n",
        "# create dataframe use transformed pairs matrix outputs and labels\n",
        "X_train_feat=pd.concat((p_merge,pd.DataFrame(pd.DataFrame(Trainlabels))),axis=1,ignore_index=True)\n",
        "\n",
        "# write to file dataframe of transformed pairs matrix and labels\n",
        "X_train_feat.to_csv('X_train.csv',header=False, index=False)\n",
        "\n",
        "# read dataframe of transformed pairs matrix and labels\n",
        "Train=pd.read_csv(\"X_train.csv\",header=None)\n",
        "Train=Train.sample(frac=1)\n",
        "\n",
        "X=Train.iloc[:,0:64].values\n",
        "y=Train.iloc[:,64:].values.ravel()\n",
        "\n",
        "extracted_df=X_train_feat\n",
        "\n",
        "X=robust_scaler.transform(X)\n",
        "\n",
        "# Predict probability from neural network output for new data\n",
        "y_true=y\n",
        "y_pred = model_.predict(X)\n",
        "cm1=confusion_matrix(y_true, np.round(y_pred))\n",
        "acc = (cm1[0,0]+cm1[1,1])/(cm1[0,0]+cm1[0,1]+cm1[1,0]+cm1[1,1])\n",
        "spec= (cm1[0,0])/(cm1[0,0]+cm1[0,1])\n",
        "sens = (cm1[1,1])/(cm1[1,0]+cm1[1,1])\n",
        "prec=cm1[1,1]/(cm1[1,1]+cm1[0,1])\n",
        "rec=cm1[1,1]/(cm1[1,1]+cm1[1,0])\n",
        "f1 = 2 * (prec * rec) / (prec + rec)\n",
        "mcc = matthews_corrcoef(y_true, np.round(y_pred))\n",
        "\n",
        "prc = metrics.average_precision_score(y_true, y_pred)\n",
        "\n",
        "try:\n",
        "  auc = metrics.roc_auc_score(y_true, y_pred)\n",
        "  print(f'accuracy: {acc}, precision: {prec}, recall: {rec}, specificity: {spec}, f1-score: {f1}, mcc: {mcc}, auroc: {auc}, auprc: {prc} ')\n",
        "  print(str(acc) + \"\\t\" + str(prec) + \"\\t\" + str(rec) + \"\\t\" + str(spec) + \"\\t\" + str(f1) + \"\\t\" + str(mcc)+\"\\t\" + str(auc)  + \"\\t\" + str(prc) + \"\\n\")\n",
        "except ValueError:\n",
        "  print(f'accuracy: {acc}, precision: {prec}, recall: {rec}, specificity: {spec}, f1-score: {f1}, mcc: {mcc}, auroc: nan, auprc: {prc} ')\n",
        "  print(str(acc) + \"\\t\" + str(prec) + \"\\t\" + str(rec) + \"\\t\" + str(spec) + \"\\t\" + str(f1) + \"\\t\" + str(mcc)+\"\\t nan\"  + \"\\t\" + str(prc) + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkbvwU7rtlxB"
      },
      "source": [
        "### Yeast"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdkKtp43tlxC"
      },
      "source": [
        "#### Read dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4vgSFY_stlxC",
        "outputId": "58998156-c000-4594-8c05-7213eaddf263"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-12-17 17:14:30--  https://raw.githubusercontent.com/anhvt00/MCAPS/master/data/Independent-testsets/cross-species/pairs/yeast_test.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1556676 (1.5M) [text/plain]\n",
            "Saving to: ‘yeast_test.tsv’\n",
            "\n",
            "yeast_test.tsv      100%[===================>]   1.48M  --.-KB/s    in 0.006s  \n",
            "\n",
            "2022-12-17 17:14:31 (253 MB/s) - ‘yeast_test.tsv’ saved [1556676/1556676]\n",
            "\n",
            "--2022-12-17 17:14:31--  https://raw.githubusercontent.com/anhvt00/MCAPS/master/data/Independent-testsets/cross-species/seqs/yeast_dict.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2014626 (1.9M) [text/plain]\n",
            "Saving to: ‘yeast_dict.tsv’\n",
            "\n",
            "yeast_dict.tsv      100%[===================>]   1.92M  --.-KB/s    in 0.006s  \n",
            "\n",
            "2022-12-17 17:14:31 (296 MB/s) - ‘yeast_dict.tsv’ saved [2014626/2014626]\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "55000it [00:00, 169654.40it/s]\n",
            "100%|██████████| 5664/5664 [00:11<00:00, 486.35it/s]\n",
            "100%|██████████| 55000/55000 [00:00<00:00, 1741912.67it/s]\n",
            "100%|██████████| 55000/55000 [00:00<00:00, 2076743.28it/s]\n"
          ]
        }
      ],
      "source": [
        "id2seq_file = 'yeast_dict.tsv'\n",
        "id2index = {}\n",
        "seqs = []\n",
        "index = 0\n",
        "sid1_index = 0\n",
        "sid2_index = 1\n",
        "ds_file = 'yeast_test.tsv'\n",
        "label_index = 2\n",
        "use_emb = 'ac5_aph.txt'\n",
        "\n",
        "if not os.path.isfile(ds_file) or not os.path.isfile(id2seq_file):\n",
        "  !wget https://raw.githubusercontent.com/anhvt00/MCAPS/master/data/Independent-testsets/cross-species/pairs/yeast_test.tsv\n",
        "  !wget https://raw.githubusercontent.com/anhvt00/MCAPS/master/data/Independent-testsets/cross-species/seqs/yeast_dict.tsv\n",
        "\n",
        "\n",
        "# Create line variable as a list of protein sequences with index is the number of protein sequences\n",
        "# id2index is a dictionary of protein id and incremental index number \n",
        "for line in open(id2seq_file):\n",
        "    line = line.strip().split('\\t')\n",
        "    id2index[line[0]] = index\n",
        "    seqs.append(line[1])\n",
        "    index += 1\n",
        "\n",
        "seq_array = []\n",
        "id2_aid = {}\n",
        "sid = 0\n",
        "\n",
        "seq2t = s2t(use_emb)\n",
        "\n",
        "max_data = -1\n",
        "limit_data = max_data > 0\n",
        "raw_data = []\n",
        "skip_head = False\n",
        "x = None\n",
        "count = 0\n",
        "\n",
        "# Create sequence array as a list of protein strings\n",
        "for line in tqdm(open(ds_file)):\n",
        "    if skip_head:\n",
        "        skip_head = False\n",
        "        continue\n",
        "    line = line.rstrip('\\n').rstrip('\\r').split('\\t')\n",
        "    if id2index.get(line[sid1_index]) is None or id2index.get(line[sid2_index]) is None:\n",
        "        continue\n",
        "    if id2_aid.get(line[sid1_index]) is None:\n",
        "        id2_aid[line[sid1_index]] = sid\n",
        "        sid += 1\n",
        "        seq_array.append(seqs[id2index[line[sid1_index]]])\n",
        "    line[sid1_index] = id2_aid[line[sid1_index]]\n",
        "    if id2_aid.get(line[sid2_index]) is None:\n",
        "        id2_aid[line[sid2_index]] = sid\n",
        "        sid += 1\n",
        "        seq_array.append(seqs[id2index[line[sid2_index]]])\n",
        "    line[sid2_index] = id2_aid[line[sid2_index]]\n",
        "    raw_data.append(line)\n",
        "    if limit_data:\n",
        "        count += 1\n",
        "        if count >= max_data:\n",
        "            break\n",
        "\n",
        "len_m_seq = np.array([len(line.split()) for line in seq_array])\n",
        "avg_m_seq = int(np.average(len_m_seq)) + 1\n",
        "max_m_seq = max(len_m_seq)\n",
        "dim = seq2t.dim\n",
        "\n",
        "# seq_tensor is tensor representation of dataset having shape of (number_of_sequences, padding_length, embedding_dim_of_aa)\n",
        "# Random for distribution of class labels\n",
        "np.random.seed(42)\n",
        "np.random.shuffle(raw_data)\n",
        "seq_tensor = np.array([encode_seq(line) for line in tqdm(seq_array)])\n",
        "\n",
        "# Extract index of 1st and 2nd sequences in pairs\n",
        "seq_index1 = np.array([line[sid1_index] for line in tqdm(raw_data)])\n",
        "seq_index2 = np.array([line[sid2_index] for line in tqdm(raw_data)])\n",
        "\n",
        "# Assign labels for pairs of sequences\n",
        "class_map = {'0':1,'1':0}\n",
        "class_labels = np.zeros((len(raw_data,)))\n",
        "for i in range(len(raw_data)):\n",
        "  class_labels[i] = float(raw_data[i][label_index])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "GRf4al_RtlxD",
        "outputId": "6119c1e6-cc15-4cde-9af0-bdbabeba926e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================= INFER BY TRAINED NEURAL NETWORK ON TRAINING SET ==============================\n",
            "1719/1719 [==============================] - 3s 2ms/step\n",
            "============= INFERENCE BY NEURAL NETWORK ===============\n",
            "accuracy: 0.8780363636363636, precision: 0.25641756988020536, recall: 0.1798, specificity: 0.94786, f1-score: 0.21138020221020457, mcc: 0.15022443324352652, auroc: 0.699082788, auprc: 0.18664255060274748 \n",
            "0.8780363636363636\t0.25641756988020536\t0.1798\t0.94786\t0.21138020221020457\t0.15022443324352652\t0.699082788\t0.18664255060274748\n",
            "\n"
          ]
        }
      ],
      "source": [
        "##################################### Load Positive and Negative Dataset ##########################################################\n",
        "\n",
        "FEATURE_NUM = seq_tensor.shape[1]\n",
        "# Create data frame of pairs and labels\n",
        "df = np.hstack([seq_tensor[seq_index1], seq_tensor[seq_index2]])\n",
        "df = np.hstack([df, class_labels.reshape(-1,1)])\n",
        "df = pd.DataFrame(df)\n",
        "df = df.sample(frac=1)\n",
        "\n",
        "# Create pairs matrix and label vector\n",
        "X = df.iloc[:,0:FEATURE_NUM*2].values\n",
        "y = df.iloc[:,FEATURE_NUM*2:].values\n",
        "\n",
        "\n",
        "# standard scaler\n",
        "X = standard_scaler.transform(X)\n",
        "\n",
        "\n",
        "X1_train = X[:, :FEATURE_NUM]\n",
        "X2_train = X[:, FEATURE_NUM:]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"============================= INFER BY TRAINED NEURAL NETWORK ON TRAINING SET ==============================\")\n",
        "y_true = y\n",
        "y_pred = model.predict([X1_train, X2_train])\n",
        "cm1=confusion_matrix(y_true, np.round(y_pred))\n",
        "acc = (cm1[0,0]+cm1[1,1])/(cm1[0,0]+cm1[0,1]+cm1[1,0]+cm1[1,1])\n",
        "spec= (cm1[0,0])/(cm1[0,0]+cm1[0,1])\n",
        "sens = (cm1[1,1])/(cm1[1,0]+cm1[1,1])\n",
        "prec=cm1[1,1]/(cm1[1,1]+cm1[0,1])\n",
        "rec=cm1[1,1]/(cm1[1,1]+cm1[1,0])\n",
        "f1 = 2 * (prec * rec) / (prec + rec)\n",
        "mcc = matthews_corrcoef(y_true, np.round(y_pred))\n",
        "\n",
        "prc = metrics.average_precision_score(y_true, y_pred)\n",
        "\n",
        "print(\"============= INFERENCE BY NEURAL NETWORK ===============\")\n",
        "try:\n",
        "  auc = metrics.roc_auc_score(y_true, y_pred)\n",
        "  print(f'accuracy: {acc}, precision: {prec}, recall: {rec}, specificity: {spec}, f1-score: {f1}, mcc: {mcc}, auroc: {auc}, auprc: {prc} ')\n",
        "  print(str(acc) + \"\\t\" + str(prec) + \"\\t\" + str(rec) + \"\\t\" + str(spec) + \"\\t\" + str(f1) + \"\\t\" + str(mcc)+\"\\t\" + str(auc)  + \"\\t\" + str(prc) + \"\\n\")\n",
        "except ValueError:\n",
        "  print(f'accuracy: {acc}, precision: {prec}, recall: {rec}, specificity: {spec}, f1-score: {f1}, mcc: {mcc}, auroc: nan, auprc: {prc} ')\n",
        "  print(str(acc) + \"\\t\" + str(prec) + \"\\t\" + str(rec) + \"\\t\" + str(spec) + \"\\t\" + str(f1) + \"\\t\" + str(mcc)+\"\\t nan\"  + \"\\t\" + str(prc) + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scCEQx7HtlxD"
      },
      "source": [
        "#### Evaluate with trained FSNN-LGBM prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_EFbL2T2tlxD",
        "outputId": "62ed2253-c88c-4582-b726-f6d570bebc9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1719/1719 [==============================] - 3s 2ms/step\n",
            "accuracy: 0.8470727272727273, precision: 0.20457301229863156, recall: 0.2362, specificity: 0.90816, f1-score: 0.21925183328692102, mcc: 0.1353986749086399, auroc: 0.5721799999999999, auprc: 0.11775650914130043 \n",
            "0.8470727272727273\t0.20457301229863156\t0.2362\t0.90816\t0.21925183328692102\t0.1353986749086399\t0.5721799999999999\t0.11775650914130043\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Read new data\n",
        "\n",
        "FEATURE_NUM = seq_tensor.shape[1]\n",
        "# Create data frame of pairs and labels\n",
        "df = np.hstack([seq_tensor[seq_index1], seq_tensor[seq_index2]])\n",
        "df = np.hstack([df, class_labels.reshape(-1,1)])\n",
        "df = pd.DataFrame(df)\n",
        "df = df.sample(frac=1)\n",
        "\n",
        "# Create pairs matrix and label vector\n",
        "X = df.iloc[:,0:FEATURE_NUM*2].values\n",
        "y = df.iloc[:,FEATURE_NUM*2:].values\n",
        "\n",
        "Trainlabels=y\n",
        "\n",
        "# standard scaler\n",
        "# scaler = StandardScaler().fit(X)\n",
        "# scaler = MinMaxScaler().fit(X)\n",
        "\n",
        "# scaler = RobustScaler().fit(X)\n",
        "X = standard_scaler.transform(X)\n",
        "\n",
        "\n",
        "X1_train = X[:, :FEATURE_NUM]\n",
        "X2_train = X[:, FEATURE_NUM:]\n",
        "\n",
        "\n",
        "# Predict representation from trained neural network\n",
        "\n",
        "intermediate_layer_model = Model(inputs=model.input,outputs=model.get_layer('Merged_feature_1').output)\n",
        "# Use intermediate layer to transform pairs matrix\n",
        "intermediate_output_p1 = intermediate_layer_model.predict([X1_train,X2_train])  \n",
        "p_merge=pd.DataFrame(intermediate_output_p1)    \n",
        "\n",
        "# create dataframe use transformed pairs matrix outputs and labels\n",
        "X_train_feat=pd.concat((p_merge,pd.DataFrame(pd.DataFrame(Trainlabels))),axis=1,ignore_index=True)\n",
        "\n",
        "# write to file dataframe of transformed pairs matrix and labels\n",
        "X_train_feat.to_csv('X_train.csv',header=False, index=False)\n",
        "\n",
        "# read dataframe of transformed pairs matrix and labels\n",
        "Train=pd.read_csv(\"X_train.csv\",header=None)\n",
        "Train=Train.sample(frac=1)\n",
        "\n",
        "X=Train.iloc[:,0:64].values\n",
        "y=Train.iloc[:,64:].values.ravel()\n",
        "\n",
        "extracted_df=X_train_feat\n",
        "\n",
        "X=robust_scaler.transform(X)\n",
        "\n",
        "# Predict probability from neural network output for new data\n",
        "y_true=y\n",
        "y_pred = model_.predict(X)\n",
        "cm1=confusion_matrix(y_true, np.round(y_pred))\n",
        "acc = (cm1[0,0]+cm1[1,1])/(cm1[0,0]+cm1[0,1]+cm1[1,0]+cm1[1,1])\n",
        "spec= (cm1[0,0])/(cm1[0,0]+cm1[0,1])\n",
        "sens = (cm1[1,1])/(cm1[1,0]+cm1[1,1])\n",
        "prec=cm1[1,1]/(cm1[1,1]+cm1[0,1])\n",
        "rec=cm1[1,1]/(cm1[1,1]+cm1[1,0])\n",
        "f1 = 2 * (prec * rec) / (prec + rec)\n",
        "mcc = matthews_corrcoef(y_true, np.round(y_pred))\n",
        "\n",
        "prc = metrics.average_precision_score(y_true, y_pred)\n",
        "\n",
        "try:\n",
        "  auc = metrics.roc_auc_score(y_true, y_pred)\n",
        "  print(f'accuracy: {acc}, precision: {prec}, recall: {rec}, specificity: {spec}, f1-score: {f1}, mcc: {mcc}, auroc: {auc}, auprc: {prc} ')\n",
        "  print(str(acc) + \"\\t\" + str(prec) + \"\\t\" + str(rec) + \"\\t\" + str(spec) + \"\\t\" + str(f1) + \"\\t\" + str(mcc)+\"\\t\" + str(auc)  + \"\\t\" + str(prc) + \"\\n\")\n",
        "except ValueError:\n",
        "  print(f'accuracy: {acc}, precision: {prec}, recall: {rec}, specificity: {spec}, f1-score: {f1}, mcc: {mcc}, auroc: nan, auprc: {prc} ')\n",
        "  print(str(acc) + \"\\t\" + str(prec) + \"\\t\" + str(rec) + \"\\t\" + str(spec) + \"\\t\" + str(f1) + \"\\t\" + str(mcc)+\"\\t nan\"  + \"\\t\" + str(prc) + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsgQ5MKHtmHE"
      },
      "source": [
        "### Guo-2008"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyKFGvKTtmHG"
      },
      "source": [
        "#### Read dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "--zu3uFrtmHH",
        "outputId": "5c5175f0-462a-4f7a-a792-17d1da419f0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-12-17 17:14:56--  https://raw.githubusercontent.com/anhvt00/MCAPS/master/data/Golden-standard-datasets/Guo-2008/guo_pairs.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 179008 (175K) [text/plain]\n",
            "Saving to: ‘guo_pairs.tsv’\n",
            "\n",
            "guo_pairs.tsv       100%[===================>] 174.81K  --.-KB/s    in 0.003s  \n",
            "\n",
            "2022-12-17 17:14:56 (52.1 MB/s) - ‘guo_pairs.tsv’ saved [179008/179008]\n",
            "\n",
            "--2022-12-17 17:14:56--  https://raw.githubusercontent.com/anhvt00/MCAPS/master/data/Golden-standard-datasets/Guo-2008/guo_dict.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1415079 (1.3M) [text/plain]\n",
            "Saving to: ‘guo_dict.tsv’\n",
            "\n",
            "guo_dict.tsv        100%[===================>]   1.35M  --.-KB/s    in 0.007s  \n",
            "\n",
            "2022-12-17 17:14:58 (196 MB/s) - ‘guo_dict.tsv’ saved [1415079/1415079]\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "11188it [00:00, 288355.82it/s]\n",
            "100%|██████████| 2497/2497 [00:08<00:00, 306.32it/s]\n",
            "100%|██████████| 11188/11188 [00:00<00:00, 1722998.83it/s]\n",
            "100%|██████████| 11188/11188 [00:00<00:00, 2319962.09it/s]\n"
          ]
        }
      ],
      "source": [
        "id2seq_file = 'guo_dict.tsv'\n",
        "id2index = {}\n",
        "seqs = []\n",
        "index = 0\n",
        "sid1_index = 0\n",
        "sid2_index = 1\n",
        "ds_file = 'guo_pairs.tsv'\n",
        "label_index = 2\n",
        "use_emb = 'ac5_aph.txt'\n",
        "\n",
        "if not os.path.isfile(ds_file) or not os.path.isfile(id2seq_file):\n",
        "  !wget https://raw.githubusercontent.com/anhvt00/MCAPS/master/data/Golden-standard-datasets/Guo-2008/guo_pairs.tsv\n",
        "  !wget https://raw.githubusercontent.com/anhvt00/MCAPS/master/data/Golden-standard-datasets/Guo-2008/guo_dict.tsv\n",
        "\n",
        "\n",
        "# Create line variable as a list of protein sequences with index is the number of protein sequences\n",
        "# id2index is a dictionary of protein id and incremental index number \n",
        "for line in open(id2seq_file):\n",
        "    line = line.strip().split('\\t')\n",
        "    id2index[line[0]] = index\n",
        "    seqs.append(line[1])\n",
        "    index += 1\n",
        "\n",
        "seq_array = []\n",
        "id2_aid = {}\n",
        "sid = 0\n",
        "\n",
        "seq2t = s2t(use_emb)\n",
        "\n",
        "max_data = -1\n",
        "limit_data = max_data > 0\n",
        "raw_data = []\n",
        "skip_head = False\n",
        "x = None\n",
        "count = 0\n",
        "\n",
        "# Create sequence array as a list of protein strings\n",
        "for line in tqdm(open(ds_file)):\n",
        "    if skip_head:\n",
        "        skip_head = False\n",
        "        continue\n",
        "    line = line.rstrip('\\n').rstrip('\\r').split('\\t')\n",
        "    if id2index.get(line[sid1_index]) is None or id2index.get(line[sid2_index]) is None:\n",
        "        continue\n",
        "    if id2_aid.get(line[sid1_index]) is None:\n",
        "        id2_aid[line[sid1_index]] = sid\n",
        "        sid += 1\n",
        "        seq_array.append(seqs[id2index[line[sid1_index]]])\n",
        "    line[sid1_index] = id2_aid[line[sid1_index]]\n",
        "    if id2_aid.get(line[sid2_index]) is None:\n",
        "        id2_aid[line[sid2_index]] = sid\n",
        "        sid += 1\n",
        "        seq_array.append(seqs[id2index[line[sid2_index]]])\n",
        "    line[sid2_index] = id2_aid[line[sid2_index]]\n",
        "    raw_data.append(line)\n",
        "    if limit_data:\n",
        "        count += 1\n",
        "        if count >= max_data:\n",
        "            break\n",
        "\n",
        "len_m_seq = np.array([len(line.split()) for line in seq_array])\n",
        "avg_m_seq = int(np.average(len_m_seq)) + 1\n",
        "max_m_seq = max(len_m_seq)\n",
        "dim = seq2t.dim\n",
        "\n",
        "# seq_tensor is tensor representation of dataset having shape of (number_of_sequences, padding_length, embedding_dim_of_aa)\n",
        "# Random for distribution of class labels\n",
        "np.random.seed(42)\n",
        "np.random.shuffle(raw_data)\n",
        "seq_tensor = np.array([encode_seq(line) for line in tqdm(seq_array)])\n",
        "\n",
        "# Extract index of 1st and 2nd sequences in pairs\n",
        "seq_index1 = np.array([line[sid1_index] for line in tqdm(raw_data)])\n",
        "seq_index2 = np.array([line[sid2_index] for line in tqdm(raw_data)])\n",
        "\n",
        "# Assign labels for pairs of sequences\n",
        "class_map = {'0':1,'1':0}\n",
        "class_labels = np.zeros((len(raw_data,)))\n",
        "for i in range(len(raw_data)):\n",
        "  class_labels[i] = float(raw_data[i][label_index])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "TAnIXfiYtmHI",
        "outputId": "b5ec218a-6f69-4588-dd7a-4ad37309464f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================= INFER BY TRAINED NEURAL NETWORK ON TRAINING SET ==============================\n",
            "350/350 [==============================] - 1s 2ms/step\n",
            "============= INFERENCE BY NEURAL NETWORK ===============\n",
            "accuracy: 0.5076868072935288, precision: 0.5458422174840085, recall: 0.09152663568108688, specificity: 0.9238469789059707, f1-score: 0.15676668707899571, mcc: 0.0277354425138481, auroc: 0.5420829387275733, auprc: 0.5323118008797516 \n",
            "0.5076868072935288\t0.5458422174840085\t0.09152663568108688\t0.9238469789059707\t0.15676668707899571\t0.0277354425138481\t0.5420829387275733\t0.5323118008797516\n",
            "\n"
          ]
        }
      ],
      "source": [
        "##################################### Load Positive and Negative Dataset ##########################################################\n",
        "\n",
        "FEATURE_NUM = seq_tensor.shape[1]\n",
        "# Create data frame of pairs and labels\n",
        "df = np.hstack([seq_tensor[seq_index1], seq_tensor[seq_index2]])\n",
        "df = np.hstack([df, class_labels.reshape(-1,1)])\n",
        "df = pd.DataFrame(df)\n",
        "df = df.sample(frac=1)\n",
        "\n",
        "# Create pairs matrix and label vector\n",
        "X = df.iloc[:,0:FEATURE_NUM*2].values\n",
        "y = df.iloc[:,FEATURE_NUM*2:].values\n",
        "\n",
        "\n",
        "# standard scaler\n",
        "X = standard_scaler.transform(X)\n",
        "\n",
        "\n",
        "X1_train = X[:, :FEATURE_NUM]\n",
        "X2_train = X[:, FEATURE_NUM:]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"============================= INFER BY TRAINED NEURAL NETWORK ON TRAINING SET ==============================\")\n",
        "y_true = y\n",
        "y_pred = model.predict([X1_train, X2_train])\n",
        "cm1=confusion_matrix(y_true, np.round(y_pred))\n",
        "acc = (cm1[0,0]+cm1[1,1])/(cm1[0,0]+cm1[0,1]+cm1[1,0]+cm1[1,1])\n",
        "spec= (cm1[0,0])/(cm1[0,0]+cm1[0,1])\n",
        "sens = (cm1[1,1])/(cm1[1,0]+cm1[1,1])\n",
        "prec=cm1[1,1]/(cm1[1,1]+cm1[0,1])\n",
        "rec=cm1[1,1]/(cm1[1,1]+cm1[1,0])\n",
        "f1 = 2 * (prec * rec) / (prec + rec)\n",
        "mcc = matthews_corrcoef(y_true, np.round(y_pred))\n",
        "\n",
        "prc = metrics.average_precision_score(y_true, y_pred)\n",
        "\n",
        "print(\"============= INFERENCE BY NEURAL NETWORK ===============\")\n",
        "try:\n",
        "  auc = metrics.roc_auc_score(y_true, y_pred)\n",
        "  print(f'accuracy: {acc}, precision: {prec}, recall: {rec}, specificity: {spec}, f1-score: {f1}, mcc: {mcc}, auroc: {auc}, auprc: {prc} ')\n",
        "  print(str(acc) + \"\\t\" + str(prec) + \"\\t\" + str(rec) + \"\\t\" + str(spec) + \"\\t\" + str(f1) + \"\\t\" + str(mcc)+\"\\t\" + str(auc)  + \"\\t\" + str(prc) + \"\\n\")\n",
        "except ValueError:\n",
        "  print(f'accuracy: {acc}, precision: {prec}, recall: {rec}, specificity: {spec}, f1-score: {f1}, mcc: {mcc}, auroc: nan, auprc: {prc} ')\n",
        "  print(str(acc) + \"\\t\" + str(prec) + \"\\t\" + str(rec) + \"\\t\" + str(spec) + \"\\t\" + str(f1) + \"\\t\" + str(mcc)+\"\\t nan\"  + \"\\t\" + str(prc) + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VG2O6OuJtmHK"
      },
      "source": [
        "#### Evaluate with trained FSNN-LGBM prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9abIldUQtmHK",
        "outputId": "b2703174-1205-4213-c81d-b7cb68e08ed5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "350/350 [==============================] - 1s 2ms/step\n",
            "accuracy: 0.5136753664640686, precision: 0.5524331734064428, recall: 0.14408294601358598, specificity: 0.8832677869145513, f1-score: 0.22855522472706652, mcc: 0.04060969083429286, auroc: 0.5136753664640686, auprc: 0.5075547260932415 \n",
            "0.5136753664640686\t0.5524331734064428\t0.14408294601358598\t0.8832677869145513\t0.22855522472706652\t0.04060969083429286\t0.5136753664640686\t0.5075547260932415\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Read new data\n",
        "\n",
        "FEATURE_NUM = seq_tensor.shape[1]\n",
        "# Create data frame of pairs and labels\n",
        "df = np.hstack([seq_tensor[seq_index1], seq_tensor[seq_index2]])\n",
        "df = np.hstack([df, class_labels.reshape(-1,1)])\n",
        "df = pd.DataFrame(df)\n",
        "df = df.sample(frac=1)\n",
        "\n",
        "# Create pairs matrix and label vector\n",
        "X = df.iloc[:,0:FEATURE_NUM*2].values\n",
        "y = df.iloc[:,FEATURE_NUM*2:].values\n",
        "\n",
        "Trainlabels=y\n",
        "\n",
        "# standard scaler\n",
        "# scaler = StandardScaler().fit(X)\n",
        "# scaler = MinMaxScaler().fit(X)\n",
        "\n",
        "# scaler = RobustScaler().fit(X)\n",
        "X = standard_scaler.transform(X)\n",
        "\n",
        "\n",
        "X1_train = X[:, :FEATURE_NUM]\n",
        "X2_train = X[:, FEATURE_NUM:]\n",
        "\n",
        "\n",
        "# Predict representation from trained neural network\n",
        "\n",
        "intermediate_layer_model = Model(inputs=model.input,outputs=model.get_layer('Merged_feature_1').output)\n",
        "# Use intermediate layer to transform pairs matrix\n",
        "intermediate_output_p1 = intermediate_layer_model.predict([X1_train,X2_train])  \n",
        "p_merge=pd.DataFrame(intermediate_output_p1)    \n",
        "\n",
        "# create dataframe use transformed pairs matrix outputs and labels\n",
        "X_train_feat=pd.concat((p_merge,pd.DataFrame(pd.DataFrame(Trainlabels))),axis=1,ignore_index=True)\n",
        "\n",
        "# write to file dataframe of transformed pairs matrix and labels\n",
        "X_train_feat.to_csv('X_train.csv',header=False, index=False)\n",
        "\n",
        "# read dataframe of transformed pairs matrix and labels\n",
        "Train=pd.read_csv(\"X_train.csv\",header=None)\n",
        "Train=Train.sample(frac=1)\n",
        "\n",
        "X=Train.iloc[:,0:64].values\n",
        "y=Train.iloc[:,64:].values.ravel()\n",
        "\n",
        "extracted_df=X_train_feat\n",
        "\n",
        "X=robust_scaler.transform(X)\n",
        "\n",
        "# Predict probability from neural network output for new data\n",
        "y_true=y\n",
        "y_pred = model_.predict(X)\n",
        "cm1=confusion_matrix(y_true, np.round(y_pred))\n",
        "acc = (cm1[0,0]+cm1[1,1])/(cm1[0,0]+cm1[0,1]+cm1[1,0]+cm1[1,1])\n",
        "spec= (cm1[0,0])/(cm1[0,0]+cm1[0,1])\n",
        "sens = (cm1[1,1])/(cm1[1,0]+cm1[1,1])\n",
        "prec=cm1[1,1]/(cm1[1,1]+cm1[0,1])\n",
        "rec=cm1[1,1]/(cm1[1,1]+cm1[1,0])\n",
        "f1 = 2 * (prec * rec) / (prec + rec)\n",
        "mcc = matthews_corrcoef(y_true, np.round(y_pred))\n",
        "\n",
        "prc = metrics.average_precision_score(y_true, y_pred)\n",
        "\n",
        "try:\n",
        "  auc = metrics.roc_auc_score(y_true, y_pred)\n",
        "  print(f'accuracy: {acc}, precision: {prec}, recall: {rec}, specificity: {spec}, f1-score: {f1}, mcc: {mcc}, auroc: {auc}, auprc: {prc} ')\n",
        "  print(str(acc) + \"\\t\" + str(prec) + \"\\t\" + str(rec) + \"\\t\" + str(spec) + \"\\t\" + str(f1) + \"\\t\" + str(mcc)+\"\\t\" + str(auc)  + \"\\t\" + str(prc) + \"\\n\")\n",
        "except ValueError:\n",
        "  print(f'accuracy: {acc}, precision: {prec}, recall: {rec}, specificity: {spec}, f1-score: {f1}, mcc: {mcc}, auroc: nan, auprc: {prc} ')\n",
        "  print(str(acc) + \"\\t\" + str(prec) + \"\\t\" + str(rec) + \"\\t\" + str(spec) + \"\\t\" + str(f1) + \"\\t\" + str(mcc)+\"\\t nan\"  + \"\\t\" + str(prc) + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOcYF8OwtmY5"
      },
      "source": [
        "### Martin-2005"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWK9Hy7wtmY6"
      },
      "source": [
        "#### Read dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8VVlYajAtmY7",
        "outputId": "e1291e44-180e-4ae0-bdd0-cc3576264c6a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-12-17 17:15:09--  https://raw.githubusercontent.com/anhvt00/MCAPS/master/data/Golden-standard-datasets/Martin-2005/martin_pairs.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 39040 (38K) [text/plain]\n",
            "Saving to: ‘martin_pairs.tsv’\n",
            "\n",
            "martin_pairs.tsv    100%[===================>]  38.12K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-12-17 17:15:10 (156 MB/s) - ‘martin_pairs.tsv’ saved [39040/39040]\n",
            "\n",
            "--2022-12-17 17:15:10--  https://raw.githubusercontent.com/anhvt00/MCAPS/master/data/Golden-standard-datasets/Martin-2005/martin_dict.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 508100 (496K) [text/plain]\n",
            "Saving to: ‘martin_dict.tsv’\n",
            "\n",
            "martin_dict.tsv     100%[===================>] 496.19K  --.-KB/s    in 0.003s  \n",
            "\n",
            "2022-12-17 17:15:11 (167 MB/s) - ‘martin_dict.tsv’ saved [508100/508100]\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2878it [00:00, 279704.50it/s]\n",
            "100%|██████████| 1414/1414 [00:02<00:00, 520.56it/s]\n",
            "100%|██████████| 2878/2878 [00:00<00:00, 1320700.98it/s]\n",
            "100%|██████████| 2878/2878 [00:00<00:00, 2013545.77it/s]\n"
          ]
        }
      ],
      "source": [
        "id2seq_file = 'martin_dict.tsv'\n",
        "id2index = {}\n",
        "seqs = []\n",
        "index = 0\n",
        "sid1_index = 0\n",
        "sid2_index = 1\n",
        "ds_file = 'martin_pairs.tsv'\n",
        "label_index = 2\n",
        "use_emb = 'ac5_aph.txt'\n",
        "\n",
        "if not os.path.isfile(ds_file) or not os.path.isfile(id2seq_file):\n",
        "  !wget https://raw.githubusercontent.com/anhvt00/MCAPS/master/data/Golden-standard-datasets/Martin-2005/martin_pairs.tsv\n",
        "  !wget https://raw.githubusercontent.com/anhvt00/MCAPS/master/data/Golden-standard-datasets/Martin-2005/martin_dict.tsv\n",
        "\n",
        "\n",
        "\n",
        "# Create line variable as a list of protein sequences with index is the number of protein sequences\n",
        "# id2index is a dictionary of protein id and incremental index number \n",
        "for line in open(id2seq_file):\n",
        "    line = line.strip().split('\\t')\n",
        "    id2index[line[0]] = index\n",
        "    seqs.append(line[1])\n",
        "    index += 1\n",
        "\n",
        "seq_array = []\n",
        "id2_aid = {}\n",
        "sid = 0\n",
        "\n",
        "seq2t = s2t(use_emb)\n",
        "\n",
        "max_data = -1\n",
        "limit_data = max_data > 0\n",
        "raw_data = []\n",
        "skip_head = False\n",
        "x = None\n",
        "count = 0\n",
        "\n",
        "# Create sequence array as a list of protein strings\n",
        "for line in tqdm(open(ds_file)):\n",
        "    if skip_head:\n",
        "        skip_head = False\n",
        "        continue\n",
        "    line = line.rstrip('\\n').rstrip('\\r').split('\\t')\n",
        "    if id2index.get(line[sid1_index]) is None or id2index.get(line[sid2_index]) is None:\n",
        "        continue\n",
        "    if id2_aid.get(line[sid1_index]) is None:\n",
        "        id2_aid[line[sid1_index]] = sid\n",
        "        sid += 1\n",
        "        seq_array.append(seqs[id2index[line[sid1_index]]])\n",
        "    line[sid1_index] = id2_aid[line[sid1_index]]\n",
        "    if id2_aid.get(line[sid2_index]) is None:\n",
        "        id2_aid[line[sid2_index]] = sid\n",
        "        sid += 1\n",
        "        seq_array.append(seqs[id2index[line[sid2_index]]])\n",
        "    line[sid2_index] = id2_aid[line[sid2_index]]\n",
        "    raw_data.append(line)\n",
        "    if limit_data:\n",
        "        count += 1\n",
        "        if count >= max_data:\n",
        "            break\n",
        "\n",
        "len_m_seq = np.array([len(line.split()) for line in seq_array])\n",
        "avg_m_seq = int(np.average(len_m_seq)) + 1\n",
        "max_m_seq = max(len_m_seq)\n",
        "dim = seq2t.dim\n",
        "\n",
        "# seq_tensor is tensor representation of dataset having shape of (number_of_sequences, padding_length, embedding_dim_of_aa)\n",
        "# Random for distribution of class labels\n",
        "np.random.seed(42)\n",
        "np.random.shuffle(raw_data)\n",
        "seq_tensor = np.array([encode_seq(line) for line in tqdm(seq_array)])\n",
        "\n",
        "# Extract index of 1st and 2nd sequences in pairs\n",
        "seq_index1 = np.array([line[sid1_index] for line in tqdm(raw_data)])\n",
        "seq_index2 = np.array([line[sid2_index] for line in tqdm(raw_data)])\n",
        "\n",
        "# Assign labels for pairs of sequences\n",
        "class_map = {'0':1,'1':0}\n",
        "class_labels = np.zeros((len(raw_data,)))\n",
        "for i in range(len(raw_data)):\n",
        "  class_labels[i] = float(raw_data[i][label_index])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "NLDHhLMetmY9",
        "outputId": "ef87a3c0-deaf-46e1-f1ae-78eccbf0b835"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================= INFER BY TRAINED NEURAL NETWORK ON TRAINING SET ==============================\n",
            "90/90 [==============================] - 0s 2ms/step\n",
            "============= INFERENCE BY NEURAL NETWORK ===============\n",
            "accuracy: 0.4742876997915219, precision: 0.33568904593639576, recall: 0.06690140845070422, specificity: 0.8710562414266118, f1-score: 0.11156782149148563, mcc: -0.10417158582534025, auroc: 0.42339061805676304, auprc: 0.43704281323156763 \n",
            "0.4742876997915219\t0.33568904593639576\t0.06690140845070422\t0.8710562414266118\t0.11156782149148563\t-0.10417158582534025\t0.42339061805676304\t0.43704281323156763\n",
            "\n"
          ]
        }
      ],
      "source": [
        "##################################### Load Positive and Negative Dataset ##########################################################\n",
        "\n",
        "FEATURE_NUM = seq_tensor.shape[1]\n",
        "# Create data frame of pairs and labels\n",
        "df = np.hstack([seq_tensor[seq_index1], seq_tensor[seq_index2]])\n",
        "df = np.hstack([df, class_labels.reshape(-1,1)])\n",
        "df = pd.DataFrame(df)\n",
        "df = df.sample(frac=1)\n",
        "\n",
        "# Create pairs matrix and label vector\n",
        "X = df.iloc[:,0:FEATURE_NUM*2].values\n",
        "y = df.iloc[:,FEATURE_NUM*2:].values\n",
        "\n",
        "\n",
        "# standard scaler\n",
        "X = standard_scaler.transform(X)\n",
        "\n",
        "\n",
        "X1_train = X[:, :FEATURE_NUM]\n",
        "X2_train = X[:, FEATURE_NUM:]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"============================= INFER BY TRAINED NEURAL NETWORK ON TRAINING SET ==============================\")\n",
        "y_true = y\n",
        "y_pred = model.predict([X1_train, X2_train])\n",
        "cm1=confusion_matrix(y_true, np.round(y_pred))\n",
        "acc = (cm1[0,0]+cm1[1,1])/(cm1[0,0]+cm1[0,1]+cm1[1,0]+cm1[1,1])\n",
        "spec= (cm1[0,0])/(cm1[0,0]+cm1[0,1])\n",
        "sens = (cm1[1,1])/(cm1[1,0]+cm1[1,1])\n",
        "prec=cm1[1,1]/(cm1[1,1]+cm1[0,1])\n",
        "rec=cm1[1,1]/(cm1[1,1]+cm1[1,0])\n",
        "f1 = 2 * (prec * rec) / (prec + rec)\n",
        "mcc = matthews_corrcoef(y_true, np.round(y_pred))\n",
        "\n",
        "prc = metrics.average_precision_score(y_true, y_pred)\n",
        "\n",
        "print(\"============= INFERENCE BY NEURAL NETWORK ===============\")\n",
        "try:\n",
        "  auc = metrics.roc_auc_score(y_true, y_pred)\n",
        "  print(f'accuracy: {acc}, precision: {prec}, recall: {rec}, specificity: {spec}, f1-score: {f1}, mcc: {mcc}, auroc: {auc}, auprc: {prc} ')\n",
        "  print(str(acc) + \"\\t\" + str(prec) + \"\\t\" + str(rec) + \"\\t\" + str(spec) + \"\\t\" + str(f1) + \"\\t\" + str(mcc)+\"\\t\" + str(auc)  + \"\\t\" + str(prc) + \"\\n\")\n",
        "except ValueError:\n",
        "  print(f'accuracy: {acc}, precision: {prec}, recall: {rec}, specificity: {spec}, f1-score: {f1}, mcc: {mcc}, auroc: nan, auprc: {prc} ')\n",
        "  print(str(acc) + \"\\t\" + str(prec) + \"\\t\" + str(rec) + \"\\t\" + str(spec) + \"\\t\" + str(f1) + \"\\t\" + str(mcc)+\"\\t nan\"  + \"\\t\" + str(prc) + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kB3rR8EmtmY9"
      },
      "source": [
        "#### Evaluate with trained FSNN-LGBM prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "uqbJKEdLtmY-",
        "outputId": "99366baa-3e09-4715-fb65-e48999224fbf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "90/90 [==============================] - 0s 2ms/step\n",
            "accuracy: 0.48054204308547604, precision: 0.41418764302059496, recall: 0.12746478873239436, specificity: 0.8244170096021948, f1-score: 0.19493807215939685, mcc: -0.0670359416015423, auroc: 0.47594089916729454, auprc: 0.4833016371470315 \n",
            "0.48054204308547604\t0.41418764302059496\t0.12746478873239436\t0.8244170096021948\t0.19493807215939685\t-0.0670359416015423\t0.47594089916729454\t0.4833016371470315\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Read new data\n",
        "\n",
        "FEATURE_NUM = seq_tensor.shape[1]\n",
        "# Create data frame of pairs and labels\n",
        "df = np.hstack([seq_tensor[seq_index1], seq_tensor[seq_index2]])\n",
        "df = np.hstack([df, class_labels.reshape(-1,1)])\n",
        "df = pd.DataFrame(df)\n",
        "df = df.sample(frac=1)\n",
        "\n",
        "# Create pairs matrix and label vector\n",
        "X = df.iloc[:,0:FEATURE_NUM*2].values\n",
        "y = df.iloc[:,FEATURE_NUM*2:].values\n",
        "\n",
        "Trainlabels=y\n",
        "\n",
        "# standard scaler\n",
        "# scaler = StandardScaler().fit(X)\n",
        "# scaler = MinMaxScaler().fit(X)\n",
        "\n",
        "# scaler = RobustScaler().fit(X)\n",
        "X = standard_scaler.transform(X)\n",
        "\n",
        "\n",
        "X1_train = X[:, :FEATURE_NUM]\n",
        "X2_train = X[:, FEATURE_NUM:]\n",
        "\n",
        "\n",
        "# Predict representation from trained neural network\n",
        "\n",
        "intermediate_layer_model = Model(inputs=model.input,outputs=model.get_layer('Merged_feature_1').output)\n",
        "# Use intermediate layer to transform pairs matrix\n",
        "intermediate_output_p1 = intermediate_layer_model.predict([X1_train,X2_train])  \n",
        "p_merge=pd.DataFrame(intermediate_output_p1)    \n",
        "\n",
        "# create dataframe use transformed pairs matrix outputs and labels\n",
        "X_train_feat=pd.concat((p_merge,pd.DataFrame(pd.DataFrame(Trainlabels))),axis=1,ignore_index=True)\n",
        "\n",
        "# write to file dataframe of transformed pairs matrix and labels\n",
        "X_train_feat.to_csv('X_train.csv',header=False, index=False)\n",
        "\n",
        "# read dataframe of transformed pairs matrix and labels\n",
        "Train=pd.read_csv(\"X_train.csv\",header=None)\n",
        "Train=Train.sample(frac=1)\n",
        "\n",
        "X=Train.iloc[:,0:64].values\n",
        "y=Train.iloc[:,64:].values.ravel()\n",
        "\n",
        "extracted_df=X_train_feat\n",
        "\n",
        "X=robust_scaler.transform(X)\n",
        "\n",
        "# Predict probability from neural network output for new data\n",
        "y_true=y\n",
        "y_pred = model_.predict(X)\n",
        "cm1=confusion_matrix(y_true, np.round(y_pred))\n",
        "acc = (cm1[0,0]+cm1[1,1])/(cm1[0,0]+cm1[0,1]+cm1[1,0]+cm1[1,1])\n",
        "spec= (cm1[0,0])/(cm1[0,0]+cm1[0,1])\n",
        "sens = (cm1[1,1])/(cm1[1,0]+cm1[1,1])\n",
        "prec=cm1[1,1]/(cm1[1,1]+cm1[0,1])\n",
        "rec=cm1[1,1]/(cm1[1,1]+cm1[1,0])\n",
        "f1 = 2 * (prec * rec) / (prec + rec)\n",
        "mcc = matthews_corrcoef(y_true, np.round(y_pred))\n",
        "\n",
        "prc = metrics.average_precision_score(y_true, y_pred)\n",
        "\n",
        "try:\n",
        "  auc = metrics.roc_auc_score(y_true, y_pred)\n",
        "  print(f'accuracy: {acc}, precision: {prec}, recall: {rec}, specificity: {spec}, f1-score: {f1}, mcc: {mcc}, auroc: {auc}, auprc: {prc} ')\n",
        "  print(str(acc) + \"\\t\" + str(prec) + \"\\t\" + str(rec) + \"\\t\" + str(spec) + \"\\t\" + str(f1) + \"\\t\" + str(mcc)+\"\\t\" + str(auc)  + \"\\t\" + str(prc) + \"\\n\")\n",
        "except ValueError:\n",
        "  print(f'accuracy: {acc}, precision: {prec}, recall: {rec}, specificity: {spec}, f1-score: {f1}, mcc: {mcc}, auroc: nan, auprc: {prc} ')\n",
        "  print(str(acc) + \"\\t\" + str(prec) + \"\\t\" + str(rec) + \"\\t\" + str(spec) + \"\\t\" + str(f1) + \"\\t\" + str(mcc)+\"\\t nan\"  + \"\\t\" + str(prc) + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2R1ESK4XvDzT"
      },
      "source": [
        "### Chen-2019 multispecies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4oTgpxCvDzU"
      },
      "source": [
        "#### Read dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "FhVmDl2evDzU",
        "outputId": "47fa49b4-2662-43e9-92ac-62edcd8d9adc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-12-17 17:15:14--  https://raw.githubusercontent.com/anhvt00/MCAPS/master/data/Independent-testsets/multi-species/dict.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5878871 (5.6M) [text/plain]\n",
            "Saving to: ‘dict.tsv’\n",
            "\n",
            "dict.tsv            100%[===================>]   5.61M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2022-12-17 17:15:15 (389 MB/s) - ‘dict.tsv’ saved [5878871/5878871]\n",
            "\n",
            "--2022-12-17 17:15:15--  https://raw.githubusercontent.com/anhvt00/MCAPS/master/data/Independent-testsets/multi-species/pairs.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1346871 (1.3M) [text/plain]\n",
            "Saving to: ‘pairs.tsv’\n",
            "\n",
            "pairs.tsv           100%[===================>]   1.28M  --.-KB/s    in 0.006s  \n",
            "\n",
            "2022-12-17 17:15:16 (229 MB/s) - ‘pairs.tsv’ saved [1346871/1346871]\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "65918it [00:00, 495058.25it/s]\n",
            "100%|██████████| 11529/11529 [00:32<00:00, 349.61it/s]\n",
            "100%|██████████| 65918/65918 [00:00<00:00, 1854600.48it/s]\n",
            "100%|██████████| 65918/65918 [00:00<00:00, 2199681.21it/s]\n"
          ]
        }
      ],
      "source": [
        "id2seq_file = 'dict.tsv'\n",
        "id2index = {}\n",
        "seqs = []\n",
        "index = 0\n",
        "sid1_index = 0\n",
        "sid2_index = 1\n",
        "ds_file = 'pairs.tsv'\n",
        "label_index = 2\n",
        "use_emb = 'ac5_aph.txt'\n",
        "\n",
        "if not os.path.isfile(ds_file) or not os.path.isfile(id2seq_file):\n",
        "  !wget https://raw.githubusercontent.com/anhvt00/MCAPS/master/data/Independent-testsets/multi-species/dict.tsv\n",
        "  !wget https://raw.githubusercontent.com/anhvt00/MCAPS/master/data/Independent-testsets/multi-species/pairs.tsv\n",
        "\n",
        "\n",
        "\n",
        "# Create line variable as a list of protein sequences with index is the number of protein sequences\n",
        "# id2index is a dictionary of protein id and incremental index number \n",
        "for line in open(id2seq_file):\n",
        "    line = line.strip().split('\\t')\n",
        "    id2index[line[0]] = index\n",
        "    seqs.append(line[1])\n",
        "    index += 1\n",
        "\n",
        "seq_array = []\n",
        "id2_aid = {}\n",
        "sid = 0\n",
        "\n",
        "seq2t = s2t(use_emb)\n",
        "\n",
        "max_data = -1\n",
        "limit_data = max_data > 0\n",
        "raw_data = []\n",
        "skip_head = False\n",
        "x = None\n",
        "count = 0\n",
        "\n",
        "# Create sequence array as a list of protein strings\n",
        "for line in tqdm(open(ds_file)):\n",
        "    if skip_head:\n",
        "        skip_head = False\n",
        "        continue\n",
        "    line = line.rstrip('\\n').rstrip('\\r').split('\\t')\n",
        "    if id2index.get(line[sid1_index]) is None or id2index.get(line[sid2_index]) is None:\n",
        "        continue\n",
        "    if id2_aid.get(line[sid1_index]) is None:\n",
        "        id2_aid[line[sid1_index]] = sid\n",
        "        sid += 1\n",
        "        seq_array.append(seqs[id2index[line[sid1_index]]])\n",
        "    line[sid1_index] = id2_aid[line[sid1_index]]\n",
        "    if id2_aid.get(line[sid2_index]) is None:\n",
        "        id2_aid[line[sid2_index]] = sid\n",
        "        sid += 1\n",
        "        seq_array.append(seqs[id2index[line[sid2_index]]])\n",
        "    line[sid2_index] = id2_aid[line[sid2_index]]\n",
        "    raw_data.append(line)\n",
        "    if limit_data:\n",
        "        count += 1\n",
        "        if count >= max_data:\n",
        "            break\n",
        "\n",
        "len_m_seq = np.array([len(line.split()) for line in seq_array])\n",
        "avg_m_seq = int(np.average(len_m_seq)) + 1\n",
        "max_m_seq = max(len_m_seq)\n",
        "dim = seq2t.dim\n",
        "\n",
        "# seq_tensor is tensor representation of dataset having shape of (number_of_sequences, padding_length, embedding_dim_of_aa)\n",
        "# Random for distribution of class labels\n",
        "np.random.seed(42)\n",
        "np.random.shuffle(raw_data)\n",
        "seq_tensor = np.array([encode_seq(line) for line in tqdm(seq_array)])\n",
        "\n",
        "# Extract index of 1st and 2nd sequences in pairs\n",
        "seq_index1 = np.array([line[sid1_index] for line in tqdm(raw_data)])\n",
        "seq_index2 = np.array([line[sid2_index] for line in tqdm(raw_data)])\n",
        "\n",
        "# Assign labels for pairs of sequences\n",
        "class_map = {'0':1,'1':0}\n",
        "class_labels = np.zeros((len(raw_data,)))\n",
        "for i in range(len(raw_data)):\n",
        "  class_labels[i] = float(raw_data[i][label_index])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "m8Gtn1wNvDzU",
        "outputId": "157515ce-40f6-48c1-94fe-90a6977fe33c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================= INFER BY TRAINED NEURAL NETWORK ON TRAINING SET ==============================\n",
            "2060/2060 [==============================] - 4s 2ms/step\n",
            "============= INFERENCE BY NEURAL NETWORK ===============\n",
            "accuracy: 0.49214175187353987, precision: 0.46588514225500527, recall: 0.10731514912467005, specificity: 0.8769683546224096, f1-score: 0.17444699267588964, mcc: -0.024616174578581812, auroc: 0.45783036257878673, auprc: 0.47437603893757857 \n",
            "0.49214175187353987\t0.46588514225500527\t0.10731514912467005\t0.8769683546224096\t0.17444699267588964\t-0.024616174578581812\t0.45783036257878673\t0.47437603893757857\n",
            "\n"
          ]
        }
      ],
      "source": [
        "##################################### Load Positive and Negative Dataset ##########################################################\n",
        "\n",
        "FEATURE_NUM = seq_tensor.shape[1]\n",
        "# Create data frame of pairs and labels\n",
        "df = np.hstack([seq_tensor[seq_index1], seq_tensor[seq_index2]])\n",
        "df = np.hstack([df, class_labels.reshape(-1,1)])\n",
        "df = pd.DataFrame(df)\n",
        "df = df.sample(frac=1)\n",
        "\n",
        "# Create pairs matrix and label vector\n",
        "X = df.iloc[:,0:FEATURE_NUM*2].values\n",
        "y = df.iloc[:,FEATURE_NUM*2:].values\n",
        "\n",
        "\n",
        "# standard scaler\n",
        "X = standard_scaler.transform(X)\n",
        "\n",
        "\n",
        "X1_train = X[:, :FEATURE_NUM]\n",
        "X2_train = X[:, FEATURE_NUM:]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"============================= INFER BY TRAINED NEURAL NETWORK ON TRAINING SET ==============================\")\n",
        "y_true = y\n",
        "y_pred = model.predict([X1_train, X2_train])\n",
        "cm1=confusion_matrix(y_true, np.round(y_pred))\n",
        "acc = (cm1[0,0]+cm1[1,1])/(cm1[0,0]+cm1[0,1]+cm1[1,0]+cm1[1,1])\n",
        "spec= (cm1[0,0])/(cm1[0,0]+cm1[0,1])\n",
        "sens = (cm1[1,1])/(cm1[1,0]+cm1[1,1])\n",
        "prec=cm1[1,1]/(cm1[1,1]+cm1[0,1])\n",
        "rec=cm1[1,1]/(cm1[1,1]+cm1[1,0])\n",
        "f1 = 2 * (prec * rec) / (prec + rec)\n",
        "mcc = matthews_corrcoef(y_true, np.round(y_pred))\n",
        "\n",
        "prc = metrics.average_precision_score(y_true, y_pred)\n",
        "\n",
        "print(\"============= INFERENCE BY NEURAL NETWORK ===============\")\n",
        "try:\n",
        "  auc = metrics.roc_auc_score(y_true, y_pred)\n",
        "  print(f'accuracy: {acc}, precision: {prec}, recall: {rec}, specificity: {spec}, f1-score: {f1}, mcc: {mcc}, auroc: {auc}, auprc: {prc} ')\n",
        "  print(str(acc) + \"\\t\" + str(prec) + \"\\t\" + str(rec) + \"\\t\" + str(spec) + \"\\t\" + str(f1) + \"\\t\" + str(mcc)+\"\\t\" + str(auc)  + \"\\t\" + str(prc) + \"\\n\")\n",
        "except ValueError:\n",
        "  print(f'accuracy: {acc}, precision: {prec}, recall: {rec}, specificity: {spec}, f1-score: {f1}, mcc: {mcc}, auroc: nan, auprc: {prc} ')\n",
        "  print(str(acc) + \"\\t\" + str(prec) + \"\\t\" + str(rec) + \"\\t\" + str(spec) + \"\\t\" + str(f1) + \"\\t\" + str(mcc)+\"\\t nan\"  + \"\\t\" + str(prc) + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UiRqS5YvDzU"
      },
      "source": [
        "#### Evaluate with trained FSNN-LGBM prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "RnSh2FwzvDzV",
        "outputId": "e4e8f4d9-6bee-4eb4-947d-240cbc8285e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2060/2060 [==============================] - 4s 2ms/step\n",
            "accuracy: 0.4919445371522194, precision: 0.4767858704205648, recall: 0.16544798082466095, specificity: 0.8184410934797779, f1-score: 0.24565276151004597, mcc: -0.021272372303364186, auroc: 0.4919445371522194, auprc: 0.4961592691344804 \n",
            "0.4919445371522194\t0.4767858704205648\t0.16544798082466095\t0.8184410934797779\t0.24565276151004597\t-0.021272372303364186\t0.4919445371522194\t0.4961592691344804\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Read new data\n",
        "\n",
        "FEATURE_NUM = seq_tensor.shape[1]\n",
        "# Create data frame of pairs and labels\n",
        "df = np.hstack([seq_tensor[seq_index1], seq_tensor[seq_index2]])\n",
        "df = np.hstack([df, class_labels.reshape(-1,1)])\n",
        "df = pd.DataFrame(df)\n",
        "df = df.sample(frac=1)\n",
        "\n",
        "# Create pairs matrix and label vector\n",
        "X = df.iloc[:,0:FEATURE_NUM*2].values\n",
        "y = df.iloc[:,FEATURE_NUM*2:].values\n",
        "\n",
        "Trainlabels=y\n",
        "\n",
        "# standard scaler\n",
        "# scaler = StandardScaler().fit(X)\n",
        "# scaler = MinMaxScaler().fit(X)\n",
        "\n",
        "# scaler = RobustScaler().fit(X)\n",
        "X = standard_scaler.transform(X)\n",
        "\n",
        "\n",
        "X1_train = X[:, :FEATURE_NUM]\n",
        "X2_train = X[:, FEATURE_NUM:]\n",
        "\n",
        "\n",
        "# Predict representation from trained neural network\n",
        "\n",
        "intermediate_layer_model = Model(inputs=model.input,outputs=model.get_layer('Merged_feature_1').output)\n",
        "# Use intermediate layer to transform pairs matrix\n",
        "intermediate_output_p1 = intermediate_layer_model.predict([X1_train,X2_train])  \n",
        "p_merge=pd.DataFrame(intermediate_output_p1)    \n",
        "\n",
        "# create dataframe use transformed pairs matrix outputs and labels\n",
        "X_train_feat=pd.concat((p_merge,pd.DataFrame(pd.DataFrame(Trainlabels))),axis=1,ignore_index=True)\n",
        "\n",
        "# write to file dataframe of transformed pairs matrix and labels\n",
        "X_train_feat.to_csv('X_train.csv',header=False, index=False)\n",
        "\n",
        "# read dataframe of transformed pairs matrix and labels\n",
        "Train=pd.read_csv(\"X_train.csv\",header=None)\n",
        "Train=Train.sample(frac=1)\n",
        "\n",
        "X=Train.iloc[:,0:64].values\n",
        "y=Train.iloc[:,64:].values.ravel()\n",
        "\n",
        "extracted_df=X_train_feat\n",
        "\n",
        "X=robust_scaler.transform(X)\n",
        "\n",
        "# Predict probability from neural network output for new data\n",
        "y_true=y\n",
        "y_pred = model_.predict(X)\n",
        "cm1=confusion_matrix(y_true, np.round(y_pred))\n",
        "acc = (cm1[0,0]+cm1[1,1])/(cm1[0,0]+cm1[0,1]+cm1[1,0]+cm1[1,1])\n",
        "spec= (cm1[0,0])/(cm1[0,0]+cm1[0,1])\n",
        "sens = (cm1[1,1])/(cm1[1,0]+cm1[1,1])\n",
        "prec=cm1[1,1]/(cm1[1,1]+cm1[0,1])\n",
        "rec=cm1[1,1]/(cm1[1,1]+cm1[1,0])\n",
        "f1 = 2 * (prec * rec) / (prec + rec)\n",
        "mcc = matthews_corrcoef(y_true, np.round(y_pred))\n",
        "\n",
        "prc = metrics.average_precision_score(y_true, y_pred)\n",
        "\n",
        "try:\n",
        "  auc = metrics.roc_auc_score(y_true, y_pred)\n",
        "  print(f'accuracy: {acc}, precision: {prec}, recall: {rec}, specificity: {spec}, f1-score: {f1}, mcc: {mcc}, auroc: {auc}, auprc: {prc} ')\n",
        "  print(str(acc) + \"\\t\" + str(prec) + \"\\t\" + str(rec) + \"\\t\" + str(spec) + \"\\t\" + str(f1) + \"\\t\" + str(mcc)+\"\\t\" + str(auc)  + \"\\t\" + str(prc) + \"\\n\")\n",
        "except ValueError:\n",
        "  print(f'accuracy: {acc}, precision: {prec}, recall: {rec}, specificity: {spec}, f1-score: {f1}, mcc: {mcc}, auroc: nan, auprc: {prc} ')\n",
        "  print(str(acc) + \"\\t\" + str(prec) + \"\\t\" + str(rec) + \"\\t\" + str(spec) + \"\\t\" + str(f1) + \"\\t\" + str(mcc)+\"\\t nan\"  + \"\\t\" + str(prc) + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AoIAac2uwTls"
      },
      "source": [
        "## Evaluation on inter-species datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBIaVuXxwTlt"
      },
      "source": [
        "### DENV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5hCuhTBwTlu"
      },
      "source": [
        "#### Read dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "h3uER3qGwTlu",
        "outputId": "d2014af0-15f7-4da6-ccbe-7838cadea50b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-12-17 17:16:03--  https://raw.githubusercontent.com/anhvt00/MCAPS/master/data/Interspecies-host-pathogen/human-virus/DENV/protein_pair_label.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 316639 (309K) [text/plain]\n",
            "Saving to: ‘protein_pair_label.txt’\n",
            "\n",
            "protein_pair_label. 100%[===================>] 309.22K  --.-KB/s    in 0.003s  \n",
            "\n",
            "2022-12-17 17:16:05 (100 MB/s) - ‘protein_pair_label.txt’ saved [316639/316639]\n",
            "\n",
            "--2022-12-17 17:16:05--  https://raw.githubusercontent.com/anhvt00/MCAPS/master/data/Interspecies-host-pathogen/human-virus/DENV/pro_seq.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4489762 (4.3M) [text/plain]\n",
            "Saving to: ‘pro_seq.txt’\n",
            "\n",
            "pro_seq.txt         100%[===================>]   4.28M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2022-12-17 17:16:06 (332 MB/s) - ‘pro_seq.txt’ saved [4489762/4489762]\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "10197it [00:00, 385253.64it/s]\n",
            "100%|██████████| 8028/8028 [00:24<00:00, 325.86it/s]\n",
            "100%|██████████| 10197/10197 [00:00<00:00, 1621892.98it/s]\n",
            "100%|██████████| 10197/10197 [00:00<00:00, 2274600.75it/s]\n"
          ]
        }
      ],
      "source": [
        "!rm -rf pro_seq.txt protein_pair_label.txt\n",
        "id2seq_file = 'pro_seq.txt'\n",
        "id2index = {}\n",
        "seqs = []\n",
        "index = 0\n",
        "sid1_index = 0\n",
        "sid2_index = 1\n",
        "ds_file = 'protein_pair_label.txt'\n",
        "label_index = 2\n",
        "use_emb = 'ac5_aph.txt'\n",
        "\n",
        "if not os.path.isfile(ds_file) or not os.path.isfile(id2seq_file):\n",
        "  !wget https://raw.githubusercontent.com/anhvt00/MCAPS/master/data/Interspecies-host-pathogen/human-virus/DENV/protein_pair_label.txt\n",
        "  !wget https://raw.githubusercontent.com/anhvt00/MCAPS/master/data/Interspecies-host-pathogen/human-virus/DENV/pro_seq.txt\n",
        "\n",
        "\n",
        "# Create line variable as a list of protein sequences with index is the number of protein sequences\n",
        "# id2index is a dictionary of protein id and incremental index number \n",
        "for line in open(id2seq_file):\n",
        "    line = line.strip().split('\\t')\n",
        "    id2index[line[0]] = index\n",
        "    seqs.append(line[1])\n",
        "    index += 1\n",
        "\n",
        "seq_array = []\n",
        "id2_aid = {}\n",
        "sid = 0\n",
        "\n",
        "seq2t = s2t(use_emb)\n",
        "\n",
        "max_data = -1\n",
        "limit_data = max_data > 0\n",
        "raw_data = []\n",
        "skip_head = False\n",
        "x = None\n",
        "count = 0\n",
        "\n",
        "# Create sequence array as a list of protein strings\n",
        "for line in tqdm(open(ds_file)):\n",
        "    if skip_head:\n",
        "        skip_head = False\n",
        "        continue\n",
        "    line = line.rstrip('\\n').rstrip('\\r').split('\\t')\n",
        "    if id2index.get(line[sid1_index]) is None or id2index.get(line[sid2_index]) is None:\n",
        "        continue\n",
        "    if id2_aid.get(line[sid1_index]) is None:\n",
        "        id2_aid[line[sid1_index]] = sid\n",
        "        sid += 1\n",
        "        seq_array.append(seqs[id2index[line[sid1_index]]])\n",
        "    line[sid1_index] = id2_aid[line[sid1_index]]\n",
        "    if id2_aid.get(line[sid2_index]) is None:\n",
        "        id2_aid[line[sid2_index]] = sid\n",
        "        sid += 1\n",
        "        seq_array.append(seqs[id2index[line[sid2_index]]])\n",
        "    line[sid2_index] = id2_aid[line[sid2_index]]\n",
        "    raw_data.append(line)\n",
        "    if limit_data:\n",
        "        count += 1\n",
        "        if count >= max_data:\n",
        "            break\n",
        "\n",
        "len_m_seq = np.array([len(line.split()) for line in seq_array])\n",
        "avg_m_seq = int(np.average(len_m_seq)) + 1\n",
        "max_m_seq = max(len_m_seq)\n",
        "dim = seq2t.dim\n",
        "\n",
        "# seq_tensor is tensor representation of dataset having shape of (number_of_sequences, padding_length, embedding_dim_of_aa)\n",
        "# Random for distribution of class labels\n",
        "np.random.seed(42)\n",
        "np.random.shuffle(raw_data)\n",
        "seq_tensor = np.array([encode_seq(line) for line in tqdm(seq_array)])\n",
        "\n",
        "# Extract index of 1st and 2nd sequences in pairs\n",
        "seq_index1 = np.array([line[sid1_index] for line in tqdm(raw_data)])\n",
        "seq_index2 = np.array([line[sid2_index] for line in tqdm(raw_data)])\n",
        "\n",
        "# Assign labels for pairs of sequences\n",
        "class_map = {'0':1,'1':0}\n",
        "class_labels = np.zeros((len(raw_data,)))\n",
        "for i in range(len(raw_data)):\n",
        "  class_labels[i] = float(raw_data[i][label_index])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-JbbR07pwTlw",
        "outputId": "e20ee537-cd66-4e74-8619-54f44daf06b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================= INFER BY TRAINED NEURAL NETWORK ON TRAINING SET ==============================\n",
            "319/319 [==============================] - 1s 2ms/step\n",
            "============= INFERENCE BY NEURAL NETWORK ===============\n",
            "accuracy: 0.8248504462096695, precision: 0.2002791346824843, recall: 0.30960086299892126, specificity: 0.8763754045307444, f1-score: 0.24322033898305082, mcc: 0.15383777212932226, auroc: 0.6891447862227389, auprc: 0.18495294942295404 \n",
            "0.8248504462096695\t0.2002791346824843\t0.30960086299892126\t0.8763754045307444\t0.24322033898305082\t0.15383777212932226\t0.6891447862227389\t0.18495294942295404\n",
            "\n"
          ]
        }
      ],
      "source": [
        "##################################### Load Positive and Negative Dataset ##########################################################\n",
        "\n",
        "FEATURE_NUM = seq_tensor.shape[1]\n",
        "# Create data frame of pairs and labels\n",
        "df = np.hstack([seq_tensor[seq_index1], seq_tensor[seq_index2]])\n",
        "df = np.hstack([df, class_labels.reshape(-1,1)])\n",
        "df = pd.DataFrame(df)\n",
        "df = df.sample(frac=1)\n",
        "\n",
        "# Create pairs matrix and label vector\n",
        "X = df.iloc[:,0:FEATURE_NUM*2].values\n",
        "y = df.iloc[:,FEATURE_NUM*2:].values\n",
        "\n",
        "\n",
        "# standard scaler\n",
        "X = standard_scaler.transform(X)\n",
        "\n",
        "\n",
        "X1_train = X[:, :FEATURE_NUM]\n",
        "X2_train = X[:, FEATURE_NUM:]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"============================= INFER BY TRAINED NEURAL NETWORK ON TRAINING SET ==============================\")\n",
        "y_true = y\n",
        "y_pred = model.predict([X1_train, X2_train])\n",
        "cm1=confusion_matrix(y_true, np.round(y_pred))\n",
        "acc = (cm1[0,0]+cm1[1,1])/(cm1[0,0]+cm1[0,1]+cm1[1,0]+cm1[1,1])\n",
        "spec= (cm1[0,0])/(cm1[0,0]+cm1[0,1])\n",
        "sens = (cm1[1,1])/(cm1[1,0]+cm1[1,1])\n",
        "prec=cm1[1,1]/(cm1[1,1]+cm1[0,1])\n",
        "rec=cm1[1,1]/(cm1[1,1]+cm1[1,0])\n",
        "f1 = 2 * (prec * rec) / (prec + rec)\n",
        "mcc = matthews_corrcoef(y_true, np.round(y_pred))\n",
        "\n",
        "prc = metrics.average_precision_score(y_true, y_pred)\n",
        "\n",
        "print(\"============= INFERENCE BY NEURAL NETWORK ===============\")\n",
        "try:\n",
        "  auc = metrics.roc_auc_score(y_true, y_pred)\n",
        "  print(f'accuracy: {acc}, precision: {prec}, recall: {rec}, specificity: {spec}, f1-score: {f1}, mcc: {mcc}, auroc: {auc}, auprc: {prc} ')\n",
        "  print(str(acc) + \"\\t\" + str(prec) + \"\\t\" + str(rec) + \"\\t\" + str(spec) + \"\\t\" + str(f1) + \"\\t\" + str(mcc)+\"\\t\" + str(auc)  + \"\\t\" + str(prc) + \"\\n\")\n",
        "except ValueError:\n",
        "  print(f'accuracy: {acc}, precision: {prec}, recall: {rec}, specificity: {spec}, f1-score: {f1}, mcc: {mcc}, auroc: nan, auprc: {prc} ')\n",
        "  print(str(acc) + \"\\t\" + str(prec) + \"\\t\" + str(rec) + \"\\t\" + str(spec) + \"\\t\" + str(f1) + \"\\t\" + str(mcc)+\"\\t nan\"  + \"\\t\" + str(prc) + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IXXrW-AwTlx"
      },
      "source": [
        "#### Evaluate with trained FSNN-LGBM prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7DEzk9GywTly",
        "outputId": "8c4cef29-720f-4828-83b2-7f3520f09c7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "319/319 [==============================] - 1s 2ms/step\n",
            "accuracy: 0.8036677454153183, precision: 0.18214074512123005, recall: 0.33225458468176916, specificity: 0.8508090614886732, f1-score: 0.23529411764705885, mcc: 0.14149703819261353, auroc: 0.5915318230852211, auprc: 0.12122122628917598 \n",
            "0.8036677454153183\t0.18214074512123005\t0.33225458468176916\t0.8508090614886732\t0.23529411764705885\t0.14149703819261353\t0.5915318230852211\t0.12122122628917598\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Read new data\n",
        "\n",
        "FEATURE_NUM = seq_tensor.shape[1]\n",
        "# Create data frame of pairs and labels\n",
        "df = np.hstack([seq_tensor[seq_index1], seq_tensor[seq_index2]])\n",
        "df = np.hstack([df, class_labels.reshape(-1,1)])\n",
        "df = pd.DataFrame(df)\n",
        "df = df.sample(frac=1)\n",
        "\n",
        "# Create pairs matrix and label vector\n",
        "X = df.iloc[:,0:FEATURE_NUM*2].values\n",
        "y = df.iloc[:,FEATURE_NUM*2:].values\n",
        "\n",
        "Trainlabels=y\n",
        "\n",
        "# standard scaler\n",
        "# scaler = StandardScaler().fit(X)\n",
        "# scaler = MinMaxScaler().fit(X)\n",
        "\n",
        "# scaler = RobustScaler().fit(X)\n",
        "X = standard_scaler.transform(X)\n",
        "\n",
        "\n",
        "X1_train = X[:, :FEATURE_NUM]\n",
        "X2_train = X[:, FEATURE_NUM:]\n",
        "\n",
        "\n",
        "# Predict representation from trained neural network\n",
        "\n",
        "intermediate_layer_model = Model(inputs=model.input,outputs=model.get_layer('Merged_feature_1').output)\n",
        "# Use intermediate layer to transform pairs matrix\n",
        "intermediate_output_p1 = intermediate_layer_model.predict([X1_train,X2_train])  \n",
        "p_merge=pd.DataFrame(intermediate_output_p1)    \n",
        "\n",
        "# create dataframe use transformed pairs matrix outputs and labels\n",
        "X_train_feat=pd.concat((p_merge,pd.DataFrame(pd.DataFrame(Trainlabels))),axis=1,ignore_index=True)\n",
        "\n",
        "# write to file dataframe of transformed pairs matrix and labels\n",
        "X_train_feat.to_csv('X_train.csv',header=False, index=False)\n",
        "\n",
        "# read dataframe of transformed pairs matrix and labels\n",
        "Train=pd.read_csv(\"X_train.csv\",header=None)\n",
        "Train=Train.sample(frac=1)\n",
        "\n",
        "X=Train.iloc[:,0:64].values\n",
        "y=Train.iloc[:,64:].values.ravel()\n",
        "\n",
        "extracted_df=X_train_feat\n",
        "\n",
        "X=robust_scaler.transform(X)\n",
        "\n",
        "# Predict probability from neural network output for new data\n",
        "y_true=y\n",
        "y_pred = model_.predict(X)\n",
        "cm1=confusion_matrix(y_true, np.round(y_pred))\n",
        "acc = (cm1[0,0]+cm1[1,1])/(cm1[0,0]+cm1[0,1]+cm1[1,0]+cm1[1,1])\n",
        "spec= (cm1[0,0])/(cm1[0,0]+cm1[0,1])\n",
        "sens = (cm1[1,1])/(cm1[1,0]+cm1[1,1])\n",
        "prec=cm1[1,1]/(cm1[1,1]+cm1[0,1])\n",
        "rec=cm1[1,1]/(cm1[1,1]+cm1[1,0])\n",
        "f1 = 2 * (prec * rec) / (prec + rec)\n",
        "mcc = matthews_corrcoef(y_true, np.round(y_pred))\n",
        "\n",
        "prc = metrics.average_precision_score(y_true, y_pred)\n",
        "\n",
        "try:\n",
        "  auc = metrics.roc_auc_score(y_true, y_pred)\n",
        "  print(f'accuracy: {acc}, precision: {prec}, recall: {rec}, specificity: {spec}, f1-score: {f1}, mcc: {mcc}, auroc: {auc}, auprc: {prc} ')\n",
        "  print(str(acc) + \"\\t\" + str(prec) + \"\\t\" + str(rec) + \"\\t\" + str(spec) + \"\\t\" + str(f1) + \"\\t\" + str(mcc)+\"\\t\" + str(auc)  + \"\\t\" + str(prc) + \"\\n\")\n",
        "except ValueError:\n",
        "  print(f'accuracy: {acc}, precision: {prec}, recall: {rec}, specificity: {spec}, f1-score: {f1}, mcc: {mcc}, auroc: nan, auprc: {prc} ')\n",
        "  print(str(acc) + \"\\t\" + str(prec) + \"\\t\" + str(rec) + \"\\t\" + str(spec) + \"\\t\" + str(f1) + \"\\t\" + str(mcc)+\"\\t nan\"  + \"\\t\" + str(prc) + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eN-h53jswTlz"
      },
      "source": [
        "### HIV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtI56EUswTlz"
      },
      "source": [
        "#### Read dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "nIxE3GGCwTlz",
        "outputId": "d673e3bb-45b1-47f1-9521-8eb932186c02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-12-17 17:16:33--  https://raw.githubusercontent.com/anhvt00/MCAPS/master/data/Interspecies-host-pathogen/human-virus/HIV/protein_pair_label.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1797825 (1.7M) [text/plain]\n",
            "Saving to: ‘protein_pair_label.txt’\n",
            "\n",
            "protein_pair_label. 100%[===================>]   1.71M  --.-KB/s    in 0.006s  \n",
            "\n",
            "2022-12-17 17:16:34 (308 MB/s) - ‘protein_pair_label.txt’ saved [1797825/1797825]\n",
            "\n",
            "--2022-12-17 17:16:34--  https://raw.githubusercontent.com/anhvt00/MCAPS/master/data/Interspecies-host-pathogen/human-virus/HIV/pro_seq.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 11354613 (11M) [text/plain]\n",
            "Saving to: ‘pro_seq.txt’\n",
            "\n",
            "pro_seq.txt         100%[===================>]  10.83M  --.-KB/s    in 0.02s   \n",
            "\n",
            "2022-12-17 17:16:35 (500 MB/s) - ‘pro_seq.txt’ saved [11354613/11354613]\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "108680it [00:00, 256156.82it/s]\n",
            "100%|██████████| 20464/20464 [01:02<00:00, 327.02it/s]\n",
            "100%|██████████| 108680/108680 [00:00<00:00, 1771293.07it/s]\n",
            "100%|██████████| 108680/108680 [00:00<00:00, 2037824.82it/s]\n"
          ]
        }
      ],
      "source": [
        "!rm -rf pro_seq.txt protein_pair_label.txt\n",
        "id2seq_file = 'pro_seq.txt'\n",
        "id2index = {}\n",
        "seqs = []\n",
        "index = 0\n",
        "sid1_index = 0\n",
        "sid2_index = 1\n",
        "ds_file = 'protein_pair_label.txt'\n",
        "label_index = 2\n",
        "use_emb = 'ac5_aph.txt'\n",
        "\n",
        "if not os.path.isfile(ds_file) or not os.path.isfile(id2seq_file):\n",
        "  !wget https://raw.githubusercontent.com/anhvt00/MCAPS/master/data/Interspecies-host-pathogen/human-virus/HIV/protein_pair_label.txt\n",
        "  !wget https://raw.githubusercontent.com/anhvt00/MCAPS/master/data/Interspecies-host-pathogen/human-virus/HIV/pro_seq.txt\n",
        "\n",
        "\n",
        "\n",
        "# Create line variable as a list of protein sequences with index is the number of protein sequences\n",
        "# id2index is a dictionary of protein id and incremental index number \n",
        "for line in open(id2seq_file):\n",
        "    line = line.strip().split('\\t')\n",
        "    id2index[line[0]] = index\n",
        "    seqs.append(line[1])\n",
        "    index += 1\n",
        "\n",
        "seq_array = []\n",
        "id2_aid = {}\n",
        "sid = 0\n",
        "\n",
        "seq2t = s2t(use_emb)\n",
        "\n",
        "max_data = -1\n",
        "limit_data = max_data > 0\n",
        "raw_data = []\n",
        "skip_head = False\n",
        "x = None\n",
        "count = 0\n",
        "\n",
        "# Create sequence array as a list of protein strings\n",
        "for line in tqdm(open(ds_file)):\n",
        "    if skip_head:\n",
        "        skip_head = False\n",
        "        continue\n",
        "    line = line.rstrip('\\n').rstrip('\\r').split('\\t')\n",
        "    if id2index.get(line[sid1_index]) is None or id2index.get(line[sid2_index]) is None:\n",
        "        continue\n",
        "    if id2_aid.get(line[sid1_index]) is None:\n",
        "        id2_aid[line[sid1_index]] = sid\n",
        "        sid += 1\n",
        "        seq_array.append(seqs[id2index[line[sid1_index]]])\n",
        "    line[sid1_index] = id2_aid[line[sid1_index]]\n",
        "    if id2_aid.get(line[sid2_index]) is None:\n",
        "        id2_aid[line[sid2_index]] = sid\n",
        "        sid += 1\n",
        "        seq_array.append(seqs[id2index[line[sid2_index]]])\n",
        "    line[sid2_index] = id2_aid[line[sid2_index]]\n",
        "    raw_data.append(line)\n",
        "    if limit_data:\n",
        "        count += 1\n",
        "        if count >= max_data:\n",
        "            break\n",
        "\n",
        "len_m_seq = np.array([len(line.split()) for line in seq_array])\n",
        "avg_m_seq = int(np.average(len_m_seq)) + 1\n",
        "max_m_seq = max(len_m_seq)\n",
        "dim = seq2t.dim\n",
        "\n",
        "# seq_tensor is tensor representation of dataset having shape of (number_of_sequences, padding_length, embedding_dim_of_aa)\n",
        "# Random for distribution of class labels\n",
        "np.random.seed(42)\n",
        "np.random.shuffle(raw_data)\n",
        "seq_tensor = np.array([encode_seq(line) for line in tqdm(seq_array)])\n",
        "\n",
        "# Extract index of 1st and 2nd sequences in pairs\n",
        "seq_index1 = np.array([line[sid1_index] for line in tqdm(raw_data)])\n",
        "seq_index2 = np.array([line[sid2_index] for line in tqdm(raw_data)])\n",
        "\n",
        "# Assign labels for pairs of sequences\n",
        "class_map = {'0':1,'1':0}\n",
        "class_labels = np.zeros((len(raw_data,)))\n",
        "for i in range(len(raw_data)):\n",
        "  class_labels[i] = float(raw_data[i][label_index])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "PHpbenT6wTl0",
        "outputId": "a5448ffd-76fd-402d-d905-017d09fa58a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================= INFER BY TRAINED NEURAL NETWORK ON TRAINING SET ==============================\n",
            "3397/3397 [==============================] - 7s 2ms/step\n",
            "============= INFERENCE BY NEURAL NETWORK ===============\n",
            "accuracy: 0.8250736105999263, precision: 0.18692998697113078, recall: 0.27591093117408905, specificity: 0.8799898785425101, f1-score: 0.22286718718064016, mcc: 0.13149038251945297, auroc: 0.6549280802832369, auprc: 0.15326659553790065 \n",
            "0.8250736105999263\t0.18692998697113078\t0.27591093117408905\t0.8799898785425101\t0.22286718718064016\t0.13149038251945297\t0.6549280802832369\t0.15326659553790065\n",
            "\n"
          ]
        }
      ],
      "source": [
        "##################################### Load Positive and Negative Dataset ##########################################################\n",
        "\n",
        "FEATURE_NUM = seq_tensor.shape[1]\n",
        "# Create data frame of pairs and labels\n",
        "df = np.hstack([seq_tensor[seq_index1], seq_tensor[seq_index2]])\n",
        "df = np.hstack([df, class_labels.reshape(-1,1)])\n",
        "df = pd.DataFrame(df)\n",
        "df = df.sample(frac=1)\n",
        "\n",
        "# Create pairs matrix and label vector\n",
        "X = df.iloc[:,0:FEATURE_NUM*2].values\n",
        "y = df.iloc[:,FEATURE_NUM*2:].values\n",
        "\n",
        "\n",
        "# standard scaler\n",
        "X = standard_scaler.transform(X)\n",
        "\n",
        "\n",
        "X1_train = X[:, :FEATURE_NUM]\n",
        "X2_train = X[:, FEATURE_NUM:]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"============================= INFER BY TRAINED NEURAL NETWORK ON TRAINING SET ==============================\")\n",
        "y_true = y\n",
        "y_pred = model.predict([X1_train, X2_train])\n",
        "cm1=confusion_matrix(y_true, np.round(y_pred))\n",
        "acc = (cm1[0,0]+cm1[1,1])/(cm1[0,0]+cm1[0,1]+cm1[1,0]+cm1[1,1])\n",
        "spec= (cm1[0,0])/(cm1[0,0]+cm1[0,1])\n",
        "sens = (cm1[1,1])/(cm1[1,0]+cm1[1,1])\n",
        "prec=cm1[1,1]/(cm1[1,1]+cm1[0,1])\n",
        "rec=cm1[1,1]/(cm1[1,1]+cm1[1,0])\n",
        "f1 = 2 * (prec * rec) / (prec + rec)\n",
        "mcc = matthews_corrcoef(y_true, np.round(y_pred))\n",
        "\n",
        "prc = metrics.average_precision_score(y_true, y_pred)\n",
        "\n",
        "print(\"============= INFERENCE BY NEURAL NETWORK ===============\")\n",
        "try:\n",
        "  auc = metrics.roc_auc_score(y_true, y_pred)\n",
        "  print(f'accuracy: {acc}, precision: {prec}, recall: {rec}, specificity: {spec}, f1-score: {f1}, mcc: {mcc}, auroc: {auc}, auprc: {prc} ')\n",
        "  print(str(acc) + \"\\t\" + str(prec) + \"\\t\" + str(rec) + \"\\t\" + str(spec) + \"\\t\" + str(f1) + \"\\t\" + str(mcc)+\"\\t\" + str(auc)  + \"\\t\" + str(prc) + \"\\n\")\n",
        "except ValueError:\n",
        "  print(f'accuracy: {acc}, precision: {prec}, recall: {rec}, specificity: {spec}, f1-score: {f1}, mcc: {mcc}, auroc: nan, auprc: {prc} ')\n",
        "  print(str(acc) + \"\\t\" + str(prec) + \"\\t\" + str(rec) + \"\\t\" + str(spec) + \"\\t\" + str(f1) + \"\\t\" + str(mcc)+\"\\t nan\"  + \"\\t\" + str(prc) + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBVv5anKwTl2"
      },
      "source": [
        "#### Evaluate with trained FSNN-LGBM prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "RMUD5V4IwTl2",
        "outputId": "f4fd01bc-cea6-46a7-8304-9c5762d78d93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3397/3397 [==============================] - 6s 2ms/step\n",
            "accuracy: 0.8033768862716231, precision: 0.17678501097169863, recall: 0.3180161943319838, specificity: 0.8519129554655871, f1-score: 0.22724478356778646, mcc: 0.13208265669426533, auroc: 0.5849645748987855, auprc: 0.11821902419215907 \n",
            "0.8033768862716231\t0.17678501097169863\t0.3180161943319838\t0.8519129554655871\t0.22724478356778646\t0.13208265669426533\t0.5849645748987855\t0.11821902419215907\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Read new data\n",
        "\n",
        "FEATURE_NUM = seq_tensor.shape[1]\n",
        "# Create data frame of pairs and labels\n",
        "df = np.hstack([seq_tensor[seq_index1], seq_tensor[seq_index2]])\n",
        "df = np.hstack([df, class_labels.reshape(-1,1)])\n",
        "df = pd.DataFrame(df)\n",
        "df = df.sample(frac=1)\n",
        "\n",
        "# Create pairs matrix and label vector\n",
        "X = df.iloc[:,0:FEATURE_NUM*2].values\n",
        "y = df.iloc[:,FEATURE_NUM*2:].values\n",
        "\n",
        "Trainlabels=y\n",
        "\n",
        "# standard scaler\n",
        "# scaler = StandardScaler().fit(X)\n",
        "# scaler = MinMaxScaler().fit(X)\n",
        "\n",
        "# scaler = RobustScaler().fit(X)\n",
        "X = standard_scaler.transform(X)\n",
        "\n",
        "\n",
        "X1_train = X[:, :FEATURE_NUM]\n",
        "X2_train = X[:, FEATURE_NUM:]\n",
        "\n",
        "\n",
        "# Predict representation from trained neural network\n",
        "\n",
        "intermediate_layer_model = Model(inputs=model.input,outputs=model.get_layer('Merged_feature_1').output)\n",
        "# Use intermediate layer to transform pairs matrix\n",
        "intermediate_output_p1 = intermediate_layer_model.predict([X1_train,X2_train])  \n",
        "p_merge=pd.DataFrame(intermediate_output_p1)    \n",
        "\n",
        "# create dataframe use transformed pairs matrix outputs and labels\n",
        "X_train_feat=pd.concat((p_merge,pd.DataFrame(pd.DataFrame(Trainlabels))),axis=1,ignore_index=True)\n",
        "\n",
        "# write to file dataframe of transformed pairs matrix and labels\n",
        "X_train_feat.to_csv('X_train.csv',header=False, index=False)\n",
        "\n",
        "# read dataframe of transformed pairs matrix and labels\n",
        "Train=pd.read_csv(\"X_train.csv\",header=None)\n",
        "Train=Train.sample(frac=1)\n",
        "\n",
        "X=Train.iloc[:,0:64].values\n",
        "y=Train.iloc[:,64:].values.ravel()\n",
        "\n",
        "extracted_df=X_train_feat\n",
        "\n",
        "X=robust_scaler.transform(X)\n",
        "\n",
        "# Predict probability from neural network output for new data\n",
        "y_true=y\n",
        "y_pred = model_.predict(X)\n",
        "cm1=confusion_matrix(y_true, np.round(y_pred))\n",
        "acc = (cm1[0,0]+cm1[1,1])/(cm1[0,0]+cm1[0,1]+cm1[1,0]+cm1[1,1])\n",
        "spec= (cm1[0,0])/(cm1[0,0]+cm1[0,1])\n",
        "sens = (cm1[1,1])/(cm1[1,0]+cm1[1,1])\n",
        "prec=cm1[1,1]/(cm1[1,1]+cm1[0,1])\n",
        "rec=cm1[1,1]/(cm1[1,1]+cm1[1,0])\n",
        "f1 = 2 * (prec * rec) / (prec + rec)\n",
        "mcc = matthews_corrcoef(y_true, np.round(y_pred))\n",
        "\n",
        "prc = metrics.average_precision_score(y_true, y_pred)\n",
        "\n",
        "try:\n",
        "  auc = metrics.roc_auc_score(y_true, y_pred)\n",
        "  print(f'accuracy: {acc}, precision: {prec}, recall: {rec}, specificity: {spec}, f1-score: {f1}, mcc: {mcc}, auroc: {auc}, auprc: {prc} ')\n",
        "  print(str(acc) + \"\\t\" + str(prec) + \"\\t\" + str(rec) + \"\\t\" + str(spec) + \"\\t\" + str(f1) + \"\\t\" + str(mcc)+\"\\t\" + str(auc)  + \"\\t\" + str(prc) + \"\\n\")\n",
        "except ValueError:\n",
        "  print(f'accuracy: {acc}, precision: {prec}, recall: {rec}, specificity: {spec}, f1-score: {f1}, mcc: {mcc}, auroc: nan, auprc: {prc} ')\n",
        "  print(str(acc) + \"\\t\" + str(prec) + \"\\t\" + str(rec) + \"\\t\" + str(spec) + \"\\t\" + str(f1) + \"\\t\" + str(mcc)+\"\\t nan\"  + \"\\t\" + str(prc) + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9kd41aTwTl3"
      },
      "source": [
        "### Hepatitis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uoOVAKcwTl5"
      },
      "source": [
        "#### Read dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Q3O_AMD8wTl5",
        "outputId": "06ffcef3-881b-408f-dc77-872e1d06c2a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-12-17 17:18:03--  https://raw.githubusercontent.com/anhvt00/MCAPS/master/data/Interspecies-host-pathogen/human-virus/Hepatitis/protein_pair_label.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 342539 (335K) [text/plain]\n",
            "Saving to: ‘protein_pair_label.txt’\n",
            "\n",
            "protein_pair_label. 100%[===================>] 334.51K  --.-KB/s    in 0.004s  \n",
            "\n",
            "2022-12-17 17:18:03 (90.6 MB/s) - ‘protein_pair_label.txt’ saved [342539/342539]\n",
            "\n",
            "--2022-12-17 17:18:03--  https://raw.githubusercontent.com/anhvt00/MCAPS/master/data/Interspecies-host-pathogen/human-virus/Hepatitis/pro_seq.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5791140 (5.5M) [text/plain]\n",
            "Saving to: ‘pro_seq.txt’\n",
            "\n",
            "pro_seq.txt         100%[===================>]   5.52M  --.-KB/s    in 0.02s   \n",
            "\n",
            "2022-12-17 17:18:04 (364 MB/s) - ‘pro_seq.txt’ saved [5791140/5791140]\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "14300it [00:00, 181878.84it/s]\n",
            "100%|██████████| 10287/10287 [00:32<00:00, 315.30it/s]\n",
            "100%|██████████| 14300/14300 [00:00<00:00, 1676033.85it/s]\n",
            "100%|██████████| 14300/14300 [00:00<00:00, 2134012.21it/s]\n"
          ]
        }
      ],
      "source": [
        "!rm -rf pro_seq.txt protein_pair_label.txt\n",
        "id2seq_file = 'pro_seq.txt'\n",
        "id2index = {}\n",
        "seqs = []\n",
        "index = 0\n",
        "sid1_index = 0\n",
        "sid2_index = 1\n",
        "ds_file = 'protein_pair_label.txt'\n",
        "label_index = 2\n",
        "use_emb = 'ac5_aph.txt'\n",
        "\n",
        "if not os.path.isfile(ds_file) or not os.path.isfile(id2seq_file):\n",
        "  !wget https://raw.githubusercontent.com/anhvt00/MCAPS/master/data/Interspecies-host-pathogen/human-virus/Hepatitis/protein_pair_label.txt\n",
        "  !wget https://raw.githubusercontent.com/anhvt00/MCAPS/master/data/Interspecies-host-pathogen/human-virus/Hepatitis/pro_seq.txt\n",
        "\n",
        "\n",
        "\n",
        "# Create line variable as a list of protein sequences with index is the number of protein sequences\n",
        "# id2index is a dictionary of protein id and incremental index number \n",
        "for line in open(id2seq_file):\n",
        "    line = line.strip().split('\\t')\n",
        "    id2index[line[0]] = index\n",
        "    seqs.append(line[1])\n",
        "    index += 1\n",
        "\n",
        "seq_array = []\n",
        "id2_aid = {}\n",
        "sid = 0\n",
        "\n",
        "seq2t = s2t(use_emb)\n",
        "\n",
        "max_data = -1\n",
        "limit_data = max_data > 0\n",
        "raw_data = []\n",
        "skip_head = False\n",
        "x = None\n",
        "count = 0\n",
        "\n",
        "# Create sequence array as a list of protein strings\n",
        "for line in tqdm(open(ds_file)):\n",
        "    if skip_head:\n",
        "        skip_head = False\n",
        "        continue\n",
        "    line = line.rstrip('\\n').rstrip('\\r').split('\\t')\n",
        "    if id2index.get(line[sid1_index]) is None or id2index.get(line[sid2_index]) is None:\n",
        "        continue\n",
        "    if id2_aid.get(line[sid1_index]) is None:\n",
        "        id2_aid[line[sid1_index]] = sid\n",
        "        sid += 1\n",
        "        seq_array.append(seqs[id2index[line[sid1_index]]])\n",
        "    line[sid1_index] = id2_aid[line[sid1_index]]\n",
        "    if id2_aid.get(line[sid2_index]) is None:\n",
        "        id2_aid[line[sid2_index]] = sid\n",
        "        sid += 1\n",
        "        seq_array.append(seqs[id2index[line[sid2_index]]])\n",
        "    line[sid2_index] = id2_aid[line[sid2_index]]\n",
        "    raw_data.append(line)\n",
        "    if limit_data:\n",
        "        count += 1\n",
        "        if count >= max_data:\n",
        "            break\n",
        "\n",
        "len_m_seq = np.array([len(line.split()) for line in seq_array])\n",
        "avg_m_seq = int(np.average(len_m_seq)) + 1\n",
        "max_m_seq = max(len_m_seq)\n",
        "dim = seq2t.dim\n",
        "\n",
        "# seq_tensor is tensor representation of dataset having shape of (number_of_sequences, padding_length, embedding_dim_of_aa)\n",
        "# Random for distribution of class labels\n",
        "np.random.seed(42)\n",
        "np.random.shuffle(raw_data)\n",
        "seq_tensor = np.array([encode_seq(line) for line in tqdm(seq_array)])\n",
        "\n",
        "# Extract index of 1st and 2nd sequences in pairs\n",
        "seq_index1 = np.array([line[sid1_index] for line in tqdm(raw_data)])\n",
        "seq_index2 = np.array([line[sid2_index] for line in tqdm(raw_data)])\n",
        "\n",
        "# Assign labels for pairs of sequences\n",
        "class_map = {'0':1,'1':0}\n",
        "class_labels = np.zeros((len(raw_data,)))\n",
        "for i in range(len(raw_data)):\n",
        "  class_labels[i] = float(raw_data[i][label_index])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "C0ermWHuwTl6",
        "outputId": "6c378ea5-f12a-4968-f6d8-c055ca6a2231"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================= INFER BY TRAINED NEURAL NETWORK ON TRAINING SET ==============================\n",
            "447/447 [==============================] - 1s 2ms/step\n",
            "============= INFERENCE BY NEURAL NETWORK ===============\n",
            "accuracy: 0.8296503496503497, precision: 0.1570048309178744, recall: 0.2, specificity: 0.8926153846153846, f1-score: 0.17591339648173207, mcc: 0.0832058912999136, auroc: 0.6169234319526626, auprc: 0.1321527721144974 \n",
            "0.8296503496503497\t0.1570048309178744\t0.2\t0.8926153846153846\t0.17591339648173207\t0.0832058912999136\t0.6169234319526626\t0.1321527721144974\n",
            "\n"
          ]
        }
      ],
      "source": [
        "##################################### Load Positive and Negative Dataset ##########################################################\n",
        "\n",
        "FEATURE_NUM = seq_tensor.shape[1]\n",
        "# Create data frame of pairs and labels\n",
        "df = np.hstack([seq_tensor[seq_index1], seq_tensor[seq_index2]])\n",
        "df = np.hstack([df, class_labels.reshape(-1,1)])\n",
        "df = pd.DataFrame(df)\n",
        "df = df.sample(frac=1)\n",
        "\n",
        "# Create pairs matrix and label vector\n",
        "X = df.iloc[:,0:FEATURE_NUM*2].values\n",
        "y = df.iloc[:,FEATURE_NUM*2:].values\n",
        "\n",
        "\n",
        "# standard scaler\n",
        "X = standard_scaler.transform(X)\n",
        "\n",
        "\n",
        "X1_train = X[:, :FEATURE_NUM]\n",
        "X2_train = X[:, FEATURE_NUM:]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"============================= INFER BY TRAINED NEURAL NETWORK ON TRAINING SET ==============================\")\n",
        "y_true = y\n",
        "y_pred = model.predict([X1_train, X2_train])\n",
        "cm1=confusion_matrix(y_true, np.round(y_pred))\n",
        "acc = (cm1[0,0]+cm1[1,1])/(cm1[0,0]+cm1[0,1]+cm1[1,0]+cm1[1,1])\n",
        "spec= (cm1[0,0])/(cm1[0,0]+cm1[0,1])\n",
        "sens = (cm1[1,1])/(cm1[1,0]+cm1[1,1])\n",
        "prec=cm1[1,1]/(cm1[1,1]+cm1[0,1])\n",
        "rec=cm1[1,1]/(cm1[1,1]+cm1[1,0])\n",
        "f1 = 2 * (prec * rec) / (prec + rec)\n",
        "mcc = matthews_corrcoef(y_true, np.round(y_pred))\n",
        "\n",
        "prc = metrics.average_precision_score(y_true, y_pred)\n",
        "\n",
        "print(\"============= INFERENCE BY NEURAL NETWORK ===============\")\n",
        "try:\n",
        "  auc = metrics.roc_auc_score(y_true, y_pred)\n",
        "  print(f'accuracy: {acc}, precision: {prec}, recall: {rec}, specificity: {spec}, f1-score: {f1}, mcc: {mcc}, auroc: {auc}, auprc: {prc} ')\n",
        "  print(str(acc) + \"\\t\" + str(prec) + \"\\t\" + str(rec) + \"\\t\" + str(spec) + \"\\t\" + str(f1) + \"\\t\" + str(mcc)+\"\\t\" + str(auc)  + \"\\t\" + str(prc) + \"\\n\")\n",
        "except ValueError:\n",
        "  print(f'accuracy: {acc}, precision: {prec}, recall: {rec}, specificity: {spec}, f1-score: {f1}, mcc: {mcc}, auroc: nan, auprc: {prc} ')\n",
        "  print(str(acc) + \"\\t\" + str(prec) + \"\\t\" + str(rec) + \"\\t\" + str(spec) + \"\\t\" + str(f1) + \"\\t\" + str(mcc)+\"\\t nan\"  + \"\\t\" + str(prc) + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDiKlHC7wTl7"
      },
      "source": [
        "#### Evaluate with trained FSNN-LGBM prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ruV37QeewTl7",
        "outputId": "f93f5a43-fa77-439b-b73e-fa62ba07be83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "447/447 [==============================] - 1s 2ms/step\n",
            "accuracy: 0.8070629370629371, precision: 0.15278438838648262, recall: 0.2469230769230769, specificity: 0.8630769230769231, f1-score: 0.18876800940899735, mcc: 0.08932250530663252, auroc: 0.555, auprc: 0.10618752974773918 \n",
            "0.8070629370629371\t0.15278438838648262\t0.2469230769230769\t0.8630769230769231\t0.18876800940899735\t0.08932250530663252\t0.555\t0.10618752974773918\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Read new data\n",
        "\n",
        "FEATURE_NUM = seq_tensor.shape[1]\n",
        "# Create data frame of pairs and labels\n",
        "df = np.hstack([seq_tensor[seq_index1], seq_tensor[seq_index2]])\n",
        "df = np.hstack([df, class_labels.reshape(-1,1)])\n",
        "df = pd.DataFrame(df)\n",
        "df = df.sample(frac=1)\n",
        "\n",
        "# Create pairs matrix and label vector\n",
        "X = df.iloc[:,0:FEATURE_NUM*2].values\n",
        "y = df.iloc[:,FEATURE_NUM*2:].values\n",
        "\n",
        "Trainlabels=y\n",
        "\n",
        "# standard scaler\n",
        "# scaler = StandardScaler().fit(X)\n",
        "# scaler = MinMaxScaler().fit(X)\n",
        "\n",
        "# scaler = RobustScaler().fit(X)\n",
        "X = standard_scaler.transform(X)\n",
        "\n",
        "\n",
        "X1_train = X[:, :FEATURE_NUM]\n",
        "X2_train = X[:, FEATURE_NUM:]\n",
        "\n",
        "\n",
        "# Predict representation from trained neural network\n",
        "\n",
        "intermediate_layer_model = Model(inputs=model.input,outputs=model.get_layer('Merged_feature_1').output)\n",
        "# Use intermediate layer to transform pairs matrix\n",
        "intermediate_output_p1 = intermediate_layer_model.predict([X1_train,X2_train])  \n",
        "p_merge=pd.DataFrame(intermediate_output_p1)    \n",
        "\n",
        "# create dataframe use transformed pairs matrix outputs and labels\n",
        "X_train_feat=pd.concat((p_merge,pd.DataFrame(pd.DataFrame(Trainlabels))),axis=1,ignore_index=True)\n",
        "\n",
        "# write to file dataframe of transformed pairs matrix and labels\n",
        "X_train_feat.to_csv('X_train.csv',header=False, index=False)\n",
        "\n",
        "# read dataframe of transformed pairs matrix and labels\n",
        "Train=pd.read_csv(\"X_train.csv\",header=None)\n",
        "Train=Train.sample(frac=1)\n",
        "\n",
        "X=Train.iloc[:,0:64].values\n",
        "y=Train.iloc[:,64:].values.ravel()\n",
        "\n",
        "extracted_df=X_train_feat\n",
        "\n",
        "X=robust_scaler.transform(X)\n",
        "\n",
        "# Predict probability from neural network output for new data\n",
        "y_true=y\n",
        "y_pred = model_.predict(X)\n",
        "cm1=confusion_matrix(y_true, np.round(y_pred))\n",
        "acc = (cm1[0,0]+cm1[1,1])/(cm1[0,0]+cm1[0,1]+cm1[1,0]+cm1[1,1])\n",
        "spec= (cm1[0,0])/(cm1[0,0]+cm1[0,1])\n",
        "sens = (cm1[1,1])/(cm1[1,0]+cm1[1,1])\n",
        "prec=cm1[1,1]/(cm1[1,1]+cm1[0,1])\n",
        "rec=cm1[1,1]/(cm1[1,1]+cm1[1,0])\n",
        "f1 = 2 * (prec * rec) / (prec + rec)\n",
        "mcc = matthews_corrcoef(y_true, np.round(y_pred))\n",
        "\n",
        "prc = metrics.average_precision_score(y_true, y_pred)\n",
        "\n",
        "try:\n",
        "  auc = metrics.roc_auc_score(y_true, y_pred)\n",
        "  print(f'accuracy: {acc}, precision: {prec}, recall: {rec}, specificity: {spec}, f1-score: {f1}, mcc: {mcc}, auroc: {auc}, auprc: {prc} ')\n",
        "  print(str(acc) + \"\\t\" + str(prec) + \"\\t\" + str(rec) + \"\\t\" + str(spec) + \"\\t\" + str(f1) + \"\\t\" + str(mcc)+\"\\t\" + str(auc)  + \"\\t\" + str(prc) + \"\\n\")\n",
        "except ValueError:\n",
        "  print(f'accuracy: {acc}, precision: {prec}, recall: {rec}, specificity: {spec}, f1-score: {f1}, mcc: {mcc}, auroc: nan, auprc: {prc} ')\n",
        "  print(str(acc) + \"\\t\" + str(prec) + \"\\t\" + str(rec) + \"\\t\" + str(spec) + \"\\t\" + str(f1) + \"\\t\" + str(mcc)+\"\\t nan\"  + \"\\t\" + str(prc) + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxBBMgKmwTl8"
      },
      "source": [
        "### Herpes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0IV0EmJwTl8"
      },
      "source": [
        "#### Read dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3YIuTIIBwTl8",
        "outputId": "aee79f22-23c8-4714-e07d-aaaaf036a221"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-12-17 17:18:40--  https://raw.githubusercontent.com/anhvt00/MCAPS/master/data/Interspecies-host-pathogen/human-virus/Herpes/protein_pair_label.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1056464 (1.0M) [text/plain]\n",
            "Saving to: ‘protein_pair_label.txt’\n",
            "\n",
            "protein_pair_label. 100%[===================>]   1.01M  --.-KB/s    in 0.006s  \n",
            "\n",
            "2022-12-17 17:18:41 (159 MB/s) - ‘protein_pair_label.txt’ saved [1056464/1056464]\n",
            "\n",
            "--2022-12-17 17:18:41--  https://raw.githubusercontent.com/anhvt00/MCAPS/master/data/Interspecies-host-pathogen/human-virus/Herpes/pro_seq.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 11055682 (11M) [text/plain]\n",
            "Saving to: ‘pro_seq.txt’\n",
            "\n",
            "pro_seq.txt         100%[===================>]  10.54M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2022-12-17 17:18:42 (414 MB/s) - ‘pro_seq.txt’ saved [11055682/11055682]\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "65626it [00:00, 419609.43it/s]\n",
            "100%|██████████| 19845/19845 [01:01<00:00, 321.61it/s]\n",
            "100%|██████████| 65626/65626 [00:00<00:00, 1784695.65it/s]\n",
            "100%|██████████| 65626/65626 [00:00<00:00, 2100272.36it/s]\n"
          ]
        }
      ],
      "source": [
        "!rm -rf pro_seq.txt protein_pair_label.txt\n",
        "id2seq_file = 'pro_seq.txt'\n",
        "id2index = {}\n",
        "seqs = []\n",
        "index = 0\n",
        "sid1_index = 0\n",
        "sid2_index = 1\n",
        "ds_file = 'protein_pair_label.txt'\n",
        "label_index = 2\n",
        "use_emb = 'ac5_aph.txt'\n",
        "\n",
        "if not os.path.isfile(ds_file) or not os.path.isfile(id2seq_file):\n",
        "  !wget https://raw.githubusercontent.com/anhvt00/MCAPS/master/data/Interspecies-host-pathogen/human-virus/Herpes/protein_pair_label.txt\n",
        "  !wget https://raw.githubusercontent.com/anhvt00/MCAPS/master/data/Interspecies-host-pathogen/human-virus/Herpes/pro_seq.txt\n",
        "\n",
        "\n",
        "\n",
        "# Create line variable as a list of protein sequences with index is the number of protein sequences\n",
        "# id2index is a dictionary of protein id and incremental index number \n",
        "for line in open(id2seq_file):\n",
        "    line = line.strip().split('\\t')\n",
        "    id2index[line[0]] = index\n",
        "    seqs.append(line[1])\n",
        "    index += 1\n",
        "\n",
        "seq_array = []\n",
        "id2_aid = {}\n",
        "sid = 0\n",
        "\n",
        "seq2t = s2t(use_emb)\n",
        "\n",
        "max_data = -1\n",
        "limit_data = max_data > 0\n",
        "raw_data = []\n",
        "skip_head = False\n",
        "x = None\n",
        "count = 0\n",
        "\n",
        "# Create sequence array as a list of protein strings\n",
        "for line in tqdm(open(ds_file)):\n",
        "    if skip_head:\n",
        "        skip_head = False\n",
        "        continue\n",
        "    line = line.rstrip('\\n').rstrip('\\r').split('\\t')\n",
        "    if id2index.get(line[sid1_index]) is None or id2index.get(line[sid2_index]) is None:\n",
        "        continue\n",
        "    if id2_aid.get(line[sid1_index]) is None:\n",
        "        id2_aid[line[sid1_index]] = sid\n",
        "        sid += 1\n",
        "        seq_array.append(seqs[id2index[line[sid1_index]]])\n",
        "    line[sid1_index] = id2_aid[line[sid1_index]]\n",
        "    if id2_aid.get(line[sid2_index]) is None:\n",
        "        id2_aid[line[sid2_index]] = sid\n",
        "        sid += 1\n",
        "        seq_array.append(seqs[id2index[line[sid2_index]]])\n",
        "    line[sid2_index] = id2_aid[line[sid2_index]]\n",
        "    raw_data.append(line)\n",
        "    if limit_data:\n",
        "        count += 1\n",
        "        if count >= max_data:\n",
        "            break\n",
        "\n",
        "len_m_seq = np.array([len(line.split()) for line in seq_array])\n",
        "avg_m_seq = int(np.average(len_m_seq)) + 1\n",
        "max_m_seq = max(len_m_seq)\n",
        "dim = seq2t.dim\n",
        "\n",
        "# seq_tensor is tensor representation of dataset having shape of (number_of_sequences, padding_length, embedding_dim_of_aa)\n",
        "# Random for distribution of class labels\n",
        "np.random.seed(42)\n",
        "np.random.shuffle(raw_data)\n",
        "seq_tensor = np.array([encode_seq(line) for line in tqdm(seq_array)])\n",
        "\n",
        "# Extract index of 1st and 2nd sequences in pairs\n",
        "seq_index1 = np.array([line[sid1_index] for line in tqdm(raw_data)])\n",
        "seq_index2 = np.array([line[sid2_index] for line in tqdm(raw_data)])\n",
        "\n",
        "# Assign labels for pairs of sequences\n",
        "class_map = {'0':1,'1':0}\n",
        "class_labels = np.zeros((len(raw_data,)))\n",
        "for i in range(len(raw_data)):\n",
        "  class_labels[i] = float(raw_data[i][label_index])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Tw_Ye-AIwTmB",
        "outputId": "52a7a523-f590-4347-ff54-cfc220f1c4ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================= INFER BY TRAINED NEURAL NETWORK ON TRAINING SET ==============================\n",
            "2051/2051 [==============================] - 4s 2ms/step\n",
            "============= INFERENCE BY NEURAL NETWORK ===============\n",
            "accuracy: 0.8273550117331545, precision: 0.17530266343825665, recall: 0.24270868253436137, specificity: 0.8858196446530339, f1-score: 0.2035709264726557, mcc: 0.11139476875022422, auroc: 0.6049634514006007, auprc: 0.13554239202396556 \n",
            "0.8273550117331545\t0.17530266343825665\t0.24270868253436137\t0.8858196446530339\t0.2035709264726557\t0.11139476875022422\t0.6049634514006007\t0.13554239202396556\n",
            "\n"
          ]
        }
      ],
      "source": [
        "##################################### Load Positive and Negative Dataset ##########################################################\n",
        "\n",
        "FEATURE_NUM = seq_tensor.shape[1]\n",
        "# Create data frame of pairs and labels\n",
        "df = np.hstack([seq_tensor[seq_index1], seq_tensor[seq_index2]])\n",
        "df = np.hstack([df, class_labels.reshape(-1,1)])\n",
        "df = pd.DataFrame(df)\n",
        "df = df.sample(frac=1)\n",
        "\n",
        "# Create pairs matrix and label vector\n",
        "X = df.iloc[:,0:FEATURE_NUM*2].values\n",
        "y = df.iloc[:,FEATURE_NUM*2:].values\n",
        "\n",
        "\n",
        "# standard scaler\n",
        "X = standard_scaler.transform(X)\n",
        "\n",
        "\n",
        "X1_train = X[:, :FEATURE_NUM]\n",
        "X2_train = X[:, FEATURE_NUM:]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"============================= INFER BY TRAINED NEURAL NETWORK ON TRAINING SET ==============================\")\n",
        "y_true = y\n",
        "y_pred = model.predict([X1_train, X2_train])\n",
        "cm1=confusion_matrix(y_true, np.round(y_pred))\n",
        "acc = (cm1[0,0]+cm1[1,1])/(cm1[0,0]+cm1[0,1]+cm1[1,0]+cm1[1,1])\n",
        "spec= (cm1[0,0])/(cm1[0,0]+cm1[0,1])\n",
        "sens = (cm1[1,1])/(cm1[1,0]+cm1[1,1])\n",
        "prec=cm1[1,1]/(cm1[1,1]+cm1[0,1])\n",
        "rec=cm1[1,1]/(cm1[1,1]+cm1[1,0])\n",
        "f1 = 2 * (prec * rec) / (prec + rec)\n",
        "mcc = matthews_corrcoef(y_true, np.round(y_pred))\n",
        "\n",
        "prc = metrics.average_precision_score(y_true, y_pred)\n",
        "\n",
        "print(\"============= INFERENCE BY NEURAL NETWORK ===============\")\n",
        "try:\n",
        "  auc = metrics.roc_auc_score(y_true, y_pred)\n",
        "  print(f'accuracy: {acc}, precision: {prec}, recall: {rec}, specificity: {spec}, f1-score: {f1}, mcc: {mcc}, auroc: {auc}, auprc: {prc} ')\n",
        "  print(str(acc) + \"\\t\" + str(prec) + \"\\t\" + str(rec) + \"\\t\" + str(spec) + \"\\t\" + str(f1) + \"\\t\" + str(mcc)+\"\\t\" + str(auc)  + \"\\t\" + str(prc) + \"\\n\")\n",
        "except ValueError:\n",
        "  print(f'accuracy: {acc}, precision: {prec}, recall: {rec}, specificity: {spec}, f1-score: {f1}, mcc: {mcc}, auroc: nan, auprc: {prc} ')\n",
        "  print(str(acc) + \"\\t\" + str(prec) + \"\\t\" + str(rec) + \"\\t\" + str(spec) + \"\\t\" + str(f1) + \"\\t\" + str(mcc)+\"\\t nan\"  + \"\\t\" + str(prc) + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pk-us5oTwTmC"
      },
      "source": [
        "#### Evaluate with trained FSNN-LGBM prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4K_3I9AZwTmC",
        "outputId": "eacfd168-3e4b-48d1-9ea0-1152b0c7f66e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2051/2051 [==============================] - 4s 2ms/step\n",
            "accuracy: 0.8057324840764332, precision: 0.16668304668304668, recall: 0.2842775729131747, specificity: 0.857877975192759, f1-score: 0.2101480701319621, mcc: 0.11290811657060225, auroc: 0.5710777740529668, auprc: 0.11244992714654128 \n",
            "0.8057324840764332\t0.16668304668304668\t0.2842775729131747\t0.857877975192759\t0.2101480701319621\t0.11290811657060225\t0.5710777740529668\t0.11244992714654128\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Read new data\n",
        "\n",
        "FEATURE_NUM = seq_tensor.shape[1]\n",
        "# Create data frame of pairs and labels\n",
        "df = np.hstack([seq_tensor[seq_index1], seq_tensor[seq_index2]])\n",
        "df = np.hstack([df, class_labels.reshape(-1,1)])\n",
        "df = pd.DataFrame(df)\n",
        "df = df.sample(frac=1)\n",
        "\n",
        "# Create pairs matrix and label vector\n",
        "X = df.iloc[:,0:FEATURE_NUM*2].values\n",
        "y = df.iloc[:,FEATURE_NUM*2:].values\n",
        "\n",
        "Trainlabels=y\n",
        "\n",
        "# standard scaler\n",
        "# scaler = StandardScaler().fit(X)\n",
        "# scaler = MinMaxScaler().fit(X)\n",
        "\n",
        "# scaler = RobustScaler().fit(X)\n",
        "X = standard_scaler.transform(X)\n",
        "\n",
        "\n",
        "X1_train = X[:, :FEATURE_NUM]\n",
        "X2_train = X[:, FEATURE_NUM:]\n",
        "\n",
        "\n",
        "# Predict representation from trained neural network\n",
        "\n",
        "intermediate_layer_model = Model(inputs=model.input,outputs=model.get_layer('Merged_feature_1').output)\n",
        "# Use intermediate layer to transform pairs matrix\n",
        "intermediate_output_p1 = intermediate_layer_model.predict([X1_train,X2_train])  \n",
        "p_merge=pd.DataFrame(intermediate_output_p1)    \n",
        "\n",
        "# create dataframe use transformed pairs matrix outputs and labels\n",
        "X_train_feat=pd.concat((p_merge,pd.DataFrame(pd.DataFrame(Trainlabels))),axis=1,ignore_index=True)\n",
        "\n",
        "# write to file dataframe of transformed pairs matrix and labels\n",
        "X_train_feat.to_csv('X_train.csv',header=False, index=False)\n",
        "\n",
        "# read dataframe of transformed pairs matrix and labels\n",
        "Train=pd.read_csv(\"X_train.csv\",header=None)\n",
        "Train=Train.sample(frac=1)\n",
        "\n",
        "X=Train.iloc[:,0:64].values\n",
        "y=Train.iloc[:,64:].values.ravel()\n",
        "\n",
        "extracted_df=X_train_feat\n",
        "\n",
        "X=robust_scaler.transform(X)\n",
        "\n",
        "# Predict probability from neural network output for new data\n",
        "y_true=y\n",
        "y_pred = model_.predict(X)\n",
        "cm1=confusion_matrix(y_true, np.round(y_pred))\n",
        "acc = (cm1[0,0]+cm1[1,1])/(cm1[0,0]+cm1[0,1]+cm1[1,0]+cm1[1,1])\n",
        "spec= (cm1[0,0])/(cm1[0,0]+cm1[0,1])\n",
        "sens = (cm1[1,1])/(cm1[1,0]+cm1[1,1])\n",
        "prec=cm1[1,1]/(cm1[1,1]+cm1[0,1])\n",
        "rec=cm1[1,1]/(cm1[1,1]+cm1[1,0])\n",
        "f1 = 2 * (prec * rec) / (prec + rec)\n",
        "mcc = matthews_corrcoef(y_true, np.round(y_pred))\n",
        "\n",
        "prc = metrics.average_precision_score(y_true, y_pred)\n",
        "\n",
        "try:\n",
        "  auc = metrics.roc_auc_score(y_true, y_pred)\n",
        "  print(f'accuracy: {acc}, precision: {prec}, recall: {rec}, specificity: {spec}, f1-score: {f1}, mcc: {mcc}, auroc: {auc}, auprc: {prc} ')\n",
        "  print(str(acc) + \"\\t\" + str(prec) + \"\\t\" + str(rec) + \"\\t\" + str(spec) + \"\\t\" + str(f1) + \"\\t\" + str(mcc)+\"\\t\" + str(auc)  + \"\\t\" + str(prc) + \"\\n\")\n",
        "except ValueError:\n",
        "  print(f'accuracy: {acc}, precision: {prec}, recall: {rec}, specificity: {spec}, f1-score: {f1}, mcc: {mcc}, auroc: nan, auprc: {prc} ')\n",
        "  print(str(acc) + \"\\t\" + str(prec) + \"\\t\" + str(rec) + \"\\t\" + str(spec) + \"\\t\" + str(f1) + \"\\t\" + str(mcc)+\"\\t nan\"  + \"\\t\" + str(prc) + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0evOeM27x0kp"
      },
      "source": [
        "### Influenza"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gq6rpyS6x0kq"
      },
      "source": [
        "#### Read dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "fYm7e9V7x0kq",
        "outputId": "ad819915-dd2f-4d32-ce0f-b4aaf78c8234"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-12-17 17:19:58--  https://raw.githubusercontent.com/anhvt00/MCAPS/master/data/Interspecies-host-pathogen/human-virus/Influenza/protein_pair_label.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 538680 (526K) [text/plain]\n",
            "Saving to: ‘protein_pair_label.txt’\n",
            "\n",
            "protein_pair_label. 100%[===================>] 526.05K  --.-KB/s    in 0.006s  \n",
            "\n",
            "2022-12-17 17:20:00 (83.9 MB/s) - ‘protein_pair_label.txt’ saved [538680/538680]\n",
            "\n",
            "--2022-12-17 17:20:00--  https://raw.githubusercontent.com/anhvt00/MCAPS/master/data/Interspecies-host-pathogen/human-virus/Influenza/pro_seq.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9114445 (8.7M) [text/plain]\n",
            "Saving to: ‘pro_seq.txt’\n",
            "\n",
            "pro_seq.txt         100%[===================>]   8.69M  --.-KB/s    in 0.02s   \n",
            "\n",
            "2022-12-17 17:20:01 (476 MB/s) - ‘pro_seq.txt’ saved [9114445/9114445]\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "33484it [00:00, 107638.33it/s]\n",
            "100%|██████████| 16377/16377 [00:50<00:00, 327.10it/s]\n",
            "100%|██████████| 33484/33484 [00:00<00:00, 1647240.47it/s]\n",
            "100%|██████████| 33484/33484 [00:00<00:00, 2333622.60it/s]\n"
          ]
        }
      ],
      "source": [
        "!rm -rf pro_seq.txt protein_pair_label.txt\n",
        "id2seq_file = 'pro_seq.txt'\n",
        "id2index = {}\n",
        "seqs = []\n",
        "index = 0\n",
        "sid1_index = 0\n",
        "sid2_index = 1\n",
        "ds_file = 'protein_pair_label.txt'\n",
        "label_index = 2\n",
        "use_emb = 'ac5_aph.txt'\n",
        "\n",
        "if not os.path.isfile(ds_file) or not os.path.isfile(id2seq_file):\n",
        "  !wget https://raw.githubusercontent.com/anhvt00/MCAPS/master/data/Interspecies-host-pathogen/human-virus/Influenza/protein_pair_label.txt\n",
        "  !wget https://raw.githubusercontent.com/anhvt00/MCAPS/master/data/Interspecies-host-pathogen/human-virus/Influenza/pro_seq.txt\n",
        "\n",
        "\n",
        "\n",
        "# Create line variable as a list of protein sequences with index is the number of protein sequences\n",
        "# id2index is a dictionary of protein id and incremental index number \n",
        "for line in open(id2seq_file):\n",
        "    line = line.strip().split('\\t')\n",
        "    id2index[line[0]] = index\n",
        "    seqs.append(line[1])\n",
        "    index += 1\n",
        "\n",
        "seq_array = []\n",
        "id2_aid = {}\n",
        "sid = 0\n",
        "\n",
        "seq2t = s2t(use_emb)\n",
        "\n",
        "max_data = -1\n",
        "limit_data = max_data > 0\n",
        "raw_data = []\n",
        "skip_head = False\n",
        "x = None\n",
        "count = 0\n",
        "\n",
        "# Create sequence array as a list of protein strings\n",
        "for line in tqdm(open(ds_file)):\n",
        "    if skip_head:\n",
        "        skip_head = False\n",
        "        continue\n",
        "    line = line.rstrip('\\n').rstrip('\\r').split('\\t')\n",
        "    if id2index.get(line[sid1_index]) is None or id2index.get(line[sid2_index]) is None:\n",
        "        continue\n",
        "    if id2_aid.get(line[sid1_index]) is None:\n",
        "        id2_aid[line[sid1_index]] = sid\n",
        "        sid += 1\n",
        "        seq_array.append(seqs[id2index[line[sid1_index]]])\n",
        "    line[sid1_index] = id2_aid[line[sid1_index]]\n",
        "    if id2_aid.get(line[sid2_index]) is None:\n",
        "        id2_aid[line[sid2_index]] = sid\n",
        "        sid += 1\n",
        "        seq_array.append(seqs[id2index[line[sid2_index]]])\n",
        "    line[sid2_index] = id2_aid[line[sid2_index]]\n",
        "    raw_data.append(line)\n",
        "    if limit_data:\n",
        "        count += 1\n",
        "        if count >= max_data:\n",
        "            break\n",
        "\n",
        "len_m_seq = np.array([len(line.split()) for line in seq_array])\n",
        "avg_m_seq = int(np.average(len_m_seq)) + 1\n",
        "max_m_seq = max(len_m_seq)\n",
        "dim = seq2t.dim\n",
        "\n",
        "# seq_tensor is tensor representation of dataset having shape of (number_of_sequences, padding_length, embedding_dim_of_aa)\n",
        "# Random for distribution of class labels\n",
        "np.random.seed(42)\n",
        "np.random.shuffle(raw_data)\n",
        "seq_tensor = np.array([encode_seq(line) for line in tqdm(seq_array)])\n",
        "\n",
        "# Extract index of 1st and 2nd sequences in pairs\n",
        "seq_index1 = np.array([line[sid1_index] for line in tqdm(raw_data)])\n",
        "seq_index2 = np.array([line[sid2_index] for line in tqdm(raw_data)])\n",
        "\n",
        "# Assign labels for pairs of sequences\n",
        "class_map = {'0':1,'1':0}\n",
        "class_labels = np.zeros((len(raw_data,)))\n",
        "for i in range(len(raw_data)):\n",
        "  class_labels[i] = float(raw_data[i][label_index])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "WkUEDypkx0kq",
        "outputId": "21fe2e47-1c19-42b1-e20f-2ff0b7e6f39c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================= INFER BY TRAINED NEURAL NETWORK ON TRAINING SET ==============================\n",
            "1047/1047 [==============================] - 2s 2ms/step\n",
            "============= INFERENCE BY NEURAL NETWORK ===============\n",
            "accuracy: 0.8231991398877075, precision: 0.1973905723905724, recall: 0.30814717477003944, specificity: 0.8747043363994744, f1-score: 0.24063622370446386, mcc: 0.15063366995450742, auroc: 0.6975917435648163, auprc: 0.18065373741448862 \n",
            "0.8231991398877075\t0.1973905723905724\t0.30814717477003944\t0.8747043363994744\t0.24063622370446386\t0.15063366995450742\t0.6975917435648163\t0.18065373741448862\n",
            "\n"
          ]
        }
      ],
      "source": [
        "##################################### Load Positive and Negative Dataset ##########################################################\n",
        "\n",
        "FEATURE_NUM = seq_tensor.shape[1]\n",
        "# Create data frame of pairs and labels\n",
        "df = np.hstack([seq_tensor[seq_index1], seq_tensor[seq_index2]])\n",
        "df = np.hstack([df, class_labels.reshape(-1,1)])\n",
        "df = pd.DataFrame(df)\n",
        "df = df.sample(frac=1)\n",
        "\n",
        "# Create pairs matrix and label vector\n",
        "X = df.iloc[:,0:FEATURE_NUM*2].values\n",
        "y = df.iloc[:,FEATURE_NUM*2:].values\n",
        "\n",
        "\n",
        "# standard scaler\n",
        "X = standard_scaler.transform(X)\n",
        "\n",
        "\n",
        "X1_train = X[:, :FEATURE_NUM]\n",
        "X2_train = X[:, FEATURE_NUM:]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"============================= INFER BY TRAINED NEURAL NETWORK ON TRAINING SET ==============================\")\n",
        "y_true = y\n",
        "y_pred = model.predict([X1_train, X2_train])\n",
        "cm1=confusion_matrix(y_true, np.round(y_pred))\n",
        "acc = (cm1[0,0]+cm1[1,1])/(cm1[0,0]+cm1[0,1]+cm1[1,0]+cm1[1,1])\n",
        "spec= (cm1[0,0])/(cm1[0,0]+cm1[0,1])\n",
        "sens = (cm1[1,1])/(cm1[1,0]+cm1[1,1])\n",
        "prec=cm1[1,1]/(cm1[1,1]+cm1[0,1])\n",
        "rec=cm1[1,1]/(cm1[1,1]+cm1[1,0])\n",
        "f1 = 2 * (prec * rec) / (prec + rec)\n",
        "mcc = matthews_corrcoef(y_true, np.round(y_pred))\n",
        "\n",
        "prc = metrics.average_precision_score(y_true, y_pred)\n",
        "\n",
        "print(\"============= INFERENCE BY NEURAL NETWORK ===============\")\n",
        "try:\n",
        "  auc = metrics.roc_auc_score(y_true, y_pred)\n",
        "  print(f'accuracy: {acc}, precision: {prec}, recall: {rec}, specificity: {spec}, f1-score: {f1}, mcc: {mcc}, auroc: {auc}, auprc: {prc} ')\n",
        "  print(str(acc) + \"\\t\" + str(prec) + \"\\t\" + str(rec) + \"\\t\" + str(spec) + \"\\t\" + str(f1) + \"\\t\" + str(mcc)+\"\\t\" + str(auc)  + \"\\t\" + str(prc) + \"\\n\")\n",
        "except ValueError:\n",
        "  print(f'accuracy: {acc}, precision: {prec}, recall: {rec}, specificity: {spec}, f1-score: {f1}, mcc: {mcc}, auroc: nan, auprc: {prc} ')\n",
        "  print(str(acc) + \"\\t\" + str(prec) + \"\\t\" + str(rec) + \"\\t\" + str(spec) + \"\\t\" + str(f1) + \"\\t\" + str(mcc)+\"\\t nan\"  + \"\\t\" + str(prc) + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BiegvmVHx0kq"
      },
      "source": [
        "#### Evaluate with trained FSNN-LGBM prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "IcATRbJTx0kq",
        "outputId": "2ccbc2a8-92f2-47fe-d641-9f9aa4783dd6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1047/1047 [==============================] - 2s 2ms/step\n",
            "accuracy: 0.8018755226376777, precision: 0.18364469510045822, recall: 0.34231274638633374, specificity: 0.8478318002628121, f1-score: 0.2390456526726313, mcc: 0.14570798832885112, auroc: 0.5950722733245729, auprc: 0.12265367026763385 \n",
            "0.8018755226376777\t0.18364469510045822\t0.34231274638633374\t0.8478318002628121\t0.2390456526726313\t0.14570798832885112\t0.5950722733245729\t0.12265367026763385\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Read new data\n",
        "\n",
        "FEATURE_NUM = seq_tensor.shape[1]\n",
        "# Create data frame of pairs and labels\n",
        "df = np.hstack([seq_tensor[seq_index1], seq_tensor[seq_index2]])\n",
        "df = np.hstack([df, class_labels.reshape(-1,1)])\n",
        "df = pd.DataFrame(df)\n",
        "df = df.sample(frac=1)\n",
        "\n",
        "# Create pairs matrix and label vector\n",
        "X = df.iloc[:,0:FEATURE_NUM*2].values\n",
        "y = df.iloc[:,FEATURE_NUM*2:].values\n",
        "\n",
        "Trainlabels=y\n",
        "\n",
        "# standard scaler\n",
        "# scaler = StandardScaler().fit(X)\n",
        "# scaler = MinMaxScaler().fit(X)\n",
        "\n",
        "# scaler = RobustScaler().fit(X)\n",
        "X = standard_scaler.transform(X)\n",
        "\n",
        "\n",
        "X1_train = X[:, :FEATURE_NUM]\n",
        "X2_train = X[:, FEATURE_NUM:]\n",
        "\n",
        "\n",
        "# Predict representation from trained neural network\n",
        "\n",
        "intermediate_layer_model = Model(inputs=model.input,outputs=model.get_layer('Merged_feature_1').output)\n",
        "# Use intermediate layer to transform pairs matrix\n",
        "intermediate_output_p1 = intermediate_layer_model.predict([X1_train,X2_train])  \n",
        "p_merge=pd.DataFrame(intermediate_output_p1)    \n",
        "\n",
        "# create dataframe use transformed pairs matrix outputs and labels\n",
        "X_train_feat=pd.concat((p_merge,pd.DataFrame(pd.DataFrame(Trainlabels))),axis=1,ignore_index=True)\n",
        "\n",
        "# write to file dataframe of transformed pairs matrix and labels\n",
        "X_train_feat.to_csv('X_train.csv',header=False, index=False)\n",
        "\n",
        "# read dataframe of transformed pairs matrix and labels\n",
        "Train=pd.read_csv(\"X_train.csv\",header=None)\n",
        "Train=Train.sample(frac=1)\n",
        "\n",
        "X=Train.iloc[:,0:64].values\n",
        "y=Train.iloc[:,64:].values.ravel()\n",
        "\n",
        "extracted_df=X_train_feat\n",
        "\n",
        "X=robust_scaler.transform(X)\n",
        "\n",
        "# Predict probability from neural network output for new data\n",
        "y_true=y\n",
        "y_pred = model_.predict(X)\n",
        "cm1=confusion_matrix(y_true, np.round(y_pred))\n",
        "acc = (cm1[0,0]+cm1[1,1])/(cm1[0,0]+cm1[0,1]+cm1[1,0]+cm1[1,1])\n",
        "spec= (cm1[0,0])/(cm1[0,0]+cm1[0,1])\n",
        "sens = (cm1[1,1])/(cm1[1,0]+cm1[1,1])\n",
        "prec=cm1[1,1]/(cm1[1,1]+cm1[0,1])\n",
        "rec=cm1[1,1]/(cm1[1,1]+cm1[1,0])\n",
        "f1 = 2 * (prec * rec) / (prec + rec)\n",
        "mcc = matthews_corrcoef(y_true, np.round(y_pred))\n",
        "\n",
        "prc = metrics.average_precision_score(y_true, y_pred)\n",
        "\n",
        "try:\n",
        "  auc = metrics.roc_auc_score(y_true, y_pred)\n",
        "  print(f'accuracy: {acc}, precision: {prec}, recall: {rec}, specificity: {spec}, f1-score: {f1}, mcc: {mcc}, auroc: {auc}, auprc: {prc} ')\n",
        "  print(str(acc) + \"\\t\" + str(prec) + \"\\t\" + str(rec) + \"\\t\" + str(spec) + \"\\t\" + str(f1) + \"\\t\" + str(mcc)+\"\\t\" + str(auc)  + \"\\t\" + str(prc) + \"\\n\")\n",
        "except ValueError:\n",
        "  print(f'accuracy: {acc}, precision: {prec}, recall: {rec}, specificity: {spec}, f1-score: {f1}, mcc: {mcc}, auroc: nan, auprc: {prc} ')\n",
        "  print(str(acc) + \"\\t\" + str(prec) + \"\\t\" + str(rec) + \"\\t\" + str(spec) + \"\\t\" + str(f1) + \"\\t\" + str(mcc)+\"\\t nan\"  + \"\\t\" + str(prc) + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZdhDISNx0xy"
      },
      "source": [
        "### Papilloma"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lknkxtCxx0x0"
      },
      "source": [
        "#### Read dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "WNGDT8hhx0x0",
        "outputId": "f75edb8c-b60b-48ba-f686-8b91065a8a85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-12-17 17:20:59--  https://raw.githubusercontent.com/anhvt00/MCAPS/master/data/Interspecies-host-pathogen/human-virus/Papilloma/protein_pair_label.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 909090 (888K) [text/plain]\n",
            "Saving to: ‘protein_pair_label.txt’\n",
            "\n",
            "protein_pair_label. 100%[===================>] 887.78K  --.-KB/s    in 0.007s  \n",
            "\n",
            "2022-12-17 17:20:59 (131 MB/s) - ‘protein_pair_label.txt’ saved [909090/909090]\n",
            "\n",
            "--2022-12-17 17:20:59--  https://raw.githubusercontent.com/anhvt00/MCAPS/master/data/Interspecies-host-pathogen/human-virus/Papilloma/pro_seq.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10618063 (10M) [text/plain]\n",
            "Saving to: ‘pro_seq.txt’\n",
            "\n",
            "pro_seq.txt         100%[===================>]  10.13M  --.-KB/s    in 0.02s   \n",
            "\n",
            "2022-12-17 17:21:00 (410 MB/s) - ‘pro_seq.txt’ saved [10618063/10618063]\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "56089it [00:00, 405119.86it/s]\n",
            "100%|██████████| 19087/19087 [00:58<00:00, 326.61it/s]\n",
            "100%|██████████| 56089/56089 [00:00<00:00, 1765987.94it/s]\n",
            "100%|██████████| 56089/56089 [00:00<00:00, 2189756.66it/s]\n"
          ]
        }
      ],
      "source": [
        "!rm -rf pro_seq.txt protein_pair_label.txt\n",
        "id2seq_file = 'pro_seq.txt'\n",
        "id2index = {}\n",
        "seqs = []\n",
        "index = 0\n",
        "sid1_index = 0\n",
        "sid2_index = 1\n",
        "ds_file = 'protein_pair_label.txt'\n",
        "label_index = 2\n",
        "use_emb = 'ac5_aph.txt'\n",
        "\n",
        "if not os.path.isfile(ds_file) or not os.path.isfile(id2seq_file):\n",
        "  !wget https://raw.githubusercontent.com/anhvt00/MCAPS/master/data/Interspecies-host-pathogen/human-virus/Papilloma/protein_pair_label.txt\n",
        "  !wget https://raw.githubusercontent.com/anhvt00/MCAPS/master/data/Interspecies-host-pathogen/human-virus/Papilloma/pro_seq.txt\n",
        "\n",
        "\n",
        "# Create line variable as a list of protein sequences with index is the number of protein sequences\n",
        "# id2index is a dictionary of protein id and incremental index number \n",
        "for line in open(id2seq_file):\n",
        "    line = line.strip().split('\\t')\n",
        "    id2index[line[0]] = index\n",
        "    seqs.append(line[1])\n",
        "    index += 1\n",
        "\n",
        "seq_array = []\n",
        "id2_aid = {}\n",
        "sid = 0\n",
        "\n",
        "seq2t = s2t(use_emb)\n",
        "\n",
        "max_data = -1\n",
        "limit_data = max_data > 0\n",
        "raw_data = []\n",
        "skip_head = False\n",
        "x = None\n",
        "count = 0\n",
        "\n",
        "# Create sequence array as a list of protein strings\n",
        "for line in tqdm(open(ds_file)):\n",
        "    if skip_head:\n",
        "        skip_head = False\n",
        "        continue\n",
        "    line = line.rstrip('\\n').rstrip('\\r').split('\\t')\n",
        "    if id2index.get(line[sid1_index]) is None or id2index.get(line[sid2_index]) is None:\n",
        "        continue\n",
        "    if id2_aid.get(line[sid1_index]) is None:\n",
        "        id2_aid[line[sid1_index]] = sid\n",
        "        sid += 1\n",
        "        seq_array.append(seqs[id2index[line[sid1_index]]])\n",
        "    line[sid1_index] = id2_aid[line[sid1_index]]\n",
        "    if id2_aid.get(line[sid2_index]) is None:\n",
        "        id2_aid[line[sid2_index]] = sid\n",
        "        sid += 1\n",
        "        seq_array.append(seqs[id2index[line[sid2_index]]])\n",
        "    line[sid2_index] = id2_aid[line[sid2_index]]\n",
        "    raw_data.append(line)\n",
        "    if limit_data:\n",
        "        count += 1\n",
        "        if count >= max_data:\n",
        "            break\n",
        "\n",
        "len_m_seq = np.array([len(line.split()) for line in seq_array])\n",
        "avg_m_seq = int(np.average(len_m_seq)) + 1\n",
        "max_m_seq = max(len_m_seq)\n",
        "dim = seq2t.dim\n",
        "\n",
        "# seq_tensor is tensor representation of dataset having shape of (number_of_sequences, padding_length, embedding_dim_of_aa)\n",
        "# Random for distribution of class labels\n",
        "np.random.seed(42)\n",
        "np.random.shuffle(raw_data)\n",
        "seq_tensor = np.array([encode_seq(line) for line in tqdm(seq_array)])\n",
        "\n",
        "# Extract index of 1st and 2nd sequences in pairs\n",
        "seq_index1 = np.array([line[sid1_index] for line in tqdm(raw_data)])\n",
        "seq_index2 = np.array([line[sid2_index] for line in tqdm(raw_data)])\n",
        "\n",
        "# Assign labels for pairs of sequences\n",
        "class_map = {'0':1,'1':0}\n",
        "class_labels = np.zeros((len(raw_data,)))\n",
        "for i in range(len(raw_data)):\n",
        "  class_labels[i] = float(raw_data[i][label_index])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_A0-6ieSx0x2",
        "outputId": "22a2a19e-c545-4f37-fd53-3214c5f9ee08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================= INFER BY TRAINED NEURAL NETWORK ON TRAINING SET ==============================\n",
            "1753/1753 [==============================] - 3s 2ms/step\n",
            "============= INFERENCE BY NEURAL NETWORK ===============\n",
            "accuracy: 0.8297348856281981, precision: 0.1771362251559553, recall: 0.23945871739556776, specificity: 0.888762502451461, f1-score: 0.20363575717144763, mcc: 0.11227314300611148, auroc: 0.6091413853513725, auprc: 0.1438046173009216 \n",
            "0.8297348856281981\t0.1771362251559553\t0.23945871739556776\t0.888762502451461\t0.20363575717144763\t0.11227314300611148\t0.6091413853513725\t0.1438046173009216\n",
            "\n"
          ]
        }
      ],
      "source": [
        "##################################### Load Positive and Negative Dataset ##########################################################\n",
        "\n",
        "FEATURE_NUM = seq_tensor.shape[1]\n",
        "# Create data frame of pairs and labels\n",
        "df = np.hstack([seq_tensor[seq_index1], seq_tensor[seq_index2]])\n",
        "df = np.hstack([df, class_labels.reshape(-1,1)])\n",
        "df = pd.DataFrame(df)\n",
        "df = df.sample(frac=1)\n",
        "\n",
        "# Create pairs matrix and label vector\n",
        "X = df.iloc[:,0:FEATURE_NUM*2].values\n",
        "y = df.iloc[:,FEATURE_NUM*2:].values\n",
        "\n",
        "\n",
        "# standard scaler\n",
        "X = standard_scaler.transform(X)\n",
        "\n",
        "\n",
        "X1_train = X[:, :FEATURE_NUM]\n",
        "X2_train = X[:, FEATURE_NUM:]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"============================= INFER BY TRAINED NEURAL NETWORK ON TRAINING SET ==============================\")\n",
        "y_true = y\n",
        "y_pred = model.predict([X1_train, X2_train])\n",
        "cm1=confusion_matrix(y_true, np.round(y_pred))\n",
        "acc = (cm1[0,0]+cm1[1,1])/(cm1[0,0]+cm1[0,1]+cm1[1,0]+cm1[1,1])\n",
        "spec= (cm1[0,0])/(cm1[0,0]+cm1[0,1])\n",
        "sens = (cm1[1,1])/(cm1[1,0]+cm1[1,1])\n",
        "prec=cm1[1,1]/(cm1[1,1]+cm1[0,1])\n",
        "rec=cm1[1,1]/(cm1[1,1]+cm1[1,0])\n",
        "f1 = 2 * (prec * rec) / (prec + rec)\n",
        "mcc = matthews_corrcoef(y_true, np.round(y_pred))\n",
        "\n",
        "prc = metrics.average_precision_score(y_true, y_pred)\n",
        "\n",
        "print(\"============= INFERENCE BY NEURAL NETWORK ===============\")\n",
        "try:\n",
        "  auc = metrics.roc_auc_score(y_true, y_pred)\n",
        "  print(f'accuracy: {acc}, precision: {prec}, recall: {rec}, specificity: {spec}, f1-score: {f1}, mcc: {mcc}, auroc: {auc}, auprc: {prc} ')\n",
        "  print(str(acc) + \"\\t\" + str(prec) + \"\\t\" + str(rec) + \"\\t\" + str(spec) + \"\\t\" + str(f1) + \"\\t\" + str(mcc)+\"\\t\" + str(auc)  + \"\\t\" + str(prc) + \"\\n\")\n",
        "except ValueError:\n",
        "  print(f'accuracy: {acc}, precision: {prec}, recall: {rec}, specificity: {spec}, f1-score: {f1}, mcc: {mcc}, auroc: nan, auprc: {prc} ')\n",
        "  print(str(acc) + \"\\t\" + str(prec) + \"\\t\" + str(rec) + \"\\t\" + str(spec) + \"\\t\" + str(f1) + \"\\t\" + str(mcc)+\"\\t nan\"  + \"\\t\" + str(prc) + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQuMJ79Ix0x3"
      },
      "source": [
        "#### Evaluate with trained FSNN-LGBM prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "V7MyzPbRx0x3",
        "outputId": "92c9fc28-9d87-4bab-d514-91ea1b388346"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1753/1753 [==============================] - 3s 2ms/step\n",
            "accuracy: 0.8071101285457042, precision: 0.16775092936802974, recall: 0.28319278289860755, specificity: 0.8595018631104138, f1-score: 0.21069526519296708, mcc: 0.1138103169189368, auroc: 0.5713473230045106, auprc: 0.11267014498532305 \n",
            "0.8071101285457042\t0.16775092936802974\t0.28319278289860755\t0.8595018631104138\t0.21069526519296708\t0.1138103169189368\t0.5713473230045106\t0.11267014498532305\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Read new data\n",
        "\n",
        "FEATURE_NUM = seq_tensor.shape[1]\n",
        "# Create data frame of pairs and labels\n",
        "df = np.hstack([seq_tensor[seq_index1], seq_tensor[seq_index2]])\n",
        "df = np.hstack([df, class_labels.reshape(-1,1)])\n",
        "df = pd.DataFrame(df)\n",
        "df = df.sample(frac=1)\n",
        "\n",
        "# Create pairs matrix and label vector\n",
        "X = df.iloc[:,0:FEATURE_NUM*2].values\n",
        "y = df.iloc[:,FEATURE_NUM*2:].values\n",
        "\n",
        "Trainlabels=y\n",
        "\n",
        "# standard scaler\n",
        "# scaler = StandardScaler().fit(X)\n",
        "# scaler = MinMaxScaler().fit(X)\n",
        "\n",
        "# scaler = RobustScaler().fit(X)\n",
        "X = standard_scaler.transform(X)\n",
        "\n",
        "\n",
        "X1_train = X[:, :FEATURE_NUM]\n",
        "X2_train = X[:, FEATURE_NUM:]\n",
        "\n",
        "\n",
        "# Predict representation from trained neural network\n",
        "\n",
        "intermediate_layer_model = Model(inputs=model.input,outputs=model.get_layer('Merged_feature_1').output)\n",
        "# Use intermediate layer to transform pairs matrix\n",
        "intermediate_output_p1 = intermediate_layer_model.predict([X1_train,X2_train])  \n",
        "p_merge=pd.DataFrame(intermediate_output_p1)    \n",
        "\n",
        "# create dataframe use transformed pairs matrix outputs and labels\n",
        "X_train_feat=pd.concat((p_merge,pd.DataFrame(pd.DataFrame(Trainlabels))),axis=1,ignore_index=True)\n",
        "\n",
        "# write to file dataframe of transformed pairs matrix and labels\n",
        "X_train_feat.to_csv('X_train.csv',header=False, index=False)\n",
        "\n",
        "# read dataframe of transformed pairs matrix and labels\n",
        "Train=pd.read_csv(\"X_train.csv\",header=None)\n",
        "Train=Train.sample(frac=1)\n",
        "\n",
        "X=Train.iloc[:,0:64].values\n",
        "y=Train.iloc[:,64:].values.ravel()\n",
        "\n",
        "extracted_df=X_train_feat\n",
        "\n",
        "X=robust_scaler.transform(X)\n",
        "\n",
        "# Predict probability from neural network output for new data\n",
        "y_true=y\n",
        "y_pred = model_.predict(X)\n",
        "cm1=confusion_matrix(y_true, np.round(y_pred))\n",
        "acc = (cm1[0,0]+cm1[1,1])/(cm1[0,0]+cm1[0,1]+cm1[1,0]+cm1[1,1])\n",
        "spec= (cm1[0,0])/(cm1[0,0]+cm1[0,1])\n",
        "sens = (cm1[1,1])/(cm1[1,0]+cm1[1,1])\n",
        "prec=cm1[1,1]/(cm1[1,1]+cm1[0,1])\n",
        "rec=cm1[1,1]/(cm1[1,1]+cm1[1,0])\n",
        "f1 = 2 * (prec * rec) / (prec + rec)\n",
        "mcc = matthews_corrcoef(y_true, np.round(y_pred))\n",
        "\n",
        "prc = metrics.average_precision_score(y_true, y_pred)\n",
        "\n",
        "try:\n",
        "  auc = metrics.roc_auc_score(y_true, y_pred)\n",
        "  print(f'accuracy: {acc}, precision: {prec}, recall: {rec}, specificity: {spec}, f1-score: {f1}, mcc: {mcc}, auroc: {auc}, auprc: {prc} ')\n",
        "  print(str(acc) + \"\\t\" + str(prec) + \"\\t\" + str(rec) + \"\\t\" + str(spec) + \"\\t\" + str(f1) + \"\\t\" + str(mcc)+\"\\t\" + str(auc)  + \"\\t\" + str(prc) + \"\\n\")\n",
        "except ValueError:\n",
        "  print(f'accuracy: {acc}, precision: {prec}, recall: {rec}, specificity: {spec}, f1-score: {f1}, mcc: {mcc}, auroc: nan, auprc: {prc} ')\n",
        "  print(str(acc) + \"\\t\" + str(prec) + \"\\t\" + str(rec) + \"\\t\" + str(spec) + \"\\t\" + str(f1) + \"\\t\" + str(mcc)+\"\\t nan\"  + \"\\t\" + str(prc) + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bogx2mMFx091"
      },
      "source": [
        "### SARS2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0NXFuuwx09_"
      },
      "source": [
        "#### Read dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "jaOmx24Cx0-B",
        "outputId": "3054ea32-e815-4358-a6d8-c7b5fab5406d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-12-17 17:22:12--  https://raw.githubusercontent.com/anhvt00/MCAPS/master/data/Interspecies-host-pathogen/human-virus/SARS2/protein_pair_label.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 157705 (154K) [text/plain]\n",
            "Saving to: ‘protein_pair_label.txt’\n",
            "\n",
            "protein_pair_label. 100%[===================>] 154.01K  --.-KB/s    in 0.002s  \n",
            "\n",
            "2022-12-17 17:22:12 (68.3 MB/s) - ‘protein_pair_label.txt’ saved [157705/157705]\n",
            "\n",
            "--2022-12-17 17:22:12--  https://raw.githubusercontent.com/anhvt00/MCAPS/master/data/Interspecies-host-pathogen/human-virus/SARS2/pro_seq.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3054179 (2.9M) [text/plain]\n",
            "Saving to: ‘pro_seq.txt’\n",
            "\n",
            "pro_seq.txt         100%[===================>]   2.91M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2022-12-17 17:22:13 (262 MB/s) - ‘pro_seq.txt’ saved [3054179/3054179]\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "6248it [00:00, 317694.83it/s]\n",
            "100%|██████████| 5360/5360 [00:16<00:00, 317.77it/s]\n",
            "100%|██████████| 6248/6248 [00:00<00:00, 1773432.46it/s]\n",
            "100%|██████████| 6248/6248 [00:00<00:00, 2193522.34it/s]\n"
          ]
        }
      ],
      "source": [
        "!rm -rf pro_seq.txt protein_pair_label.txt\n",
        "id2seq_file = 'pro_seq.txt'\n",
        "id2index = {}\n",
        "seqs = []\n",
        "index = 0\n",
        "sid1_index = 0\n",
        "sid2_index = 1\n",
        "ds_file = 'protein_pair_label.txt'\n",
        "label_index = 2\n",
        "use_emb = 'ac5_aph.txt'\n",
        "\n",
        "if not os.path.isfile(ds_file) or not os.path.isfile(id2seq_file):\n",
        "  !wget https://raw.githubusercontent.com/anhvt00/MCAPS/master/data/Interspecies-host-pathogen/human-virus/SARS2/protein_pair_label.txt\n",
        "  !wget https://raw.githubusercontent.com/anhvt00/MCAPS/master/data/Interspecies-host-pathogen/human-virus/SARS2/pro_seq.txt\n",
        "\n",
        "\n",
        "\n",
        "# Create line variable as a list of protein sequences with index is the number of protein sequences\n",
        "# id2index is a dictionary of protein id and incremental index number \n",
        "for line in open(id2seq_file):\n",
        "    line = line.strip().split('\\t')\n",
        "    id2index[line[0]] = index\n",
        "    seqs.append(line[1])\n",
        "    index += 1\n",
        "\n",
        "seq_array = []\n",
        "id2_aid = {}\n",
        "sid = 0\n",
        "\n",
        "seq2t = s2t(use_emb)\n",
        "\n",
        "max_data = -1\n",
        "limit_data = max_data > 0\n",
        "raw_data = []\n",
        "skip_head = False\n",
        "x = None\n",
        "count = 0\n",
        "\n",
        "# Create sequence array as a list of protein strings\n",
        "for line in tqdm(open(ds_file)):\n",
        "    if skip_head:\n",
        "        skip_head = False\n",
        "        continue\n",
        "    line = line.rstrip('\\n').rstrip('\\r').split('\\t')\n",
        "    if id2index.get(line[sid1_index]) is None or id2index.get(line[sid2_index]) is None:\n",
        "        continue\n",
        "    if id2_aid.get(line[sid1_index]) is None:\n",
        "        id2_aid[line[sid1_index]] = sid\n",
        "        sid += 1\n",
        "        seq_array.append(seqs[id2index[line[sid1_index]]])\n",
        "    line[sid1_index] = id2_aid[line[sid1_index]]\n",
        "    if id2_aid.get(line[sid2_index]) is None:\n",
        "        id2_aid[line[sid2_index]] = sid\n",
        "        sid += 1\n",
        "        seq_array.append(seqs[id2index[line[sid2_index]]])\n",
        "    line[sid2_index] = id2_aid[line[sid2_index]]\n",
        "    raw_data.append(line)\n",
        "    if limit_data:\n",
        "        count += 1\n",
        "        if count >= max_data:\n",
        "            break\n",
        "\n",
        "len_m_seq = np.array([len(line.split()) for line in seq_array])\n",
        "avg_m_seq = int(np.average(len_m_seq)) + 1\n",
        "max_m_seq = max(len_m_seq)\n",
        "dim = seq2t.dim\n",
        "\n",
        "# seq_tensor is tensor representation of dataset having shape of (number_of_sequences, padding_length, embedding_dim_of_aa)\n",
        "# Random for distribution of class labels\n",
        "np.random.seed(42)\n",
        "np.random.shuffle(raw_data)\n",
        "seq_tensor = np.array([encode_seq(line) for line in tqdm(seq_array)])\n",
        "\n",
        "# Extract index of 1st and 2nd sequences in pairs\n",
        "seq_index1 = np.array([line[sid1_index] for line in tqdm(raw_data)])\n",
        "seq_index2 = np.array([line[sid2_index] for line in tqdm(raw_data)])\n",
        "\n",
        "# Assign labels for pairs of sequences\n",
        "class_map = {'0':1,'1':0}\n",
        "class_labels = np.zeros((len(raw_data,)))\n",
        "for i in range(len(raw_data)):\n",
        "  class_labels[i] = float(raw_data[i][label_index])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "QPmd1faDx0-H",
        "outputId": "640fd979-4dee-44ac-c845-df9a4810ee85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================= INFER BY TRAINED NEURAL NETWORK ON TRAINING SET ==============================\n",
            "196/196 [==============================] - 0s 2ms/step\n",
            "============= INFERENCE BY NEURAL NETWORK ===============\n",
            "accuracy: 0.8165813060179258, precision: 0.14841849148418493, recall: 0.2147887323943662, specificity: 0.8767605633802817, f1-score: 0.17553956834532378, mcc: 0.07786235590333669, auroc: 0.6135826844872049, auprc: 0.13196592046680622 \n",
            "0.8165813060179258\t0.14841849148418493\t0.2147887323943662\t0.8767605633802817\t0.17553956834532378\t0.07786235590333669\t0.6135826844872049\t0.13196592046680622\n",
            "\n"
          ]
        }
      ],
      "source": [
        "##################################### Load Positive and Negative Dataset ##########################################################\n",
        "\n",
        "FEATURE_NUM = seq_tensor.shape[1]\n",
        "# Create data frame of pairs and labels\n",
        "df = np.hstack([seq_tensor[seq_index1], seq_tensor[seq_index2]])\n",
        "df = np.hstack([df, class_labels.reshape(-1,1)])\n",
        "df = pd.DataFrame(df)\n",
        "df = df.sample(frac=1)\n",
        "\n",
        "# Create pairs matrix and label vector\n",
        "X = df.iloc[:,0:FEATURE_NUM*2].values\n",
        "y = df.iloc[:,FEATURE_NUM*2:].values\n",
        "\n",
        "\n",
        "# standard scaler\n",
        "X = standard_scaler.transform(X)\n",
        "\n",
        "\n",
        "X1_train = X[:, :FEATURE_NUM]\n",
        "X2_train = X[:, FEATURE_NUM:]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"============================= INFER BY TRAINED NEURAL NETWORK ON TRAINING SET ==============================\")\n",
        "y_true = y\n",
        "y_pred = model.predict([X1_train, X2_train])\n",
        "cm1=confusion_matrix(y_true, np.round(y_pred))\n",
        "acc = (cm1[0,0]+cm1[1,1])/(cm1[0,0]+cm1[0,1]+cm1[1,0]+cm1[1,1])\n",
        "spec= (cm1[0,0])/(cm1[0,0]+cm1[0,1])\n",
        "sens = (cm1[1,1])/(cm1[1,0]+cm1[1,1])\n",
        "prec=cm1[1,1]/(cm1[1,1]+cm1[0,1])\n",
        "rec=cm1[1,1]/(cm1[1,1]+cm1[1,0])\n",
        "f1 = 2 * (prec * rec) / (prec + rec)\n",
        "mcc = matthews_corrcoef(y_true, np.round(y_pred))\n",
        "\n",
        "prc = metrics.average_precision_score(y_true, y_pred)\n",
        "\n",
        "print(\"============= INFERENCE BY NEURAL NETWORK ===============\")\n",
        "try:\n",
        "  auc = metrics.roc_auc_score(y_true, y_pred)\n",
        "  print(f'accuracy: {acc}, precision: {prec}, recall: {rec}, specificity: {spec}, f1-score: {f1}, mcc: {mcc}, auroc: {auc}, auprc: {prc} ')\n",
        "  print(str(acc) + \"\\t\" + str(prec) + \"\\t\" + str(rec) + \"\\t\" + str(spec) + \"\\t\" + str(f1) + \"\\t\" + str(mcc)+\"\\t\" + str(auc)  + \"\\t\" + str(prc) + \"\\n\")\n",
        "except ValueError:\n",
        "  print(f'accuracy: {acc}, precision: {prec}, recall: {rec}, specificity: {spec}, f1-score: {f1}, mcc: {mcc}, auroc: nan, auprc: {prc} ')\n",
        "  print(str(acc) + \"\\t\" + str(prec) + \"\\t\" + str(rec) + \"\\t\" + str(spec) + \"\\t\" + str(f1) + \"\\t\" + str(mcc)+\"\\t nan\"  + \"\\t\" + str(prc) + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJfAGvtzx0-H"
      },
      "source": [
        "#### Evaluate with trained FSNN-LGBM prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JEAUimhKx0-H",
        "outputId": "a780240c-1f42-4b77-a310-a444976ceead"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 0s 2ms/step\n",
            "accuracy: 0.7924135723431498, precision: 0.1518624641833811, recall: 0.27992957746478875, specificity: 0.8436619718309859, f1-score: 0.19690402476780186, mcc: 0.09513062376667293, auroc: 0.5617957746478873, auprc: 0.10797174293481651 \n",
            "0.7924135723431498\t0.1518624641833811\t0.27992957746478875\t0.8436619718309859\t0.19690402476780186\t0.09513062376667293\t0.5617957746478873\t0.10797174293481651\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Read new data\n",
        "\n",
        "FEATURE_NUM = seq_tensor.shape[1]\n",
        "# Create data frame of pairs and labels\n",
        "df = np.hstack([seq_tensor[seq_index1], seq_tensor[seq_index2]])\n",
        "df = np.hstack([df, class_labels.reshape(-1,1)])\n",
        "df = pd.DataFrame(df)\n",
        "df = df.sample(frac=1)\n",
        "\n",
        "# Create pairs matrix and label vector\n",
        "X = df.iloc[:,0:FEATURE_NUM*2].values\n",
        "y = df.iloc[:,FEATURE_NUM*2:].values\n",
        "\n",
        "Trainlabels=y\n",
        "\n",
        "# standard scaler\n",
        "# scaler = StandardScaler().fit(X)\n",
        "# scaler = MinMaxScaler().fit(X)\n",
        "\n",
        "# scaler = RobustScaler().fit(X)\n",
        "X = standard_scaler.transform(X)\n",
        "\n",
        "\n",
        "X1_train = X[:, :FEATURE_NUM]\n",
        "X2_train = X[:, FEATURE_NUM:]\n",
        "\n",
        "\n",
        "# Predict representation from trained neural network\n",
        "\n",
        "intermediate_layer_model = Model(inputs=model.input,outputs=model.get_layer('Merged_feature_1').output)\n",
        "# Use intermediate layer to transform pairs matrix\n",
        "intermediate_output_p1 = intermediate_layer_model.predict([X1_train,X2_train])  \n",
        "p_merge=pd.DataFrame(intermediate_output_p1)    \n",
        "\n",
        "# create dataframe use transformed pairs matrix outputs and labels\n",
        "X_train_feat=pd.concat((p_merge,pd.DataFrame(pd.DataFrame(Trainlabels))),axis=1,ignore_index=True)\n",
        "\n",
        "# write to file dataframe of transformed pairs matrix and labels\n",
        "X_train_feat.to_csv('X_train.csv',header=False, index=False)\n",
        "\n",
        "# read dataframe of transformed pairs matrix and labels\n",
        "Train=pd.read_csv(\"X_train.csv\",header=None)\n",
        "Train=Train.sample(frac=1)\n",
        "\n",
        "X=Train.iloc[:,0:64].values\n",
        "y=Train.iloc[:,64:].values.ravel()\n",
        "\n",
        "extracted_df=X_train_feat\n",
        "\n",
        "X=robust_scaler.transform(X)\n",
        "\n",
        "# Predict probability from neural network output for new data\n",
        "y_true=y\n",
        "y_pred = model_.predict(X)\n",
        "cm1=confusion_matrix(y_true, np.round(y_pred))\n",
        "acc = (cm1[0,0]+cm1[1,1])/(cm1[0,0]+cm1[0,1]+cm1[1,0]+cm1[1,1])\n",
        "spec= (cm1[0,0])/(cm1[0,0]+cm1[0,1])\n",
        "sens = (cm1[1,1])/(cm1[1,0]+cm1[1,1])\n",
        "prec=cm1[1,1]/(cm1[1,1]+cm1[0,1])\n",
        "rec=cm1[1,1]/(cm1[1,1]+cm1[1,0])\n",
        "f1 = 2 * (prec * rec) / (prec + rec)\n",
        "mcc = matthews_corrcoef(y_true, np.round(y_pred))\n",
        "\n",
        "prc = metrics.average_precision_score(y_true, y_pred)\n",
        "\n",
        "try:\n",
        "  auc = metrics.roc_auc_score(y_true, y_pred)\n",
        "  print(f'accuracy: {acc}, precision: {prec}, recall: {rec}, specificity: {spec}, f1-score: {f1}, mcc: {mcc}, auroc: {auc}, auprc: {prc} ')\n",
        "  print(str(acc) + \"\\t\" + str(prec) + \"\\t\" + str(rec) + \"\\t\" + str(spec) + \"\\t\" + str(f1) + \"\\t\" + str(mcc)+\"\\t\" + str(auc)  + \"\\t\" + str(prc) + \"\\n\")\n",
        "except ValueError:\n",
        "  print(f'accuracy: {acc}, precision: {prec}, recall: {rec}, specificity: {spec}, f1-score: {f1}, mcc: {mcc}, auroc: nan, auprc: {prc} ')\n",
        "  print(str(acc) + \"\\t\" + str(prec) + \"\\t\" + str(rec) + \"\\t\" + str(spec) + \"\\t\" + str(f1) + \"\\t\" + str(mcc)+\"\\t nan\"  + \"\\t\" + str(prc) + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGeozh65x1Mt"
      },
      "source": [
        "### ZIKV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXjl0qccx1Mt"
      },
      "source": [
        "#### Read dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-8919Kkzx1Mt",
        "outputId": "4e77232a-2718-4215-f6cb-f7d969e3ff05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-12-17 17:22:32--  https://raw.githubusercontent.com/anhvt00/MCAPS/master/data/Interspecies-host-pathogen/human-virus/ZIKV/protein_pair_label.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 257373 (251K) [text/plain]\n",
            "Saving to: ‘protein_pair_label.txt’\n",
            "\n",
            "protein_pair_label. 100%[===================>] 251.34K  --.-KB/s    in 0.003s  \n",
            "\n",
            "2022-12-17 17:22:32 (76.5 MB/s) - ‘protein_pair_label.txt’ saved [257373/257373]\n",
            "\n",
            "--2022-12-17 17:22:32--  https://raw.githubusercontent.com/anhvt00/MCAPS/master/data/Interspecies-host-pathogen/human-virus/ZIKV/pro_seq.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3654310 (3.5M) [text/plain]\n",
            "Saving to: ‘pro_seq.txt’\n",
            "\n",
            "pro_seq.txt         100%[===================>]   3.48M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2022-12-17 17:22:33 (313 MB/s) - ‘pro_seq.txt’ saved [3654310/3654310]\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "7799it [00:00, 339904.37it/s]\n",
            "100%|██████████| 6480/6480 [00:20<00:00, 321.92it/s]\n",
            "100%|██████████| 7799/7799 [00:00<00:00, 1810559.41it/s]\n",
            "100%|██████████| 7799/7799 [00:00<00:00, 2315685.75it/s]\n"
          ]
        }
      ],
      "source": [
        "!rm -rf pro_seq.txt protein_pair_label.txt\n",
        "id2seq_file = 'pro_seq.txt'\n",
        "id2index = {}\n",
        "seqs = []\n",
        "index = 0\n",
        "sid1_index = 0\n",
        "sid2_index = 1\n",
        "ds_file = 'protein_pair_label.txt'\n",
        "label_index = 2\n",
        "use_emb = 'ac5_aph.txt'\n",
        "\n",
        "if not os.path.isfile(ds_file) or not os.path.isfile(id2seq_file):\n",
        "  !wget https://raw.githubusercontent.com/anhvt00/MCAPS/master/data/Interspecies-host-pathogen/human-virus/ZIKV/protein_pair_label.txt\n",
        "  !wget https://raw.githubusercontent.com/anhvt00/MCAPS/master/data/Interspecies-host-pathogen/human-virus/ZIKV/pro_seq.txt\n",
        "\n",
        "\n",
        "\n",
        "# Create line variable as a list of protein sequences with index is the number of protein sequences\n",
        "# id2index is a dictionary of protein id and incremental index number \n",
        "for line in open(id2seq_file):\n",
        "    line = line.strip().split('\\t')\n",
        "    id2index[line[0]] = index\n",
        "    seqs.append(line[1])\n",
        "    index += 1\n",
        "\n",
        "seq_array = []\n",
        "id2_aid = {}\n",
        "sid = 0\n",
        "\n",
        "seq2t = s2t(use_emb)\n",
        "\n",
        "max_data = -1\n",
        "limit_data = max_data > 0\n",
        "raw_data = []\n",
        "skip_head = False\n",
        "x = None\n",
        "count = 0\n",
        "\n",
        "# Create sequence array as a list of protein strings\n",
        "for line in tqdm(open(ds_file)):\n",
        "    if skip_head:\n",
        "        skip_head = False\n",
        "        continue\n",
        "    line = line.rstrip('\\n').rstrip('\\r').split('\\t')\n",
        "    if id2index.get(line[sid1_index]) is None or id2index.get(line[sid2_index]) is None:\n",
        "        continue\n",
        "    if id2_aid.get(line[sid1_index]) is None:\n",
        "        id2_aid[line[sid1_index]] = sid\n",
        "        sid += 1\n",
        "        seq_array.append(seqs[id2index[line[sid1_index]]])\n",
        "    line[sid1_index] = id2_aid[line[sid1_index]]\n",
        "    if id2_aid.get(line[sid2_index]) is None:\n",
        "        id2_aid[line[sid2_index]] = sid\n",
        "        sid += 1\n",
        "        seq_array.append(seqs[id2index[line[sid2_index]]])\n",
        "    line[sid2_index] = id2_aid[line[sid2_index]]\n",
        "    raw_data.append(line)\n",
        "    if limit_data:\n",
        "        count += 1\n",
        "        if count >= max_data:\n",
        "            break\n",
        "\n",
        "len_m_seq = np.array([len(line.split()) for line in seq_array])\n",
        "avg_m_seq = int(np.average(len_m_seq)) + 1\n",
        "max_m_seq = max(len_m_seq)\n",
        "dim = seq2t.dim\n",
        "\n",
        "# seq_tensor is tensor representation of dataset having shape of (number_of_sequences, padding_length, embedding_dim_of_aa)\n",
        "# Random for distribution of class labels\n",
        "np.random.seed(42)\n",
        "np.random.shuffle(raw_data)\n",
        "seq_tensor = np.array([encode_seq(line) for line in tqdm(seq_array)])\n",
        "\n",
        "# Extract index of 1st and 2nd sequences in pairs\n",
        "seq_index1 = np.array([line[sid1_index] for line in tqdm(raw_data)])\n",
        "seq_index2 = np.array([line[sid2_index] for line in tqdm(raw_data)])\n",
        "\n",
        "# Assign labels for pairs of sequences\n",
        "class_map = {'0':1,'1':0}\n",
        "class_labels = np.zeros((len(raw_data,)))\n",
        "for i in range(len(raw_data)):\n",
        "  class_labels[i] = float(raw_data[i][label_index])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "BfCzpOhBx1Mu",
        "outputId": "1f806f82-90d9-4ca2-ea91-83fecb244715"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================= INFER BY TRAINED NEURAL NETWORK ON TRAINING SET ==============================\n",
            "244/244 [==============================] - 0s 2ms/step\n",
            "============= INFERENCE BY NEURAL NETWORK ===============\n",
            "accuracy: 0.7997179125528914, precision: 0.15797914995990378, recall: 0.2778561354019746, specificity: 0.8519040902679831, f1-score: 0.2014314928425358, mcc: 0.10178122093588193, auroc: 0.6079505690487605, auprc: 0.13478663232539323 \n",
            "0.7997179125528914\t0.15797914995990378\t0.2778561354019746\t0.8519040902679831\t0.2014314928425358\t0.10178122093588193\t0.6079505690487605\t0.13478663232539323\n",
            "\n"
          ]
        }
      ],
      "source": [
        "##################################### Load Positive and Negative Dataset ##########################################################\n",
        "\n",
        "FEATURE_NUM = seq_tensor.shape[1]\n",
        "# Create data frame of pairs and labels\n",
        "df = np.hstack([seq_tensor[seq_index1], seq_tensor[seq_index2]])\n",
        "df = np.hstack([df, class_labels.reshape(-1,1)])\n",
        "df = pd.DataFrame(df)\n",
        "df = df.sample(frac=1)\n",
        "\n",
        "# Create pairs matrix and label vector\n",
        "X = df.iloc[:,0:FEATURE_NUM*2].values\n",
        "y = df.iloc[:,FEATURE_NUM*2:].values\n",
        "\n",
        "\n",
        "# standard scaler\n",
        "X = standard_scaler.transform(X)\n",
        "\n",
        "\n",
        "X1_train = X[:, :FEATURE_NUM]\n",
        "X2_train = X[:, FEATURE_NUM:]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"============================= INFER BY TRAINED NEURAL NETWORK ON TRAINING SET ==============================\")\n",
        "y_true = y\n",
        "y_pred = model.predict([X1_train, X2_train])\n",
        "cm1=confusion_matrix(y_true, np.round(y_pred))\n",
        "acc = (cm1[0,0]+cm1[1,1])/(cm1[0,0]+cm1[0,1]+cm1[1,0]+cm1[1,1])\n",
        "spec= (cm1[0,0])/(cm1[0,0]+cm1[0,1])\n",
        "sens = (cm1[1,1])/(cm1[1,0]+cm1[1,1])\n",
        "prec=cm1[1,1]/(cm1[1,1]+cm1[0,1])\n",
        "rec=cm1[1,1]/(cm1[1,1]+cm1[1,0])\n",
        "f1 = 2 * (prec * rec) / (prec + rec)\n",
        "mcc = matthews_corrcoef(y_true, np.round(y_pred))\n",
        "\n",
        "prc = metrics.average_precision_score(y_true, y_pred)\n",
        "\n",
        "print(\"============= INFERENCE BY NEURAL NETWORK ===============\")\n",
        "try:\n",
        "  auc = metrics.roc_auc_score(y_true, y_pred)\n",
        "  print(f'accuracy: {acc}, precision: {prec}, recall: {rec}, specificity: {spec}, f1-score: {f1}, mcc: {mcc}, auroc: {auc}, auprc: {prc} ')\n",
        "  print(str(acc) + \"\\t\" + str(prec) + \"\\t\" + str(rec) + \"\\t\" + str(spec) + \"\\t\" + str(f1) + \"\\t\" + str(mcc)+\"\\t\" + str(auc)  + \"\\t\" + str(prc) + \"\\n\")\n",
        "except ValueError:\n",
        "  print(f'accuracy: {acc}, precision: {prec}, recall: {rec}, specificity: {spec}, f1-score: {f1}, mcc: {mcc}, auroc: nan, auprc: {prc} ')\n",
        "  print(str(acc) + \"\\t\" + str(prec) + \"\\t\" + str(rec) + \"\\t\" + str(spec) + \"\\t\" + str(f1) + \"\\t\" + str(mcc)+\"\\t nan\"  + \"\\t\" + str(prc) + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ym0ZSKdx1Mu"
      },
      "source": [
        "#### Evaluate with trained FSNN-LGBM prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "i2U6Diddx1Mu",
        "outputId": "487e0c99-3c54-45e9-dea0-9d00bce1f973"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "244/244 [==============================] - 1s 2ms/step\n",
            "accuracy: 0.7827926657263752, precision: 0.1499644633972992, recall: 0.29760225669957685, specificity: 0.831311706629055, f1-score: 0.1994328922495274, mcc: 0.09637863338435428, auroc: 0.5644569816643159, auprc: 0.10848410303181581 \n",
            "0.7827926657263752\t0.1499644633972992\t0.29760225669957685\t0.831311706629055\t0.1994328922495274\t0.09637863338435428\t0.5644569816643159\t0.10848410303181581\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Read new data\n",
        "\n",
        "FEATURE_NUM = seq_tensor.shape[1]\n",
        "# Create data frame of pairs and labels\n",
        "df = np.hstack([seq_tensor[seq_index1], seq_tensor[seq_index2]])\n",
        "df = np.hstack([df, class_labels.reshape(-1,1)])\n",
        "df = pd.DataFrame(df)\n",
        "df = df.sample(frac=1)\n",
        "\n",
        "# Create pairs matrix and label vector\n",
        "X = df.iloc[:,0:FEATURE_NUM*2].values\n",
        "y = df.iloc[:,FEATURE_NUM*2:].values\n",
        "\n",
        "Trainlabels=y\n",
        "\n",
        "# standard scaler\n",
        "# scaler = StandardScaler().fit(X)\n",
        "# scaler = MinMaxScaler().fit(X)\n",
        "\n",
        "# scaler = RobustScaler().fit(X)\n",
        "X = standard_scaler.transform(X)\n",
        "\n",
        "\n",
        "X1_train = X[:, :FEATURE_NUM]\n",
        "X2_train = X[:, FEATURE_NUM:]\n",
        "\n",
        "\n",
        "# Predict representation from trained neural network\n",
        "\n",
        "intermediate_layer_model = Model(inputs=model.input,outputs=model.get_layer('Merged_feature_1').output)\n",
        "# Use intermediate layer to transform pairs matrix\n",
        "intermediate_output_p1 = intermediate_layer_model.predict([X1_train,X2_train])  \n",
        "p_merge=pd.DataFrame(intermediate_output_p1)    \n",
        "\n",
        "# create dataframe use transformed pairs matrix outputs and labels\n",
        "X_train_feat=pd.concat((p_merge,pd.DataFrame(pd.DataFrame(Trainlabels))),axis=1,ignore_index=True)\n",
        "\n",
        "# write to file dataframe of transformed pairs matrix and labels\n",
        "X_train_feat.to_csv('X_train.csv',header=False, index=False)\n",
        "\n",
        "# read dataframe of transformed pairs matrix and labels\n",
        "Train=pd.read_csv(\"X_train.csv\",header=None)\n",
        "Train=Train.sample(frac=1)\n",
        "\n",
        "X=Train.iloc[:,0:64].values\n",
        "y=Train.iloc[:,64:].values.ravel()\n",
        "\n",
        "extracted_df=X_train_feat\n",
        "\n",
        "X=robust_scaler.transform(X)\n",
        "\n",
        "# Predict probability from neural network output for new data\n",
        "y_true=y\n",
        "y_pred = model_.predict(X)\n",
        "cm1=confusion_matrix(y_true, np.round(y_pred))\n",
        "acc = (cm1[0,0]+cm1[1,1])/(cm1[0,0]+cm1[0,1]+cm1[1,0]+cm1[1,1])\n",
        "spec= (cm1[0,0])/(cm1[0,0]+cm1[0,1])\n",
        "sens = (cm1[1,1])/(cm1[1,0]+cm1[1,1])\n",
        "prec=cm1[1,1]/(cm1[1,1]+cm1[0,1])\n",
        "rec=cm1[1,1]/(cm1[1,1]+cm1[1,0])\n",
        "f1 = 2 * (prec * rec) / (prec + rec)\n",
        "mcc = matthews_corrcoef(y_true, np.round(y_pred))\n",
        "\n",
        "prc = metrics.average_precision_score(y_true, y_pred)\n",
        "\n",
        "try:\n",
        "  auc = metrics.roc_auc_score(y_true, y_pred)\n",
        "  print(f'accuracy: {acc}, precision: {prec}, recall: {rec}, specificity: {spec}, f1-score: {f1}, mcc: {mcc}, auroc: {auc}, auprc: {prc} ')\n",
        "  print(str(acc) + \"\\t\" + str(prec) + \"\\t\" + str(rec) + \"\\t\" + str(spec) + \"\\t\" + str(f1) + \"\\t\" + str(mcc)+\"\\t\" + str(auc)  + \"\\t\" + str(prc) + \"\\n\")\n",
        "except ValueError:\n",
        "  print(f'accuracy: {acc}, precision: {prec}, recall: {rec}, specificity: {spec}, f1-score: {f1}, mcc: {mcc}, auroc: nan, auprc: {prc} ')\n",
        "  print(str(acc) + \"\\t\" + str(prec) + \"\\t\" + str(rec) + \"\\t\" + str(spec) + \"\\t\" + str(f1) + \"\\t\" + str(mcc)+\"\\t nan\"  + \"\\t\" + str(prc) + \"\\n\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}