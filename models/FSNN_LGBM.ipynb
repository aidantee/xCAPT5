{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anhvt00/PIPR/blob/master/models/FSNN_LGBM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HH-e5IC_hGa"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "id": "S-FZrex4_i2z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d23713e0-d753-4af3-d9ec-5914050b5690"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.7/dist-packages (2.2.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from lightgbm) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from lightgbm) (1.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from lightgbm) (1.4.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->lightgbm) (3.0.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->lightgbm) (1.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install lightgbm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Input, Dropout\n",
        "from tensorflow.keras.layers import concatenate\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_curve\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import sklearn.metrics as metrics\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import matthews_corrcoef,accuracy_score, precision_score,recall_score\n",
        "from sklearn.manifold import TSNE\n",
        "import tensorflow_transform as tft\n",
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "import lightgbm as lgb\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DoWrUQG82w2"
      },
      "source": [
        "### Sample seq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "44Ulg_0s846i"
      },
      "outputs": [],
      "source": [
        "seq = \"MKFVYKEEHPFEKRRSEGEKIRKKYPDRVPVIVEKAPKARIGDLDKKKYLVPSDLTVGQFYFLIRKRIHLRAEDALFFFVNNVIPPTSATMGQLYQEHHEEDFFLYIAYSDESVYGL\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kiTiIFTn8Uyz"
      },
      "source": [
        "### Conjoint Triad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "doTa8fDt_B3C"
      },
      "outputs": [],
      "source": [
        "# # https://sci-hub.se/https://doi.org/10.3390/ijms18112373\n",
        "# # After that, the conjoint triad method is introduced to extract the sequence information, which includes the properties of one amino acid and its vicinal amino acids and regards any \n",
        "# # three continuous amino acids as a unit. Firstly, they replaced each amino acid in the protein sequence by the index depending on its grouping. For instance, protein sequence\n",
        "# # “VCCPPVCVVCPPVCVPVPPCCV” is replaced by 0112201001220102022110. Then, binary space (V, F) stands for a protein sequence. Here, V is the vector sp… acids grouped into seven classes, the size V \n",
        "# # should be 7 × 7 × 7; therefore, i = 0, 1, · · · , 342. The detailed definition and description is shown in Figure 4. Clearly, each protein has a corresponding F vector. Nevertheless, the value\n",
        "# # of fi relates to the length of amino acid sequence. A longer amino acid sequence generally have a larger value of fi, which complicates the comparison between two heterogeneous proteins. \n",
        "# # As such they employed the normalization to solve this problem as follows: di = (fi - min fi) / max fi for i = 1 to 343, where the value of di is normalized in the range [0, 1]. fi is the\n",
        "# # frequency of conjoint triad unit vi appearing in the protein sequence. Finally, they connected the vector spaces of two proteins to present the interaction features. Thus, a 686-dimensional \n",
        "# # vector (343 for each protein) is generated for each pair of proteins.\n",
        "\n",
        "# # original paper: https://www.pnas.org/content/104/11/4337 (shen et al., 2007)\n",
        "# ct_encoding = {\n",
        "#     \"A\": 0,\n",
        "#     \"G\": 0,\n",
        "#     \"V\": 0,\n",
        "#     \"C\": 1,\n",
        "#     \"F\": 2,\n",
        "#     \"I\": 2,\n",
        "#     \"L\": 2,\n",
        "#     \"P\": 2,\n",
        "#     \"M\": 3,\n",
        "#     \"S\": 3,\n",
        "#     \"T\": 3,\n",
        "#     \"Y\": 3,\n",
        "#     \"H\": 4,\n",
        "#     \"N\": 4,\n",
        "#     \"Q\": 4,\n",
        "#     \"W\": 4,\n",
        "#     \"K\": 5,\n",
        "#     \"R\": 5,\n",
        "#     \"D\": 6,\n",
        "#     \"E\": 6,\n",
        "# }\n",
        "\n",
        "# f = open('encode_ct.txt', 'w')\n",
        "# for aa, value in ct_encoding.items():\n",
        "#   f.write(str(aa)+'\\t'+str(value)+'\\n')\n",
        "\n",
        "# f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1QbOrTYesLt",
        "outputId": "bafc344a-a88d-4b5e-f02c-de49376fde72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4 0.125\n",
            "13 0.125\n",
            "23 0.125\n",
            "71 0.125\n",
            "89 0.125\n",
            "149 0.125\n",
            "158 0.125\n",
            "276 0.125\n"
          ]
        }
      ],
      "source": [
        "\"\"\"Implementation of CT coding method\n",
        "\"\"\"\n",
        "\n",
        "__all__ = ['ct_code_of']\n",
        "\n",
        "# AAC: Classification of amino acids.\n",
        "AAC = {\n",
        "    '1': ['A', 'G', 'V'],\n",
        "    '2': ['I', 'L', 'F', 'P'],\n",
        "    '3': ['Y', 'M', 'T', 'S'],\n",
        "    '4': ['H', 'N', 'Q', 'W'],\n",
        "    '5': ['R', 'K'],\n",
        "    '6': ['D', 'E'],\n",
        "    '7': ['C']\n",
        "}\n",
        "\n",
        "# AAC_R: Reverse of AAC.\n",
        "AAC_R = {}\n",
        "for C, AAS in AAC.items():\n",
        "    for AA in AAS:\n",
        "        AAC_R[AA] = C\n",
        "\n",
        "def classification_of(AA):\n",
        "    \"\"\"Get classification of amino acids.\"\"\"\n",
        "    return AAC_R[AA]\n",
        "\n",
        "def classification_sequence_of(PS):\n",
        "    \"\"\"Make classification sequence from protein sequence.\"\"\"\n",
        "    CS = ''\n",
        "    for I, CH in enumerate(PS):\n",
        "        CS = CS + classification_of(CH)\n",
        "    return CS\n",
        "\n",
        "def ct_code_of(PS):\n",
        "    \"\"\"Get CT Code of protein sequence.\"\"\"\n",
        "    CT_Code = [0]*343\n",
        "    CS = classification_sequence_of(PS)\n",
        "    for I in range(len(CS)-2):\n",
        "        SubCS = CS[I:I+3]\n",
        "        CT_Code_Index = int(SubCS[0]) + (int(SubCS[1])-1)*7 + (int(SubCS[2])-1)*7*7\n",
        "        CT_Code[CT_Code_Index-1] = CT_Code[CT_Code_Index-1] + 1\n",
        "    SUM = sum(CT_Code)\n",
        "    CT_Code = [N*1.0/SUM for N in CT_Code]\n",
        "    # Normalizing CT_Code\n",
        "    # MIN_CODE = min(CT_Code)\n",
        "    # MAX_CODE = max(CT_Code)\n",
        "    # CT_Code = [(N-MIN_CODE)*1.0/(MAX_CODE-MIN_CODE) for N in CT_Code]\n",
        "    return CT_Code\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "    for I, Frequency in enumerate(ct_code_of('MREIVHIQAG')):\n",
        "        if Frequency>0 :\n",
        "            print(I+1, Frequency)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gCFza7fKfJAC",
        "outputId": "6327ffc0-2825-4d91-8238-364243a54f27"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.0,\n",
              " 0.0,\n",
              " 0.008695652173913044,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.017391304347826087,\n",
              " 0.008695652173913044,\n",
              " 0.017391304347826087,\n",
              " 0.0,\n",
              " 0.017391304347826087,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.008695652173913044,\n",
              " 0.008695652173913044,\n",
              " 0.017391304347826087,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.008695652173913044,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.008695652173913044,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.017391304347826087,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.017391304347826087,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.008695652173913044,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.008695652173913044,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.017391304347826087,\n",
              " 0.008695652173913044,\n",
              " 0.008695652173913044,\n",
              " 0.017391304347826087,\n",
              " 0.008695652173913044,\n",
              " 0.0,\n",
              " 0.017391304347826087,\n",
              " 0.043478260869565216,\n",
              " 0.008695652173913044,\n",
              " 0.008695652173913044,\n",
              " 0.0,\n",
              " 0.008695652173913044,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.017391304347826087,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.017391304347826087,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.017391304347826087,\n",
              " 0.008695652173913044,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.008695652173913044,\n",
              " 0.0,\n",
              " 0.008695652173913044,\n",
              " 0.0,\n",
              " 0.008695652173913044,\n",
              " 0.0,\n",
              " 0.008695652173913044,\n",
              " 0.008695652173913044,\n",
              " 0.0,\n",
              " 0.008695652173913044,\n",
              " 0.0,\n",
              " 0.008695652173913044,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.008695652173913044,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.017391304347826087,\n",
              " 0.017391304347826087,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.008695652173913044,\n",
              " 0.017391304347826087,\n",
              " 0.0,\n",
              " 0.017391304347826087,\n",
              " 0.0,\n",
              " 0.008695652173913044,\n",
              " 0.0,\n",
              " 0.017391304347826087,\n",
              " 0.008695652173913044,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.02608695652173913,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.008695652173913044,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.008695652173913044,\n",
              " 0.008695652173913044,\n",
              " 0.008695652173913044,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.008695652173913044,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.008695652173913044,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.008695652173913044,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.008695652173913044,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.008695652173913044,\n",
              " 0.0,\n",
              " 0.008695652173913044,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.008695652173913044,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.008695652173913044,\n",
              " 0.008695652173913044,\n",
              " 0.0,\n",
              " 0.008695652173913044,\n",
              " 0.008695652173913044,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.008695652173913044,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.017391304347826087,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.034782608695652174,\n",
              " 0.017391304347826087,\n",
              " 0.0,\n",
              " 0.017391304347826087,\n",
              " 0.02608695652173913,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.017391304347826087,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.008695652173913044,\n",
              " 0.008695652173913044,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.008695652173913044,\n",
              " 0.008695652173913044,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.008695652173913044,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.008695652173913044,\n",
              " 0.008695652173913044,\n",
              " 0.0,\n",
              " 0.008695652173913044,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.008695652173913044,\n",
              " 0.008695652173913044,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.008695652173913044,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.008695652173913044,\n",
              " 0.0,\n",
              " 0.008695652173913044,\n",
              " 0.008695652173913044,\n",
              " 0.008695652173913044,\n",
              " 0.008695652173913044,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0,\n",
              " 0.0]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# 343-dimensional conjoint triad vector \n",
        "ct_code_of(seq)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEzG9WEi8g88"
      },
      "source": [
        "### Local Descriptor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39B9qBKQLWxk",
        "outputId": "503d7841-a086-4a44-98d9-5604284b2673"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PS= VCCPPVCVVCPPVCVPVPPCCV\n",
            "CS= 1772217117221712122771\n",
            "LD_INFO= (22, {'1': [1, 6, 8, 9, 13, 15, 17, 22], '7': [2, 3, 7, 10, 14, 20, 21], '2': [4, 5, 11, 12, 16, 18, 19]}, {'17': 7, '27': 3, '12': 5})\n",
            "LD_CODE= [0.36363636363636365, 0.3181818181818182, 0, 0, 0, 0, 0.3181818181818182, 0.23809523809523808, 0, 0, 0, 0, 0.3333333333333333, 0, 0, 0, 0, 0.14285714285714285, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.045454545454545456, 0.2727272727272727, 0.4090909090909091, 0.6818181818181818, 1.0, 0.18181818181818182, 0.18181818181818182, 0.5, 0.7272727272727273, 0.8636363636363636, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.09090909090909091, 0.09090909090909091, 0.3181818181818182, 0.6363636363636364, 0.9545454545454546]\n",
            "LD_CODE= [0.2, 0.4, 0, 0, 0, 0, 0.4, 0, 0, 0, 0, 0, 0.25, 0, 0, 0, 0, 0.25, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.2, 0.2, 0.2, 0.2, 0.2, 0.8, 1.0, 0.8, 0.8, 1.0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.4, 0.6, 0.4, 0.4, 0.6, 0.5, 0.16666666666666666, 0, 0, 0, 0, 0.3333333333333333, 0, 0, 0, 0, 0, 0.6, 0, 0, 0, 0, 0.2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.16666666666666666, 0.6666666666666666, 0.16666666666666666, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.3333333333333333, 0.8333333333333334, 0.3333333333333333, 0.3333333333333333, 0.8333333333333334, 0.4, 0.4, 0, 0, 0, 0, 0.2, 0.5, 0, 0, 0, 0, 0.5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.4, 0.8, 0.4, 0.4, 0.8, 0.2, 1.0, 0.2, 0.2, 1.0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.6, 0.6, 0.6, 0.6, 0.6, 0.3333333333333333, 0.3333333333333333, 0, 0, 0, 0, 0.3333333333333333, 0.2, 0, 0, 0, 0, 0.2, 0, 0, 0, 0, 0.2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.16666666666666666, 1.0, 0.16666666666666666, 0.16666666666666666, 1.0, 0.3333333333333333, 0.5, 0.3333333333333333, 0.3333333333333333, 0.5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.6666666666666666, 0.8333333333333334, 0.6666666666666666, 0.6666666666666666, 0.8333333333333334, 0.36363636363636365, 0.2727272727272727, 0, 0, 0, 0, 0.36363636363636365, 0.1, 0, 0, 0, 0, 0.4, 0, 0, 0, 0, 0.2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.09090909090909091, 0.09090909090909091, 0.5454545454545454, 0.7272727272727273, 0.8181818181818182, 0.36363636363636365, 1.0, 0.36363636363636365, 0.45454545454545453, 1.0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.18181818181818182, 0.18181818181818182, 0.2727272727272727, 0.6363636363636364, 0.9090909090909091, 0.36363636363636365, 0.36363636363636365, 0, 0, 0, 0, 0.2727272727272727, 0.4, 0, 0, 0, 0, 0.3, 0, 0, 0, 0, 0.1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.18181818181818182, 0.18181818181818182, 0.36363636363636365, 0.5454545454545454, 1.0, 0.09090909090909091, 0.09090909090909091, 0.45454545454545453, 0.6363636363636364, 0.7272727272727273, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.2727272727272727, 0.9090909090909091, 0.2727272727272727, 0.8181818181818182, 0.9090909090909091, 0.45454545454545453, 0.2727272727272727, 0, 0, 0, 0, 0.2727272727272727, 0.2, 0, 0, 0, 0, 0.5, 0, 0, 0, 0, 0.1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.09090909090909091, 0.09090909090909091, 0.2727272727272727, 0.36363636363636365, 0.9090909090909091, 0.5454545454545454, 1.0, 0.5454545454545454, 0.6363636363636364, 1.0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.18181818181818182, 0.8181818181818182, 0.18181818181818182, 0.45454545454545453, 0.8181818181818182, 0.375, 0.3125, 0, 0, 0, 0, 0.3125, 0.2, 0, 0, 0, 0, 0.4, 0, 0, 0, 0, 0.13333333333333333, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0625, 0.0625, 0.5, 0.5625, 0.9375, 0.25, 0.25, 0.3125, 0.6875, 1.0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.125, 0.125, 0.1875, 0.4375, 0.875, 0.4117647058823529, 0.29411764705882354, 0, 0, 0, 0, 0.29411764705882354, 0.25, 0, 0, 0, 0, 0.375, 0, 0, 0, 0, 0.125, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.058823529411764705, 0.058823529411764705, 0.23529411764705882, 0.5882352941176471, 1.0, 0.35294117647058826, 0.35294117647058826, 0.4117647058823529, 0.6470588235294118, 0.8235294117647058, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.11764705882352941, 0.11764705882352941, 0.29411764705882354, 0.5294117647058824, 0.9411764705882353, 0.35294117647058826, 0.4117647058823529, 0, 0, 0, 0, 0.23529411764705882, 0.3125, 0, 0, 0, 0, 0.3125, 0, 0, 0, 0, 0.125, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.23529411764705882, 0.23529411764705882, 0.4117647058823529, 0.6470588235294118, 0.8823529411764706, 0.11764705882352941, 0.11764705882352941, 0.5294117647058824, 0.8235294117647058, 1.0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.058823529411764705, 0.058823529411764705, 0.29411764705882354, 0.47058823529411764, 0.7058823529411765]\n"
          ]
        }
      ],
      "source": [
        "\"\"\"Implementation of LD coding method\n",
        "\"\"\"\n",
        "\n",
        "__all__ = ['ld_code_of']\n",
        "\n",
        "# AAC: Classification of amino acids.\n",
        "AAC = {\n",
        "    '1': ['A', 'G', 'V'],\n",
        "    '2': ['I', 'L', 'F', 'P'],\n",
        "    '3': ['Y', 'M', 'T', 'S'],\n",
        "    '4': ['H', 'N', 'Q', 'W'],\n",
        "    '5': ['R', 'K'],\n",
        "    '6': ['D', 'E'],\n",
        "    '7': ['C']\n",
        "}\n",
        "\n",
        "# AAC_R: Reverse of AAC.\n",
        "AAC_R = {}\n",
        "for C, AAS in AAC.items():\n",
        "    for AA in AAS:\n",
        "        AAC_R[AA] = C\n",
        "\n",
        "def classification_of(AA):\n",
        "    \"\"\"Get classification of amino acids.\"\"\"\n",
        "    return AAC_R[AA]\n",
        "\n",
        "def classification_sequence_of(PS):\n",
        "    \"\"\"Make classification sequence from protein sequence.\"\"\"\n",
        "    CS = ''\n",
        "    for I, CH in enumerate(PS):\n",
        "        CS = CS + classification_of(CH)\n",
        "    return CS\n",
        "\n",
        "def ld_info_of(CS):\n",
        "    L = len(CS)\n",
        "    C = {}\n",
        "    T = {}\n",
        "    for I, CH in enumerate(CS):\n",
        "        if CH not in C:\n",
        "            C[CH] = []\n",
        "        C[CH].append(I+1)\n",
        "        if I > 0:\n",
        "            PCH = CS[I-1]\n",
        "            if PCH != CH:\n",
        "                if int(PCH)<int(CH):\n",
        "                    TIndex = PCH + CH\n",
        "                else:\n",
        "                    TIndex = CH + PCH\n",
        "                if TIndex not in T:\n",
        "                    T[TIndex] = 0\n",
        "                T[TIndex] = T[TIndex]+1\n",
        "    return L, C, T\n",
        "\n",
        "def ld_code_of_0(CS):\n",
        "    RC = [0]*7\n",
        "    RT = [0]*21\n",
        "    RD = [0]*35\n",
        "    L, C, T = ld_info_of(CS)\n",
        "    for Class, Indexs in C.items():\n",
        "        Len = len(Indexs)\n",
        "        RC[int(Class)-1]=Len*1.0/L\n",
        "        Residues = [1, int(Len*0.25), int(Len*0.5), int(Len*0.75), Len]\n",
        "        # Residues = list(map(lambda x:x*1.0/L, Residues))\n",
        "        Residues = list(map(lambda x:Indexs[x-1]*1.0/L, Residues))\n",
        "        RD[(int(Class)-1)*5:int(Class)*5] = Residues\n",
        "    for Trans, Frequency in T.items():\n",
        "        PI, I = int(Trans[0])-1, int(Trans[1])-1\n",
        "        Index = int((21-(6-PI)*(6-PI+1)/2)+(I-PI-1))\n",
        "        RT[Index] = Frequency*1.0/(L-1)\n",
        "    # return RC, RT, RD\n",
        "    return RC+RT+RD\n",
        "\n",
        "def ld_code_of(PS):\n",
        "    \"\"\"Get LD Code of protein sequence.\"\"\"\n",
        "    CS = classification_sequence_of(PS)\n",
        "    L = len(CS)\n",
        "    A = ld_code_of_0(CS[          0:int(L*0.25)])\n",
        "    B = ld_code_of_0(CS[int(L*0.25):int(L*0.50)])\n",
        "    C = ld_code_of_0(CS[int(L*0.50):int(L*0.75)])\n",
        "    D = ld_code_of_0(CS[int(L*0.75):L          ])\n",
        "    E = ld_code_of_0(CS[          0:int(L*0.50)])\n",
        "    F = ld_code_of_0(CS[int(L*0.50):L          ])\n",
        "    G = ld_code_of_0(CS[int(L*0.25):int(L*0.75)])\n",
        "    H = ld_code_of_0(CS[          0:int(L*0.75)])\n",
        "    I = ld_code_of_0(CS[int(L*0.25):L          ])\n",
        "    J = ld_code_of_0(CS[int(L*0.125):int(L*0.875)])\n",
        "    return A+B+C+D+E+F+G+H+I+J\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "    PS = 'VCCPPVCVVCPPVCVPVPPCCV'\n",
        "    print('PS=', PS)\n",
        "    CS = classification_sequence_of(PS)\n",
        "    print('CS=', CS)\n",
        "    LD_INFO = ld_info_of(CS)\n",
        "    print('LD_INFO=', LD_INFO)\n",
        "    LD_CODE = ld_code_of_0(CS)\n",
        "    print('LD_CODE=', LD_CODE)\n",
        "    LD_CODE = ld_code_of(PS)\n",
        "    print('LD_CODE=', LD_CODE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ImrE95uFfSDe",
        "outputId": "15ea3569-9704-4f2e-fb6a-eb6e4bb89c65"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.10344827586206896,\n",
              " 0.1724137931034483,\n",
              " 0.13793103448275862,\n",
              " 0.034482758620689655,\n",
              " 0.3448275862068966,\n",
              " 0.20689655172413793,\n",
              " 0,\n",
              " 0.03571428571428571,\n",
              " 0.03571428571428571,\n",
              " 0,\n",
              " 0.03571428571428571,\n",
              " 0.07142857142857142,\n",
              " 0,\n",
              " 0.03571428571428571,\n",
              " 0.03571428571428571,\n",
              " 0.10714285714285714,\n",
              " 0.07142857142857142,\n",
              " 0,\n",
              " 0,\n",
              " 0.14285714285714285,\n",
              " 0.03571428571428571,\n",
              " 0,\n",
              " 0,\n",
              " 0.03571428571428571,\n",
              " 0,\n",
              " 0.14285714285714285,\n",
              " 0,\n",
              " 0,\n",
              " 0.13793103448275862,\n",
              " 1.0,\n",
              " 0.13793103448275862,\n",
              " 0.6206896551724138,\n",
              " 1.0,\n",
              " 0.10344827586206896,\n",
              " 0.10344827586206896,\n",
              " 0.3448275862068966,\n",
              " 0.3793103448275862,\n",
              " 0.896551724137931,\n",
              " 0.034482758620689655,\n",
              " 0.034482758620689655,\n",
              " 0.1724137931034483,\n",
              " 0.5517241379310345,\n",
              " 0.8620689655172413,\n",
              " 0.3103448275862069,\n",
              " 0.3103448275862069,\n",
              " 0.3103448275862069,\n",
              " 0.3103448275862069,\n",
              " 0.3103448275862069,\n",
              " 0.06896551724137931,\n",
              " 0.20689655172413793,\n",
              " 0.5172413793103449,\n",
              " 0.7586206896551724,\n",
              " 0.9655172413793104,\n",
              " 0.2413793103448276,\n",
              " 0.2413793103448276,\n",
              " 0.41379310344827586,\n",
              " 0.5862068965517241,\n",
              " 0.9310344827586207,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0.27586206896551724,\n",
              " 0.27586206896551724,\n",
              " 0.10344827586206896,\n",
              " 0,\n",
              " 0.20689655172413793,\n",
              " 0.13793103448275862,\n",
              " 0,\n",
              " 0.25,\n",
              " 0.03571428571428571,\n",
              " 0,\n",
              " 0.10714285714285714,\n",
              " 0.07142857142857142,\n",
              " 0,\n",
              " 0.10714285714285714,\n",
              " 0,\n",
              " 0.07142857142857142,\n",
              " 0.10714285714285714,\n",
              " 0,\n",
              " 0,\n",
              " 0.03571428571428571,\n",
              " 0.03571428571428571,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0.07142857142857142,\n",
              " 0,\n",
              " 0,\n",
              " 0.06896551724137931,\n",
              " 0.13793103448275862,\n",
              " 0.3448275862068966,\n",
              " 0.7586206896551724,\n",
              " 1.0,\n",
              " 0.034482758620689655,\n",
              " 0.10344827586206896,\n",
              " 0.41379310344827586,\n",
              " 0.7241379310344828,\n",
              " 0.896551724137931,\n",
              " 0.6896551724137931,\n",
              " 0.9310344827586207,\n",
              " 0.6896551724137931,\n",
              " 0.8275862068965517,\n",
              " 0.9310344827586207,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0.20689655172413793,\n",
              " 0.20689655172413793,\n",
              " 0.3793103448275862,\n",
              " 0.5862068965517241,\n",
              " 0.6551724137931034,\n",
              " 0.1724137931034483,\n",
              " 0.1724137931034483,\n",
              " 0.4827586206896552,\n",
              " 0.5517241379310345,\n",
              " 0.8620689655172413,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0.13793103448275862,\n",
              " 0.4482758620689655,\n",
              " 0.06896551724137931,\n",
              " 0.13793103448275862,\n",
              " 0.13793103448275862,\n",
              " 0.06896551724137931,\n",
              " 0,\n",
              " 0.10714285714285714,\n",
              " 0,\n",
              " 0.07142857142857142,\n",
              " 0.03571428571428571,\n",
              " 0.07142857142857142,\n",
              " 0,\n",
              " 0.10714285714285714,\n",
              " 0.10714285714285714,\n",
              " 0.10714285714285714,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0.4827586206896552,\n",
              " 0.4827586206896552,\n",
              " 0.5862068965517241,\n",
              " 0.7586206896551724,\n",
              " 0.8620689655172413,\n",
              " 0.06896551724137931,\n",
              " 0.1724137931034483,\n",
              " 0.41379310344827586,\n",
              " 0.6896551724137931,\n",
              " 0.9655172413793104,\n",
              " 0.10344827586206896,\n",
              " 1.0,\n",
              " 0.10344827586206896,\n",
              " 0.10344827586206896,\n",
              " 1.0,\n",
              " 0.034482758620689655,\n",
              " 0.034482758620689655,\n",
              " 0.3793103448275862,\n",
              " 0.7931034482758621,\n",
              " 0.8275862068965517,\n",
              " 0.2413793103448276,\n",
              " 0.2413793103448276,\n",
              " 0.27586206896551724,\n",
              " 0.3103448275862069,\n",
              " 0.4482758620689655,\n",
              " 0.5172413793103449,\n",
              " 0.5517241379310345,\n",
              " 0.5172413793103449,\n",
              " 0.5172413793103449,\n",
              " 0.5517241379310345,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0.16666666666666666,\n",
              " 0.2,\n",
              " 0.3,\n",
              " 0.13333333333333333,\n",
              " 0,\n",
              " 0.2,\n",
              " 0,\n",
              " 0.06896551724137931,\n",
              " 0.2413793103448276,\n",
              " 0.034482758620689655,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0.10344827586206896,\n",
              " 0.034482758620689655,\n",
              " 0,\n",
              " 0.034482758620689655,\n",
              " 0,\n",
              " 0.034482758620689655,\n",
              " 0,\n",
              " 0.06896551724137931,\n",
              " 0,\n",
              " 0,\n",
              " 0.10344827586206896,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0.06666666666666667,\n",
              " 0.06666666666666667,\n",
              " 0.16666666666666666,\n",
              " 0.7,\n",
              " 0.9666666666666667,\n",
              " 0.23333333333333334,\n",
              " 0.23333333333333334,\n",
              " 0.5666666666666667,\n",
              " 0.6,\n",
              " 1.0,\n",
              " 0.03333333333333333,\n",
              " 0.1,\n",
              " 0.26666666666666666,\n",
              " 0.7333333333333333,\n",
              " 0.9333333333333333,\n",
              " 0.2,\n",
              " 0.2,\n",
              " 0.3,\n",
              " 0.36666666666666664,\n",
              " 0.4,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0.3333333333333333,\n",
              " 0.3333333333333333,\n",
              " 0.4666666666666667,\n",
              " 0.5,\n",
              " 0.8333333333333334,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0.1896551724137931,\n",
              " 0.22413793103448276,\n",
              " 0.1206896551724138,\n",
              " 0.017241379310344827,\n",
              " 0.27586206896551724,\n",
              " 0.1724137931034483,\n",
              " 0,\n",
              " 0.15789473684210525,\n",
              " 0.03508771929824561,\n",
              " 0,\n",
              " 0.07017543859649122,\n",
              " 0.07017543859649122,\n",
              " 0,\n",
              " 0.07017543859649122,\n",
              " 0.017543859649122806,\n",
              " 0.08771929824561403,\n",
              " 0.08771929824561403,\n",
              " 0,\n",
              " 0,\n",
              " 0.08771929824561403,\n",
              " 0.03508771929824561,\n",
              " 0,\n",
              " 0,\n",
              " 0.017543859649122806,\n",
              " 0,\n",
              " 0.10526315789473684,\n",
              " 0,\n",
              " 0,\n",
              " 0.06896551724137931,\n",
              " 0.3103448275862069,\n",
              " 0.5689655172413793,\n",
              " 0.7241379310344828,\n",
              " 1.0,\n",
              " 0.05172413793103448,\n",
              " 0.1896551724137931,\n",
              " 0.5172413793103449,\n",
              " 0.7068965517241379,\n",
              " 0.9482758620689655,\n",
              " 0.017241379310344827,\n",
              " 0.017241379310344827,\n",
              " 0.27586206896551724,\n",
              " 0.8448275862068966,\n",
              " 0.9655172413793104,\n",
              " 0.15517241379310345,\n",
              " 0.15517241379310345,\n",
              " 0.15517241379310345,\n",
              " 0.15517241379310345,\n",
              " 0.15517241379310345,\n",
              " 0.034482758620689655,\n",
              " 0.2413793103448276,\n",
              " 0.39655172413793105,\n",
              " 0.6551724137931034,\n",
              " 0.8275862068965517,\n",
              " 0.1206896551724138,\n",
              " 0.13793103448275862,\n",
              " 0.3275862068965517,\n",
              " 0.5862068965517241,\n",
              " 0.9310344827586207,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0.15254237288135594,\n",
              " 0.3220338983050847,\n",
              " 0.1864406779661017,\n",
              " 0.13559322033898305,\n",
              " 0.06779661016949153,\n",
              " 0.13559322033898305,\n",
              " 0,\n",
              " 0.08620689655172414,\n",
              " 0.1206896551724138,\n",
              " 0.05172413793103448,\n",
              " 0.017241379310344827,\n",
              " 0.034482758620689655,\n",
              " 0,\n",
              " 0.10344827586206896,\n",
              " 0.06896551724137931,\n",
              " 0.05172413793103448,\n",
              " 0.017241379310344827,\n",
              " 0,\n",
              " 0.017241379310344827,\n",
              " 0,\n",
              " 0.034482758620689655,\n",
              " 0,\n",
              " 0,\n",
              " 0.05172413793103448,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0.23728813559322035,\n",
              " 0.288135593220339,\n",
              " 0.423728813559322,\n",
              " 0.576271186440678,\n",
              " 0.9830508474576272,\n",
              " 0.03389830508474576,\n",
              " 0.1016949152542373,\n",
              " 0.3389830508474576,\n",
              " 0.6101694915254238,\n",
              " 1.0,\n",
              " 0.05084745762711865,\n",
              " 0.4915254237288136,\n",
              " 0.559322033898305,\n",
              " 0.864406779661017,\n",
              " 0.9661016949152542,\n",
              " 0.01694915254237288,\n",
              " 0.1864406779661017,\n",
              " 0.4067796610169492,\n",
              " 0.6440677966101694,\n",
              " 0.6949152542372882,\n",
              " 0.11864406779661017,\n",
              " 0.11864406779661017,\n",
              " 0.13559322033898305,\n",
              " 0.15254237288135594,\n",
              " 0.22033898305084745,\n",
              " 0.2542372881355932,\n",
              " 0.2711864406779661,\n",
              " 0.711864406779661,\n",
              " 0.7457627118644068,\n",
              " 0.9152542372881356,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0.20689655172413793,\n",
              " 0.3620689655172414,\n",
              " 0.08620689655172414,\n",
              " 0.06896551724137931,\n",
              " 0.1724137931034483,\n",
              " 0.10344827586206896,\n",
              " 0,\n",
              " 0.17543859649122806,\n",
              " 0.017543859649122806,\n",
              " 0.05263157894736842,\n",
              " 0.07017543859649122,\n",
              " 0.07017543859649122,\n",
              " 0,\n",
              " 0.10526315789473684,\n",
              " 0.05263157894736842,\n",
              " 0.08771929824561403,\n",
              " 0.05263157894736842,\n",
              " 0,\n",
              " 0,\n",
              " 0.017543859649122806,\n",
              " 0.017543859649122806,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0.03508771929824561,\n",
              " 0,\n",
              " 0,\n",
              " 0.034482758620689655,\n",
              " 0.1206896551724138,\n",
              " 0.3793103448275862,\n",
              " 0.7413793103448276,\n",
              " 0.9310344827586207,\n",
              " 0.017241379310344827,\n",
              " 0.25862068965517243,\n",
              " 0.5689655172413793,\n",
              " 0.8103448275862069,\n",
              " 0.9827586206896551,\n",
              " 0.3448275862068966,\n",
              " 0.3448275862068966,\n",
              " 0.41379310344827586,\n",
              " 0.46551724137931033,\n",
              " 1.0,\n",
              " 0.5172413793103449,\n",
              " 0.5172413793103449,\n",
              " 0.6896551724137931,\n",
              " 0.896551724137931,\n",
              " 0.9137931034482759,\n",
              " 0.10344827586206896,\n",
              " 0.15517241379310345,\n",
              " 0.3103448275862069,\n",
              " 0.6206896551724138,\n",
              " 0.7241379310344828,\n",
              " 0.08620689655172414,\n",
              " 0.08620689655172414,\n",
              " 0.27586206896551724,\n",
              " 0.43103448275862066,\n",
              " 0.7758620689655172,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0.1724137931034483,\n",
              " 0.2988505747126437,\n",
              " 0.10344827586206896,\n",
              " 0.05747126436781609,\n",
              " 0.22988505747126436,\n",
              " 0.13793103448275862,\n",
              " 0,\n",
              " 0.13953488372093023,\n",
              " 0.023255813953488372,\n",
              " 0.03488372093023256,\n",
              " 0.05813953488372093,\n",
              " 0.06976744186046512,\n",
              " 0,\n",
              " 0.08139534883720931,\n",
              " 0.046511627906976744,\n",
              " 0.09302325581395349,\n",
              " 0.05813953488372093,\n",
              " 0,\n",
              " 0,\n",
              " 0.05813953488372093,\n",
              " 0.023255813953488372,\n",
              " 0,\n",
              " 0,\n",
              " 0.011627906976744186,\n",
              " 0,\n",
              " 0.06976744186046512,\n",
              " 0,\n",
              " 0,\n",
              " 0.04597701149425287,\n",
              " 0.3333333333333333,\n",
              " 0.4482758620689655,\n",
              " 0.6666666666666666,\n",
              " 0.9540229885057471,\n",
              " 0.034482758620689655,\n",
              " 0.3448275862068966,\n",
              " 0.632183908045977,\n",
              " 0.8045977011494253,\n",
              " 0.9885057471264368,\n",
              " 0.011494252873563218,\n",
              " 0.05747126436781609,\n",
              " 0.28735632183908044,\n",
              " 0.6091954022988506,\n",
              " 1.0,\n",
              " 0.10344827586206896,\n",
              " 0.10344827586206896,\n",
              " 0.6781609195402298,\n",
              " 0.7931034482758621,\n",
              " 0.9425287356321839,\n",
              " 0.022988505747126436,\n",
              " 0.1724137931034483,\n",
              " 0.3218390804597701,\n",
              " 0.5402298850574713,\n",
              " 0.8160919540229885,\n",
              " 0.08045977011494253,\n",
              " 0.13793103448275862,\n",
              " 0.3103448275862069,\n",
              " 0.5172413793103449,\n",
              " 0.8505747126436781,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0.19318181818181818,\n",
              " 0.3068181818181818,\n",
              " 0.1590909090909091,\n",
              " 0.09090909090909091,\n",
              " 0.11363636363636363,\n",
              " 0.13636363636363635,\n",
              " 0,\n",
              " 0.13793103448275862,\n",
              " 0.09195402298850575,\n",
              " 0.04597701149425287,\n",
              " 0.04597701149425287,\n",
              " 0.04597701149425287,\n",
              " 0,\n",
              " 0.10344827586206896,\n",
              " 0.04597701149425287,\n",
              " 0.05747126436781609,\n",
              " 0.04597701149425287,\n",
              " 0,\n",
              " 0.011494252873563218,\n",
              " 0.011494252873563218,\n",
              " 0.034482758620689655,\n",
              " 0,\n",
              " 0,\n",
              " 0.034482758620689655,\n",
              " 0,\n",
              " 0.022988505747126436,\n",
              " 0,\n",
              " 0,\n",
              " 0.022727272727272728,\n",
              " 0.11363636363636363,\n",
              " 0.32954545454545453,\n",
              " 0.6136363636363636,\n",
              " 0.9886363636363636,\n",
              " 0.011363636363636364,\n",
              " 0.23863636363636365,\n",
              " 0.4431818181818182,\n",
              " 0.6363636363636364,\n",
              " 1.0,\n",
              " 0.22727272727272727,\n",
              " 0.3068181818181818,\n",
              " 0.6931818181818182,\n",
              " 0.875,\n",
              " 0.9772727272727273,\n",
              " 0.3409090909090909,\n",
              " 0.45454545454545453,\n",
              " 0.6022727272727273,\n",
              " 0.7613636363636364,\n",
              " 0.7954545454545454,\n",
              " 0.06818181818181818,\n",
              " 0.10227272727272728,\n",
              " 0.20454545454545456,\n",
              " 0.4090909090909091,\n",
              " 0.4772727272727273,\n",
              " 0.056818181818181816,\n",
              " 0.18181818181818182,\n",
              " 0.5113636363636364,\n",
              " 0.8181818181818182,\n",
              " 0.9431818181818182,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0.18181818181818182,\n",
              " 0.2727272727272727,\n",
              " 0.125,\n",
              " 0.09090909090909091,\n",
              " 0.18181818181818182,\n",
              " 0.14772727272727273,\n",
              " 0,\n",
              " 0.12643678160919541,\n",
              " 0.04597701149425287,\n",
              " 0.04597701149425287,\n",
              " 0.05747126436781609,\n",
              " 0.06896551724137931,\n",
              " 0,\n",
              " 0.09195402298850575,\n",
              " 0.04597701149425287,\n",
              " 0.08045977011494253,\n",
              " 0.04597701149425287,\n",
              " 0,\n",
              " 0.011494252873563218,\n",
              " 0.034482758620689655,\n",
              " 0.022988505747126436,\n",
              " 0,\n",
              " 0,\n",
              " 0.034482758620689655,\n",
              " 0,\n",
              " 0.04597701149425287,\n",
              " 0,\n",
              " 0,\n",
              " 0.045454545454545456,\n",
              " 0.2159090909090909,\n",
              " 0.42045454545454547,\n",
              " 0.6931818181818182,\n",
              " 0.8863636363636364,\n",
              " 0.07954545454545454,\n",
              " 0.3068181818181818,\n",
              " 0.5454545454545454,\n",
              " 0.7159090909090909,\n",
              " 0.9090909090909091,\n",
              " 0.022727272727272728,\n",
              " 0.125,\n",
              " 0.4772727272727273,\n",
              " 0.8409090909090909,\n",
              " 0.9204545454545454,\n",
              " 0.5113636363636364,\n",
              " 0.625,\n",
              " 0.7727272727272727,\n",
              " 0.9318181818181818,\n",
              " 0.9659090909090909,\n",
              " 0.011363636363636364,\n",
              " 0.10227272727272728,\n",
              " 0.2727272727272727,\n",
              " 0.38636363636363635,\n",
              " 0.6477272727272727,\n",
              " 0.03409090909090909,\n",
              " 0.14772727272727273,\n",
              " 0.3522727272727273,\n",
              " 0.6818181818181818,\n",
              " 1.0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# 630-dimensional local descriptor vector\n",
        "ld_code_of(seq)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ihzqb1mB8j6B"
      },
      "source": [
        "### Auto Covariance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "4Ri4QqyxGwL0"
      },
      "outputs": [],
      "source": [
        "# alphabet = [\"A\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"K\", \"L\", \"M\", \"N\", \"P\", \"Q\", \"R\", \"S\", \"T\", \"V\", \"W\", \"Y\"]\n",
        "# chem_props = np.array([0.62, -0.5,0.007187,8.1,0.046,1.181,27.5,0.29,-1,-0.03661,5.5,0.128,1.461,44.6,-0.9,3,-0.02382,13,0.105,1.587,40,-0.74,3,0.006802,12.3,0.151,1.862,62,1.19,-2.5,0.037552,5.2,0.29,2.228,115.5,0.48,0,0.179052,9,0,0.881,0,-0.4,-0.5,-0.01069,10.4,0.23,2.025,79,1.38,-1.8,0.021631,5.2,0.186,1.81,93.5,-1.5,3,0.017708,11.3,0.219,2.258,100,1.06,-1.8,0.051672,4.9,0.186,1.931,93.5,0.64,-1.3,0.002683,5.7,0.221,2.034,94.1,-0.78,2,0.005392,11.6,0.134,1.655,58.7,0.12,0,0.239531,8,0.131,1.468,41.9,-0.85,0.2,0.049211,10.5,0.18,1.932,80.7,-2.53,3,0.043587,10.5,0.291,2.56,105,-0.18,0.3,0.004627,9.2,0.062,1.298,29.3,-0.05,-0.4,0.003352,8.6,0.108,1.525,51.3,1.08,-1.5,0.057004,5.9,0.14,1.645,71.5,0.81,-3.4,0.037977,5.4,0.409,2.663,145.5,0.26,-2.3,117.3,6.2,0.298,2.368,0.023599])\n",
        "# chem_props = chem_props.reshape(len(alphabet),7)\n",
        "\n",
        "# # a = np.asarray([ [1,2,3], [4,5,6], [7,8,9] ])\n",
        "# # for i in range(7):\n",
        "# import pandas as pd \n",
        "# df = pd.DataFrame(chem_props)\n",
        "# df = df.rename(index={0: \"A\", 1: \"C\", 2: \"D\", 3: \"E\", 4: \"F\", 5: \"G\", 6: \"H\", 7: \"I\", 8: \"K\", 9: \"L\", 10: \"M\", 11: \"N\", 12: \"P\", 13:\"Q\", 14:\"R\", 15: \"S\", 16:\"T\", 17: \"V\", 18: \"W\", 19: \"Y\"})\n",
        "# df.to_csv(\"encode_ac.txt\", sep='\\t', header=None)\n",
        "# # df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXJI0Myaf0Rj",
        "outputId": "156b1bf3-9320-4d21-9c45-e14d70f3e49e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.10790803428842154, 0.11173882903839012, -0.14460761970272953, -0.07535489086506163, 0.006237248073411141, 0.0013678539469620598, 0.05270459435279943, 0.1828958100504284, -0.06548284241673244, -0.06356580523179726, -0.0827079689124269, 0.0017860602217885661, 0.13334465990147337, -0.024160265667395202, -0.037172464278242986, -0.06483520719302295, -0.13815087172208335, 0.0897187757953958, -0.040758691200317373, 0.18293023336208467, 0.12094364277218479, 0.09689785077692985, 0.049025209707293, 0.06979361502005867, 0.218261048339127, 0.1893236184497817, 0.018408227449015546, 0.08829965543046875, 0.052992276171663556, -0.12454362437003005, 0.32508896178416097, 0.06277731652196941, -0.17305532936980655, -0.06880909521329583, 0.007179034232072711, -0.0009166973877763069, 0.10437143296785639, 0.21149735698003105, -0.05294867285423665, 0.04381139122214881, 0.17104549428953786, 0.032328074948299, -0.07969681488713316, -0.051078254991167714, 0.07857539050459156, 0.006677363314985386, -0.16450598856132026, -0.005023355716242042, 0.13447492229141475, 0.2725606506999623, 0.06972775465272221, -0.10439079233878017, -0.042462529041505166, -0.06525527769839443, -0.007620719414048265, 0.2929423870463783, 0.1698837839119438, 0.34368375048203115, 0.05364604359784541, 0.0865836477533407, -0.09979393067695948, -0.1023334318205191, 0.09404673973154629, -0.09341365460725153, -0.08143986121176419, 0.10615002323508783, -0.08491195292088012, -0.08693758734869886, 0.12068427066929377, -0.07602626943625378, 0.12012154373173536, 0.134815923662001, -0.06800504219689996, 0.13491253696728084, -0.07093044946192124, -0.07318664579059081, -0.0742398973620677, -0.07670748508358097, -0.07744721943693919, 0.3542482387516324, -0.08082791763539843, -0.08396376436926489, -0.06936817429741346, 0.1553165092704602, -0.057744888658659295, -0.06009246184686986, -0.06153552146714124, -0.06366557817467383, -0.06522554712943501, -0.06681398915483294, 0.2742891805951771, 0.0230304043317034, -0.12292307987298794, -0.10812428048967351, -0.07140079055430806, -0.12313018528106721, 0.029119289807633546, 0.12958368953587043, -0.0781614311723816, 0.02444938842952771, 0.17196013425884496, 0.013359731523309278, -0.05043821949700768, -0.05622880675505674, 0.09371760849494398, -0.03494851113768867, -0.24735071296973993, -0.06242874059343693, 0.08926891500575204, 0.1498449073149161, 0.007879507848047874, -0.13648669759815008, -0.04722946844876689, -0.08230829240123116, -0.08349532175232492, 0.2327225476156886, 0.1705090730041967, 0.29656740391166486, 0.124518743273069, 0.053158802172273534, 0.08429033960587115, 0.0784269594239277, 0.09858020626009595, -0.11899949623741407, -0.04247129246150611, 0.07352898646383896, -0.20920354321570217, 0.041066825607296874, -0.019035819330898193, -0.05140892548426074, 0.0026747731573285812, -0.011297630148535838, -0.08553000724400059, -0.016733659903766872, -0.04098354508224823, 0.08220009310606198, 0.01349952571069666, -0.0030177472257386824, 0.011802538631382685, -0.012382573878674481, -0.039488829507421226, -0.021417524901129907, -0.13347769450074365, 0.023441603483421957, -0.0648941442216763, -0.03879933455225307, 0.06554742475995017, -0.09162228182983588, 0.0007407293592178221, 0.007811157463376443, 0.08901384659482531, 0.09686261607218066, 0.13941681960649904, -0.13000565009692766, -0.021766381430552543, 0.13660769049296473, -0.20453043142370772, 0.027468349774399044, -0.03435692650993974, -0.054976281770174794, -0.004913069849397187, -0.015493718156269785, -0.07198828822266251, -0.032557246717684354, -0.041356143651355044, 0.07391663877343498, -0.016644490035137377, -0.018413903228021555, -0.015284487422400444, -0.02969323181356979, 0.0032449767663577257, -0.016378731691578895, -0.11054490073761429, 0.03452649153659403, -0.05347842900159755, 0.009133573842133277, 0.09119946731010772, -0.10786912443540668, -0.03945050855068204, -0.026254579235587553, 0.046881420880807656, 0.014121866300099857, 0.10260631465175607, -0.08728678622993302, -0.037565207510200496, -0.007288668744566924, 0.1007613325881951, -0.05234734188176808, 0.05612806804452678, -0.06486998122640711, -0.029519493378660673, -0.029339986264426934, 0.025886588935373107, 0.06350573350290536, -0.0925061804413994, 0.023658082282278073, -0.00022690216577556002, -0.0049267887928321295, 0.016573618705815727, 0.12849803544634225, 0.02032403430626441, -0.07137030432764936, -0.043789209010392544, 0.05080259733110751, -0.05631605447914313, -0.00287696514061479, 0.10973743165259096, -0.07342673761532045, -0.06931273815410258, -0.14370165374581315]\n"
          ]
        }
      ],
      "source": [
        "'''Implementation of AC coding method\n",
        "'''\n",
        "\n",
        "__all__ = ['ac_code_of']\n",
        "\n",
        "# PCPNS: Physicochemical property names\n",
        "PCPNS = ['H1', 'H2', 'NCI', 'P1', 'P2', 'SASA', 'V']\n",
        "\n",
        "# AAPCPVS: Physicochemical property values of amino acid\n",
        "AAPCPVS = {\n",
        "    'A': { 'H1': 0.62, 'H2':-0.5, 'NCI': 0.007187, 'P1': 8.1, 'P2':0.046, 'SASA':1.181, 'V': 27.5 },\n",
        "    'C': { 'H1': 0.29, 'H2':-1.0, 'NCI':-0.036610, 'P1': 5.5, 'P2':0.128, 'SASA':1.461, 'V': 44.6 },\n",
        "    'D': { 'H1':-0.90, 'H2': 3.0, 'NCI':-0.023820, 'P1':13.0, 'P2':0.105, 'SASA':1.587, 'V': 40.0 },\n",
        "    'E': { 'H1': 0.74, 'H2': 3.0, 'NCI': 0.006802, 'P1':12.3, 'P2':0.151, 'SASA':1.862, 'V': 62.0 },\n",
        "    'F': { 'H1': 1.19, 'H2':-2.5, 'NCI': 0.037552, 'P1': 5.2, 'P2':0.290, 'SASA':2.228, 'V':115.5 },\n",
        "    'G': { 'H1': 0.48, 'H2': 0.0, 'NCI': 0.179052, 'P1': 9.0, 'P2':0.000, 'SASA':0.881, 'V':  0.0 },\n",
        "    'H': { 'H1':-0.40, 'H2':-0.5, 'NCI':-0.010690, 'P1':10.4, 'P2':0.230, 'SASA':2.025, 'V': 79.0 },\n",
        "    'I': { 'H1': 1.38, 'H2':-1.8, 'NCI': 0.021631, 'P1': 5.2, 'P2':0.186, 'SASA':1.810, 'V': 93.5 },\n",
        "    'K': { 'H1':-1.50, 'H2': 3.0, 'NCI': 0.017708, 'P1':11.3, 'P2':0.219, 'SASA':2.258, 'V':100.0 },\n",
        "    'L': { 'H1': 1.06, 'H2':-1.8, 'NCI': 0.051672, 'P1': 4.9, 'P2':0.186, 'SASA':1.931, 'V': 93.5 },\n",
        "    'M': { 'H1': 0.64, 'H2':-1.3, 'NCI': 0.002683, 'P1': 5.7, 'P2':0.221, 'SASA':2.034, 'V': 94.1 },\n",
        "    'N': { 'H1':-0.78, 'H2': 2.0, 'NCI': 0.005392, 'P1':11.6, 'P2':0.134, 'SASA':1.655, 'V': 58.7 },\n",
        "    'P': { 'H1': 0.12, 'H2': 0.0, 'NCI': 0.239531, 'P1': 8.0, 'P2':0.131, 'SASA':1.468, 'V': 41.9 },\n",
        "    'Q': { 'H1':-0.85, 'H2': 0.2, 'NCI': 0.049211, 'P1':10.5, 'P2':0.180, 'SASA':1.932, 'V': 80.7 },\n",
        "    'R': { 'H1':-2.53, 'H2': 3.0, 'NCI': 0.043587, 'P1':10.5, 'P2':0.291, 'SASA':2.560, 'V':105.0 },\n",
        "    'S': { 'H1':-0.18, 'H2': 0.3, 'NCI': 0.004627, 'P1': 9.2, 'P2':0.062, 'SASA':1.298, 'V': 29.3 },\n",
        "    'T': { 'H1':-0.05, 'H2':-0.4, 'NCI': 0.003352, 'P1': 8.6, 'P2':0.108, 'SASA':1.525, 'V': 51.3 },\n",
        "    'V': { 'H1': 1.08, 'H2':-1.5, 'NCI': 0.057004, 'P1': 5.9, 'P2':0.140, 'SASA':1.645, 'V': 71.5 },\n",
        "    'W': { 'H1': 0.81, 'H2':-3.4, 'NCI': 0.037977, 'P1': 5.4, 'P2':0.409, 'SASA':2.663, 'V':145.5 },\n",
        "    'Y': { 'H1': 0.26, 'H2':-2.3, 'NCI': 117.3000, 'P1': 6.2, 'P2':0.298, 'SASA':2.368, 'V':  0.023599 },\n",
        "}\n",
        "\n",
        "import math\n",
        "\n",
        "def avg_sd(NUMBERS):\n",
        "    AVG = sum(NUMBERS)/len(NUMBERS)\n",
        "    TEM = [pow(NUMBER-AVG, 2) for NUMBER in NUMBERS]\n",
        "    DEV = sum(TEM)/len(TEM)\n",
        "    SD = math.sqrt(DEV)\n",
        "    return (AVG, SD)\n",
        "\n",
        "# PCPVS: Physicochemical property values\n",
        "PCPVS = {'H1':[], 'H2':[], 'NCI':[], 'P1':[], 'P2':[], 'SASA':[], 'V':[]}\n",
        "for AA, PCPS in AAPCPVS.items():\n",
        "    for PCPN in PCPNS:\n",
        "        PCPVS[PCPN].append(PCPS[PCPN])\n",
        "\n",
        "# PCPASDS: Physicochemical property avg and sds\n",
        "PCPASDS = {}\n",
        "for PCP, VS in PCPVS.items():\n",
        "    PCPASDS[PCP] = avg_sd(VS)\n",
        "\n",
        "# NORMALIZED_AAPCPVS\n",
        "NORMALIZED_AAPCPVS = {}\n",
        "for AA, PCPS in AAPCPVS.items():\n",
        "    NORMALIZED_PCPVS = {}\n",
        "    for PCP, V in PCPS.items():\n",
        "        NORMALIZED_PCPVS[PCP] = (V-PCPASDS[PCP][0])/PCPASDS[PCP][1]\n",
        "    NORMALIZED_AAPCPVS[AA] = NORMALIZED_PCPVS\n",
        "\n",
        "def pcp_value_of(AA, PCP):\n",
        "    \"\"\"Get physicochemical properties value of amino acid.\"\"\"\n",
        "    return NORMALIZED_AAPCPVS[AA][PCP];\n",
        "\n",
        "def pcp_sequence_of(PS, PCP):\n",
        "    \"\"\"Make physicochemical properties sequence of protein sequence.\"\"\"\n",
        "    PCPS = []\n",
        "    for I, CH in enumerate(PS):\n",
        "        PCPS.append(pcp_value_of(CH, PCP))\n",
        "    # Centralization\n",
        "    AVG = sum(PCPS)/len(PCPS)\n",
        "    for I, PCP in enumerate(PCPS):\n",
        "        PCPS[I] = PCP - AVG\n",
        "    return PCPS\n",
        "\n",
        "def ac_values_of(PS, PCP, LAG):\n",
        "    \"\"\"Get ac values of protein sequence.\"\"\"\n",
        "    AVS = []\n",
        "    PCPS = pcp_sequence_of(PS, PCP)\n",
        "    for LG in range(1, LAG+1):\n",
        "        SUM = 0\n",
        "        for I in range(len(PCPS)-LG):\n",
        "            SUM = SUM + PCPS[I]*PCPS[I+LG]\n",
        "        SUM = SUM / (len(PCPS)-LG)\n",
        "        AVS.append(SUM)\n",
        "    return AVS\n",
        "\n",
        "def all_ac_values_of(PS, LAG):\n",
        "    \"\"\"Get all ac values of protein sequence.\"\"\"\n",
        "    AAVS = []\n",
        "    for PCP in PCPS:\n",
        "        AVS = ac_values_of(PS, PCP, LAG)\n",
        "        AAVS = AAVS + AVS\n",
        "    return AAVS\n",
        "\n",
        "def ac_code_of(PS):\n",
        "    \"\"\"Get ac code of protein sequence.\"\"\"\n",
        "    AC_Code = all_ac_values_of(PS, 30)\n",
        "    # Normalizing AC_Code\n",
        "    # MIN_CODE = min(AC_Code)\n",
        "    # MAX_CODE = max(AC_Code)\n",
        "    # AC_Code = [(N-MIN_CODE)*1.0/(MAX_CODE-MIN_CODE) for N in AC_Code]\n",
        "    return AC_Code\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "    AC = ac_code_of('MKFVYKEEHPFEKRRSEGEKIRKKYPDRVPVIVEKAPKARIGDLDKKKYLVPSDLTVGQFYFLIRKRIHLRAEDALFFFVNNVIPPTSATMGQLYQEHHEEDFFLYIAYSDESVYGL')\n",
        "    print(AC)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PItkMayM2z4w",
        "outputId": "6ba87624-9f54-4d8c-c2a0-59502c0a5fe3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "210"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# 210-dimension auto covariance vector\n",
        "len(AC)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8ncX7ZW8ma7"
      },
      "source": [
        "### Pseudo amino acid composition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CHIsFXbi07ja",
        "outputId": "6528cf88-31c7-442b-e8b0-67258c0ed254"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.03915340164841422,\n",
              " 0.052204535531218975,\n",
              " 0.013051133882804744,\n",
              " 0.0456789685898166,\n",
              " 0.0,\n",
              " 0.07178123635542608,\n",
              " 0.01957670082420711,\n",
              " 0.03262783470701185,\n",
              " 0.026102267765609487,\n",
              " 0.0456789685898166,\n",
              " 0.058730102472621344,\n",
              " 0.07830680329682844,\n",
              " 0.013051133882804744,\n",
              " 0.058730102472621344,\n",
              " 0.0456789685898166,\n",
              " 0.03262783470701185,\n",
              " 0.01957670082420711,\n",
              " 0.0,\n",
              " 0.052204535531218975,\n",
              " 0.058730102472621344,\n",
              " 0.07192890297900868,\n",
              " 0.07609696697620942,\n",
              " 0.08848279790070446]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "def paac(str_, lambda_=0):\n",
        "  # str_=\"ATTRCDEQGGGMFSTQW\"\n",
        "  # lambda_ = 3\n",
        "  len_=len(str_)\n",
        "  tt=['A', 'R', 'N', 'D', 'C', 'E', 'Q', 'G', 'H', 'I',  'L', 'K', 'M', 'F', 'P', 'S', 'T', 'W', 'Y', 'V']\n",
        "  A = [0.62,  -0.5, 15]\n",
        "  R = [-2.53,   3, 101]\n",
        "  N = [-0.78,  0.2, 58]\n",
        "  D = [-0.9,    3, 59]\n",
        "  C = [0.29,    -1, 47]\n",
        "  E = [-0.74,   3, 73]\n",
        "  Q = [-0.85,  0.2, 72]\n",
        "  G =[0.48,    0, 1]\n",
        "  H = [-0.4,  -0.5, 82]\n",
        "  I = [1.38, -1.8, 57]\n",
        "  L = [1.06,  -1.8, 57]\n",
        "  K = [-1.5,    3, 73]\n",
        "  M = [0.64,  -1.3, 75]\n",
        "  F =[1.19, -2.5, 91]\n",
        "  P = [0.12,     0, 42]\n",
        "  S = [-0.18, 0.3, 31]\n",
        "  T = [-0.05, -0.4, 45]\n",
        "  W = [0.81, -3.4, 130] \n",
        "  Y = [0.26,  -2.3, 107]\n",
        "  V = [1.08, -1.5, 43]\n",
        "  H1=[A[0],R[0],N[0],D[0],C[0],E[0],Q[0],G[0],H[0],I[0],L[0],K[0],M[0],F[0],P[0],S[0],T[0],W[0],Y[0],V[0]]\n",
        "  H2=[A[1],R[1],N[1],D[1],C[1],E[1],Q[1],G[1],H[1],I[1],L[1],K[1],M[1],F[1],P[1],S[1],T[1],W[1],Y[1],V[1]]\n",
        "  M=[A[2],R[2],N[2],D[2],C[2],E[2],Q[2],G[2],H[2],I[2],L[2],K[2],M[2],F[2],P[2],S[2],T[2],W[2],Y[2],V[2]]\n",
        "  # Normalization\n",
        "  mean_H1=np.mean(H1)\n",
        "  std_H1=np.std(H1)\n",
        "  H1=(H1-mean_H1)/(std_H1)\n",
        "\n",
        "\n",
        "\n",
        "  mean_H2=np.mean(H2)\n",
        "  std_H2=np.std(H2)\n",
        "  H2=(H2-mean_H2)/(std_H2)\n",
        "\n",
        "  mean_M=np.mean(M)\n",
        "  std_M=np.std(M)\n",
        "  M=(M-mean_M)/(std_M)\n",
        "  data=np.zeros((1,len_))\n",
        "  f=np.zeros((1,20))\n",
        "\n",
        "  for j in range(len_):\n",
        "      for k in range(20):\n",
        "          # if strcmp(str(j),tt(k))==1\n",
        "          if str_[j] == tt[k]:\n",
        "              # print(j, k)\n",
        "              data[:,j]=int(k)+1\n",
        "              f[:,k]=f[:,k]+1\n",
        "  data = data.astype('int32')\n",
        "  Theta=np.zeros((lambda_,len_))\n",
        "  H=np.hstack((H1,H2,M))\n",
        "  H=H.reshape(3,-1)\n",
        "  for i in range(lambda_):\n",
        "      # for j=1:len-i\n",
        "      for j in range(len_-i):\n",
        "          if j+i+1<len_:\n",
        "              Theta[i,j]=np.mean(np.mean((H[:, data[:,j]-1]-H[:, data[:,j+i+1]-1])**2))\n",
        "\n",
        "  theta=np.zeros((1,lambda_))\n",
        "  for j in range(lambda_):\n",
        "      theta[:,j]=np.mean(Theta[j,:(len_-j-1)])\n",
        "\n",
        "  f=f/len_\n",
        "  XC=f/(1+0.05*np.sum(theta))\n",
        "  XC2=(0.05*theta)/(1+0.05*np.sum(theta))\n",
        "\n",
        "  paac = np.hstack((XC, XC2))\n",
        "  paac = paac.reshape(-1,).tolist()\n",
        "  return paac\n",
        "\n",
        "\n",
        "# 23 dimension paac vector\n",
        "paac(seq, 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGEuw7JBLpcY"
      },
      "source": [
        "### Amino acid composition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "oGMJF17fN9vn"
      },
      "outputs": [],
      "source": [
        "def aac(seq):\n",
        "  aa_list = ['A', 'R', 'N', 'D', 'C', 'E', 'Q', 'G', 'H', 'I',  'L', 'K', 'M', 'F', 'P', 'S', 'T', 'W', 'Y', 'V']\n",
        "  sum_freq = 0\n",
        "  for i in range(20):\n",
        "    sum_freq += seq.count(aa_list[i])\n",
        "\n",
        "  aa_freq = []\n",
        "  for i in range(20):\n",
        "    aa_freq.append(seq.count(aa_list[i])/sum_freq)\n",
        "  return aa_freq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EdaN4-dO-WQ"
      },
      "source": [
        "### Concatenate of protein sequence features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ih153Zn6RTxj"
      },
      "outputs": [],
      "source": [
        "def encode_seq(seq):\n",
        "  encoding = paac(seq, 3)+ct_code_of(seq)\n",
        "  encoding = np.array(encoding)\n",
        "  encoding = encoding.reshape(-1, )\n",
        "  return encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download pairs-label and id-seq file"
      ],
      "metadata": {
        "id": "YrsmOfEvwNR8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6X_2PtoHQAfm",
        "outputId": "a259b5a9-2f6b-4191-c746-8c84fdf1a5cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-01-31 07:05:25--  https://raw.githubusercontent.com/anhvt00/PIPR/master/embeddings/seq2tensor.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1104 (1.1K) [text/plain]\n",
            "Saving to: ‘seq2tensor.py’\n",
            "\n",
            "seq2tensor.py       100%[===================>]   1.08K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-01-31 07:05:25 (73.4 MB/s) - ‘seq2tensor.py’ saved [1104/1104]\n",
            "\n",
            "--2022-01-31 07:05:25--  https://raw.githubusercontent.com/anhvt00/PIPR/master/embeddings/ac5_aph.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2335 (2.3K) [text/plain]\n",
            "Saving to: ‘ac5_aph.txt’\n",
            "\n",
            "ac5_aph.txt         100%[===================>]   2.28K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-01-31 07:05:25 (57.5 MB/s) - ‘ac5_aph.txt’ saved [2335/2335]\n",
            "\n",
            "--2022-01-31 07:05:25--  https://raw.githubusercontent.com/anhvt00/PIPR/master/data/Tunning-architecture-dataset/yeast_pairs.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 179008 (175K) [text/plain]\n",
            "Saving to: ‘yeast_pairs.tsv’\n",
            "\n",
            "yeast_pairs.tsv     100%[===================>] 174.81K  --.-KB/s    in 0.008s  \n",
            "\n",
            "2022-01-31 07:05:25 (20.2 MB/s) - ‘yeast_pairs.tsv’ saved [179008/179008]\n",
            "\n",
            "--2022-01-31 07:05:25--  https://raw.githubusercontent.com/anhvt00/PIPR/master/data/Tunning-architecture-dataset/yeast_dictionary.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1415079 (1.3M) [text/plain]\n",
            "Saving to: ‘yeast_dictionary.tsv’\n",
            "\n",
            "yeast_dictionary.ts 100%[===================>]   1.35M  --.-KB/s    in 0.02s   \n",
            "\n",
            "2022-01-31 07:05:26 (69.0 MB/s) - ‘yeast_dictionary.tsv’ saved [1415079/1415079]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Download file ac5_aph.txt for ac5_aph embedding \n",
        "!wget https://raw.githubusercontent.com/anhvt00/PIPR/master/embeddings/ac5_aph.txt\n",
        "\n",
        "# Download dictionary file (id: sequence)\n",
        "!wget https://raw.githubusercontent.com/anhvt00/PIPR/master/data/Tunning-architecture-dataset/yeast_pairs.tsv\n",
        "\n",
        "# Download pairs of proteins with labels file\n",
        "!wget https://raw.githubusercontent.com/anhvt00/PIPR/master/data/Tunning-architecture-dataset/yeast_dictionary.tsv"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create dataset in cluding pairs and labels representation"
      ],
      "metadata": {
        "id": "zHQ8Wr72wkqi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Y2Tp-uWLhS8",
        "outputId": "660baa77-5fea-4467-91ca-0eeb1b9b95da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "11188it [00:00, 409532.51it/s]\n",
            "100%|██████████| 2497/2497 [01:59<00:00, 20.97it/s]\n",
            "100%|██████████| 11187/11187 [00:00<00:00, 1712282.55it/s]\n",
            "100%|██████████| 11187/11187 [00:00<00:00, 1973904.29it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1., 1., 0., ..., 1., 1., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 121
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# Hyperparameter for embedding file\n",
        "ds_file = 'yeast_pairs.tsv'\n",
        "label_index = 2\n",
        "id2seq_file = 'yeast_dictionary.tsv'\n",
        "id2index = {}\n",
        "seqs = []\n",
        "index = 0\n",
        "sid1_index = 0\n",
        "sid2_index = 1\n",
        "\n",
        "# Create line variable as a list of protein sequences with index is the number of protein sequences\n",
        "# id2index is a dictionary of protein id and incremental index number \n",
        "for line in open(id2seq_file):\n",
        "    line = line.strip().split('\\t')\n",
        "    id2index[line[0]] = index\n",
        "    seqs.append(line[1])\n",
        "    index += 1\n",
        "\n",
        "seq_array = []\n",
        "id2_aid = {}\n",
        "sid = 0\n",
        "max_data = -1\n",
        "limit_data = max_data > 0\n",
        "raw_data = []\n",
        "skip_head = True\n",
        "x = None\n",
        "count = 0\n",
        "\n",
        "# Create sequence array as a list of protein strings\n",
        "# Create raw data as list of pairs and label\n",
        "for line in tqdm(open(ds_file)):\n",
        "    if skip_head:\n",
        "        skip_head = False\n",
        "        continue\n",
        "    line = line.rstrip('\\n').rstrip('\\r').split('\\t')\n",
        "    if id2index.get(line[sid1_index]) is None or id2index.get(line[sid2_index]) is None:\n",
        "        continue\n",
        "    if id2_aid.get(line[sid1_index]) is None:\n",
        "        id2_aid[line[sid1_index]] = sid\n",
        "        sid += 1\n",
        "        seq_array.append(seqs[id2index[line[sid1_index]]])\n",
        "    line[sid1_index] = id2_aid[line[sid1_index]]\n",
        "    if id2_aid.get(line[sid2_index]) is None:\n",
        "        id2_aid[line[sid2_index]] = sid\n",
        "        sid += 1\n",
        "        seq_array.append(seqs[id2index[line[sid2_index]]])\n",
        "    line[sid2_index] = id2_aid[line[sid2_index]]\n",
        "    raw_data.append(line)\n",
        "    if limit_data:\n",
        "        count += 1\n",
        "        if count >= max_data:\n",
        "            break\n",
        "\n",
        "len_m_seq = np.array([len(line.split()) for line in seq_array])\n",
        "avg_m_seq = int(np.average(len_m_seq)) + 1\n",
        "max_m_seq = max(len_m_seq)\n",
        "\n",
        "# Random for distribution of class labels\n",
        "np.random.seed(42)\n",
        "np.random.shuffle(raw_data)\n",
        "seq_tensor = np.array([encode_seq(line) for line in tqdm(seq_array)])\n",
        "\n",
        "# Extract index of 1st and 2nd sequences in pairs\n",
        "seq_index1 = np.array([line[sid1_index] for line in tqdm(raw_data)])\n",
        "seq_index2 = np.array([line[sid2_index] for line in tqdm(raw_data)])\n",
        "\n",
        "# Create class labels\n",
        "class_labels = np.zeros((len(raw_data,)))\n",
        "for i in range(len(raw_data)):\n",
        "  class_labels[i] = float(raw_data[i][label_index])\n",
        "class_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Architecture of FSNN"
      ],
      "metadata": {
        "id": "paWVRDN40lw4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "def define_model():\n",
        "    \n",
        "    ########################################################\"Channel-1\" ########################################################\n",
        "    \n",
        "    input_1 = Input(shape=(366, ), name='Protein_a')\n",
        "    p1 = Dense(256, activation='relu', kernel_initializer='glorot_normal', name='ProA_feature_1', kernel_regularizer=l2(0.01))(input_1)\n",
        "    p1 = Dropout(.2)(p1)\n",
        "    p1_cos = Dense(256, activation='relu', kernel_initializer='glorot_normal', name='ProA_feature_1_cos', kernel_regularizer=l2(0.01))(tf.math.cos(math.pi*input_1))\n",
        "    p1_cos = Dropout(.2)(p1_cos)\n",
        "    p1_sin = Dense(256, activation='relu', kernel_initializer='glorot_normal', name='ProA_feature_1_sin', kernel_regularizer=l2(0.01))(tf.math.sin(math.pi*input_1))\n",
        "    p1_sin = Dropout(.2)(p1_sin)\n",
        "    p1 = tf.keras.layers.Add()([tf.keras.layers.Add()([p1, p1_cos]), p1_sin])\n",
        "    p1 = Dense(128, activation='relu', kernel_initializer='glorot_normal', name='ProA_feature_12', kernel_regularizer=l2(0.01))(p1)\n",
        "    p1 = Dropout(.2)(p1)\n",
        "    \n",
        "    ########################################################\"Channel-2\" ########################################################\n",
        "    \n",
        "   \n",
        "    input_2 = Input(shape=(366, ), name='Protein_b')\n",
        "    p2 = Dense(256, activation='relu', kernel_initializer='glorot_normal', name='ProB_feature_1', kernel_regularizer=l2(0.01))(input_2)\n",
        "    p2 = Dropout(.2)(p2)\n",
        "    p2_cos = Dense(256, activation='relu', kernel_initializer='glorot_normal', name='ProB_feature_1_cos', kernel_regularizer=l2(0.01))(tf.math.cos(math.pi*input_2))\n",
        "    p2_cos = Dropout(.2)(p2_cos)\n",
        "    p2_sin = Dense(256, activation='relu', kernel_initializer='glorot_normal', name='ProB_feature_1_sin', kernel_regularizer=l2(0.01))(tf.math.sin(math.pi*input_2))\n",
        "    p2_sin = Dropout(.2)(p2_sin)\n",
        "    p2 = tf.keras.layers.Add()([tf.keras.layers.Add()([p2, p2_cos]), p2_sin])\n",
        "    p2 = Dense(128, activation='relu', kernel_initializer='glorot_normal', name='ProB_feature_12', kernel_regularizer=l2(0.01))(p2)\n",
        "    p2 = Dropout(.2)(p2)\n",
        "\n",
        "\n",
        "    ##################################### Merge Abstraction features ##################################################\n",
        "    \n",
        "    # Hadamard multiplication\n",
        "    merged = tf.keras.layers.multiply([p1,p2], name='merged_protein1_2')\n",
        " \n",
        "    # Min-max scaling\n",
        "    merged = tf.divide(\n",
        "   tf.subtract(\n",
        "      merged, \n",
        "      tf.reduce_min(merged)\n",
        "   ), \n",
        "   tf.subtract(\n",
        "      tf.reduce_max(merged), \n",
        "      tf.reduce_min(merged)\n",
        "   )\n",
        ")\n",
        "    ##################################### Prediction Module ##########################################################\n",
        "\n",
        "    \n",
        "    pre_output = Dense(64, activation='relu', kernel_initializer='glorot_normal', name='Merged_feature_1')(merged)\n",
        "    # pre_output = Dense(32, activation='relu', kernel_initializer='glorot_normal', name='Merged_feature_2')(pre_output)\n",
        "    # pre_output = Dense(16, activation='relu', kernel_initializer='he_uniform', name='Merged_feature_3')(pre_output)\n",
        "\n",
        "    pre_output=Dropout(0.2)(pre_output)\n",
        "\n",
        "    output = Dense(1, activation='sigmoid', name='output')(pre_output)\n",
        "    model = Model(inputs=[input_1, input_2], outputs=output)\n",
        "   \n",
        "    sgd = SGD(learning_rate=0.01, momentum=0.9, decay=0.001)\n",
        "    model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "cIWepo66wCiT"
      },
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### FSNN cross-validation"
      ],
      "metadata": {
        "id": "Soj-MPVb6Hj1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##################################### Load Positive and Negative Dataset ##########################################################\n",
        "    \n",
        "# df_pos= pd.read_csv('PositiveYH.csv',header=None)\n",
        "# df_neg = pd.read_csv('NegativeYH.csv',header=None)\n",
        "#df_neg=df_neg.sample(n=min(len(df_pos),len(df_neg)))\n",
        "#df_pos=df_pos.sample(n=min(len(df_pos),len(df_neg)))\n",
        "# df_neg['Status'] = 0\n",
        "# df_pos['Status'] = 1\n",
        "# df_neg=df_neg.sample(n=len(df_pos))\n",
        "\n",
        "# df = pd.concat([df_pos,df_neg])\n",
        "# df = df.reset_index()\n",
        "# df=df.sample(frac=1)\n",
        "# df = df.iloc[:,1:]\n",
        "\n",
        "# X = df.iloc[:,0:1986].values\n",
        "\n",
        "\n",
        "# Create data frame of pairs and labels\n",
        "df = np.hstack([seq_tensor[seq_index1], seq_tensor[seq_index2]])\n",
        "df = np.hstack([df, class_labels.reshape(-1,1)])\n",
        "df = pd.DataFrame(df)\n",
        "df = df.sample(frac=1)\n",
        "\n",
        "# Create pairs matrix and label vector\n",
        "X = df.iloc[:,0:366*2].values\n",
        "y = df.iloc[:,366*2:].values\n",
        "\n",
        "Trainlabels=y\n",
        "\n",
        "# standard scaler\n",
        "scaler = StandardScaler().fit(X)\n",
        "\n",
        "# scaler = RobustScaler().fit(X)\n",
        "X = scaler.transform(X)\n",
        "\n",
        "\n",
        "X1_train = X[:, :366]\n",
        "X2_train = X[:, 366:]\n",
        "\n",
        "\n",
        "##################################### Five-fold Cross-Validation ##########################################################\n",
        "    \n",
        "kf=StratifiedKFold(n_splits=5)\n",
        "\n",
        "\n",
        "accuracy1 = []\n",
        "specificity1 = []\n",
        "sensitivity1 = []\n",
        "precision1=[]\n",
        "recall1=[]\n",
        "\n",
        "m_coef=[]\n",
        "dnn_fpr_list=[]\n",
        "dnn_tpr_list=[]\n",
        "dnn_auc_list = []\n",
        "o=0\n",
        "max_accuracy=float(\"-inf\")\n",
        "dnn_fpr=None\n",
        "dnn_tpr=None\n",
        "training_time = 1\n",
        "for train, test in kf.split(X,y):\n",
        "    global model\n",
        "    model=define_model()\n",
        "    o=o+1\n",
        "    print(f\"====================================== START TRAINING TIME {training_time} ==============================\")\n",
        "    model.fit([X1_train[train],X2_train[train]],y[train],epochs=50,batch_size=64,verbose=1)\n",
        "    print(f\"====================================== END TRAINING TIME {training_time} ==============================\")\n",
        "    y_test=y[test]\n",
        "    y_score = model.predict([X1_train[test],X2_train[test]])\n",
        "    \n",
        "    fpr, tpr, _= roc_curve(y_test,  y_score)\n",
        "    auc = metrics.roc_auc_score(y_test, y_score)\n",
        "    \n",
        "    dnn_auc_list.append(auc)\n",
        "    \n",
        "    y_score=y_score[:,0]\n",
        "    \n",
        "    for i in range(0,len(y_score)):\n",
        "        if(y_score[i]>0.5):\n",
        "            y_score[i]=1\n",
        "        else:\n",
        "            y_score[i]=0\n",
        "            \n",
        "    cm1=confusion_matrix(y[test][:,0],y_score)\n",
        "    acc1 = accuracy_score(y[test][:,0], y_score, sample_weight=None)\n",
        "    spec1= (cm1[0,0])/(cm1[0,0]+cm1[0,1])\n",
        "    sens1 = recall_score(y[test][:,0], y_score, sample_weight=None)\n",
        "    prec1=precision_score(y[test][:,0], y_score, sample_weight=None)\n",
        "    sensitivity1.append(sens1)\n",
        "    specificity1.append(spec1)\n",
        "    accuracy1.append(acc1)\n",
        "    precision1.append(prec1)\n",
        "    \n",
        "    coef=matthews_corrcoef(y[test], y_score, sample_weight=None)\n",
        "    m_coef.append(coef)\n",
        "    dnn_fpr_list.append(fpr)\n",
        "    dnn_tpr_list.append(tpr)\n",
        "    print(f\"============================== PERFORMANCE METRICS AT TRAINING TIME {training_time}==============================\")\n",
        "    print(f\"accuracy: {acc1}, specificity: {spec1}, sensitivity: {sens1}, precision= {prec1}, mcc-score: {coef} \")\n",
        "    if acc1>max_accuracy:\n",
        "        max_accuracy=acc1\n",
        "        dnn_fpr=fpr[:]\n",
        "        dnn_tpr=tpr[:]\n",
        "    training_time += 1\n",
        "\n",
        "\n",
        "dnn_fpr=pd.DataFrame(dnn_fpr)\n",
        "dnn_tpr=pd.DataFrame(dnn_tpr)\n",
        "dnn_fpr.to_csv('fprDNN.csv',header=False, index=False)\n",
        "dnn_tpr.to_csv('tprDNN.csv',header=False, index=False)\n",
        "\n",
        "\n",
        "mean_acc1=np.mean(accuracy1)\n",
        "std_acc1=np.std(accuracy1)\n",
        "var_acc1=np.var(accuracy1)\n",
        "print(\"Accuracy1:\"+str(mean_acc1)+\" Â± \"+str(std_acc1))\n",
        "print(\"Accuracy_Var:\"+str(mean_acc1)+\" Â± \"+str(var_acc1))\n",
        "mean_spec1=np.mean(specificity1)\n",
        "std_spec1=np.std(specificity1)\n",
        "print(\"Specificity1:\"+str(mean_spec1)+\" Â± \"+str(std_spec1))\n",
        "mean_sens1=np.mean(sensitivity1)\n",
        "std_sens1=np.std(sensitivity1)\n",
        "print(\"Sensitivity1:\"+str(mean_sens1)+\" Â± \"+str(std_sens1))\n",
        "mean_prec1=np.mean(precision1)\n",
        "std_prec1=np.std(precision1)\n",
        "print(\"Precison1:\"+str(mean_prec1)+\" Â± \"+str(std_prec1))\n",
        "\n",
        "mean_coef=np.mean(m_coef)\n",
        "std_coef=np.std(m_coef)\n",
        "print(\"MCC1:\"+str(mean_coef)+\" Â± \"+str(std_coef))\n",
        "\n",
        "print(\"AUC1:\"+str(np.mean(dnn_auc_list)))\n",
        "\n",
        "\n",
        "end1 = time.time()\n",
        "end11=end1 - start\n",
        "print(f\"Runtime of the program is {end1 - start}\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tBexzmnxVsBK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac6caf0b-c1e9-4000-ba97-f15d9cda794c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "====================================== START TRAINING TIME 1 ==============================\n",
            "Train on 8949 samples\n",
            "Epoch 1/50\n",
            "8949/8949 [==============================] - 2s 189us/sample - loss: 17.8337 - acc: 0.5083\n",
            "Epoch 2/50\n",
            "8949/8949 [==============================] - 1s 101us/sample - loss: 11.0368 - acc: 0.5126\n",
            "Epoch 3/50\n",
            "8949/8949 [==============================] - 1s 99us/sample - loss: 7.2801 - acc: 0.5517\n",
            "Epoch 4/50\n",
            "8949/8949 [==============================] - 1s 98us/sample - loss: 5.0809 - acc: 0.5694\n",
            "Epoch 5/50\n",
            "8949/8949 [==============================] - 1s 101us/sample - loss: 3.7176 - acc: 0.6039\n",
            "Epoch 6/50\n",
            "8949/8949 [==============================] - 1s 99us/sample - loss: 2.8258 - acc: 0.6537\n",
            "Epoch 7/50\n",
            "8949/8949 [==============================] - 1s 107us/sample - loss: 2.2166 - acc: 0.6934\n",
            "Epoch 8/50\n",
            "8949/8949 [==============================] - 1s 106us/sample - loss: 1.7769 - acc: 0.7393\n",
            "Epoch 9/50\n",
            "8949/8949 [==============================] - 1s 108us/sample - loss: 1.4468 - acc: 0.7947\n",
            "Epoch 10/50\n",
            "8949/8949 [==============================] - 1s 98us/sample - loss: 1.2125 - acc: 0.8332\n",
            "Epoch 11/50\n",
            "8949/8949 [==============================] - 1s 100us/sample - loss: 1.0151 - acc: 0.8666\n",
            "Epoch 12/50\n",
            "8949/8949 [==============================] - 1s 98us/sample - loss: 0.8568 - acc: 0.8933\n",
            "Epoch 13/50\n",
            "8949/8949 [==============================] - 1s 98us/sample - loss: 0.7461 - acc: 0.9087\n",
            "Epoch 14/50\n",
            "8949/8949 [==============================] - 1s 97us/sample - loss: 0.6405 - acc: 0.9194\n",
            "Epoch 15/50\n",
            "8949/8949 [==============================] - 1s 96us/sample - loss: 0.5642 - acc: 0.9303\n",
            "Epoch 16/50\n",
            "8949/8949 [==============================] - 1s 98us/sample - loss: 0.5128 - acc: 0.9325\n",
            "Epoch 17/50\n",
            "8949/8949 [==============================] - 1s 98us/sample - loss: 0.4500 - acc: 0.9407\n",
            "Epoch 18/50\n",
            "8949/8949 [==============================] - 1s 99us/sample - loss: 0.4049 - acc: 0.9459\n",
            "Epoch 19/50\n",
            "8949/8949 [==============================] - 1s 98us/sample - loss: 0.3754 - acc: 0.9466\n",
            "Epoch 20/50\n",
            "8949/8949 [==============================] - 1s 98us/sample - loss: 0.3452 - acc: 0.9528\n",
            "Epoch 21/50\n",
            "8949/8949 [==============================] - 1s 97us/sample - loss: 0.3295 - acc: 0.9555\n",
            "Epoch 22/50\n",
            "8949/8949 [==============================] - 1s 97us/sample - loss: 0.3133 - acc: 0.9611\n",
            "Epoch 23/50\n",
            "8949/8949 [==============================] - 1s 98us/sample - loss: 0.3028 - acc: 0.9628\n",
            "Epoch 24/50\n",
            "8949/8949 [==============================] - 1s 99us/sample - loss: 0.3103 - acc: 0.9560\n",
            "Epoch 25/50\n",
            "8949/8949 [==============================] - 1s 99us/sample - loss: 0.2950 - acc: 0.9614\n",
            "Epoch 26/50\n",
            "8949/8949 [==============================] - 1s 99us/sample - loss: 0.2732 - acc: 0.9697\n",
            "Epoch 27/50\n",
            "8949/8949 [==============================] - 1s 99us/sample - loss: 0.2551 - acc: 0.9728\n",
            "Epoch 28/50\n",
            "8949/8949 [==============================] - 1s 99us/sample - loss: 0.2388 - acc: 0.9771\n",
            "Epoch 29/50\n",
            "8949/8949 [==============================] - 1s 98us/sample - loss: 0.2232 - acc: 0.9806\n",
            "Epoch 30/50\n",
            "8949/8949 [==============================] - 1s 98us/sample - loss: 0.2120 - acc: 0.9798\n",
            "Epoch 31/50\n",
            "8949/8949 [==============================] - 1s 100us/sample - loss: 0.2057 - acc: 0.9801\n",
            "Epoch 32/50\n",
            "8949/8949 [==============================] - 1s 98us/sample - loss: 0.1916 - acc: 0.9819\n",
            "Epoch 33/50\n",
            "8949/8949 [==============================] - 1s 98us/sample - loss: 0.1978 - acc: 0.9807\n",
            "Epoch 34/50\n",
            "8949/8949 [==============================] - 1s 98us/sample - loss: 0.1967 - acc: 0.9778\n",
            "Epoch 35/50\n",
            "8949/8949 [==============================] - 1s 99us/sample - loss: 0.1888 - acc: 0.9817\n",
            "Epoch 36/50\n",
            "8949/8949 [==============================] - 1s 98us/sample - loss: 0.1874 - acc: 0.9794\n",
            "Epoch 37/50\n",
            "8949/8949 [==============================] - 1s 99us/sample - loss: 0.1825 - acc: 0.9806\n",
            "Epoch 38/50\n",
            "8949/8949 [==============================] - 1s 98us/sample - loss: 0.1816 - acc: 0.9817\n",
            "Epoch 39/50\n",
            "8949/8949 [==============================] - 1s 100us/sample - loss: 0.1754 - acc: 0.9818\n",
            "Epoch 40/50\n",
            "8949/8949 [==============================] - 1s 101us/sample - loss: 0.1833 - acc: 0.9801\n",
            "Epoch 41/50\n",
            "8949/8949 [==============================] - 1s 99us/sample - loss: 0.1714 - acc: 0.9828\n",
            "Epoch 42/50\n",
            "8949/8949 [==============================] - 1s 100us/sample - loss: 0.1670 - acc: 0.9819\n",
            "Epoch 43/50\n",
            "8949/8949 [==============================] - 1s 99us/sample - loss: 0.1583 - acc: 0.9855\n",
            "Epoch 44/50\n",
            "8949/8949 [==============================] - 1s 97us/sample - loss: 0.1611 - acc: 0.9845\n",
            "Epoch 45/50\n",
            "8949/8949 [==============================] - 1s 97us/sample - loss: 0.1633 - acc: 0.9844\n",
            "Epoch 46/50\n",
            "8949/8949 [==============================] - 1s 100us/sample - loss: 0.1444 - acc: 0.9880\n",
            "Epoch 47/50\n",
            "8949/8949 [==============================] - 1s 100us/sample - loss: 0.1354 - acc: 0.9897\n",
            "Epoch 48/50\n",
            "8949/8949 [==============================] - 1s 99us/sample - loss: 0.1374 - acc: 0.9869\n",
            "Epoch 49/50\n",
            "8949/8949 [==============================] - 1s 115us/sample - loss: 0.1303 - acc: 0.9907\n",
            "Epoch 50/50\n",
            "8949/8949 [==============================] - 1s 98us/sample - loss: 0.1293 - acc: 0.9871\n",
            "====================================== END TRAINING TIME 1 ==============================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================== PERFORMANCE METRICS AT TRAINING TIME 1==============================\n",
            "accuracy: 0.9052725647899911, specificity: 0.9168900804289544, sensitivity: 0.8936550491510277, precision= 0.9149130832570905, mcc-score: 0.8107640113626409 \n",
            "====================================== START TRAINING TIME 2 ==============================\n",
            "Train on 8949 samples\n",
            "Epoch 1/50\n",
            "8949/8949 [==============================] - 2s 184us/sample - loss: 17.8220 - acc: 0.5061\n",
            "Epoch 2/50\n",
            "8949/8949 [==============================] - 1s 98us/sample - loss: 11.0296 - acc: 0.5227\n",
            "Epoch 3/50\n",
            "8949/8949 [==============================] - 1s 100us/sample - loss: 7.2770 - acc: 0.5399\n",
            "Epoch 4/50\n",
            "8949/8949 [==============================] - 1s 97us/sample - loss: 5.0788 - acc: 0.5700\n",
            "Epoch 5/50\n",
            "8949/8949 [==============================] - 1s 97us/sample - loss: 3.7161 - acc: 0.6004\n",
            "Epoch 6/50\n",
            "8949/8949 [==============================] - 1s 99us/sample - loss: 2.8249 - acc: 0.6443\n",
            "Epoch 7/50\n",
            "8949/8949 [==============================] - 1s 99us/sample - loss: 2.2088 - acc: 0.7023\n",
            "Epoch 8/50\n",
            "8949/8949 [==============================] - 1s 99us/sample - loss: 1.7731 - acc: 0.7479\n",
            "Epoch 9/50\n",
            "8949/8949 [==============================] - 1s 98us/sample - loss: 1.4371 - acc: 0.8032\n",
            "Epoch 10/50\n",
            "8949/8949 [==============================] - 1s 103us/sample - loss: 1.1807 - acc: 0.8485\n",
            "Epoch 11/50\n",
            "8949/8949 [==============================] - 1s 102us/sample - loss: 0.9998 - acc: 0.8728\n",
            "Epoch 12/50\n",
            "8949/8949 [==============================] - 1s 100us/sample - loss: 0.8548 - acc: 0.8925\n",
            "Epoch 13/50\n",
            "8949/8949 [==============================] - 1s 106us/sample - loss: 0.7317 - acc: 0.9130\n",
            "Epoch 14/50\n",
            "8949/8949 [==============================] - 1s 99us/sample - loss: 0.6357 - acc: 0.9201\n",
            "Epoch 15/50\n",
            "8949/8949 [==============================] - 1s 98us/sample - loss: 0.5616 - acc: 0.9305\n",
            "Epoch 16/50\n",
            "8949/8949 [==============================] - 1s 98us/sample - loss: 0.5009 - acc: 0.9357\n",
            "Epoch 17/50\n",
            "8949/8949 [==============================] - 1s 99us/sample - loss: 0.4524 - acc: 0.9410\n",
            "Epoch 18/50\n",
            "8949/8949 [==============================] - 1s 101us/sample - loss: 0.4143 - acc: 0.9464\n",
            "Epoch 19/50\n",
            "8949/8949 [==============================] - 1s 99us/sample - loss: 0.3817 - acc: 0.9503\n",
            "Epoch 20/50\n",
            "8949/8949 [==============================] - 1s 99us/sample - loss: 0.3547 - acc: 0.9549\n",
            "Epoch 21/50\n",
            "8949/8949 [==============================] - 1s 98us/sample - loss: 0.3306 - acc: 0.9617\n",
            "Epoch 22/50\n",
            "8949/8949 [==============================] - 1s 98us/sample - loss: 0.3236 - acc: 0.9616\n",
            "Epoch 23/50\n",
            "8949/8949 [==============================] - 1s 98us/sample - loss: 0.3027 - acc: 0.9656\n",
            "Epoch 24/50\n",
            "8949/8949 [==============================] - 1s 99us/sample - loss: 0.2856 - acc: 0.9686\n",
            "Epoch 25/50\n",
            "8949/8949 [==============================] - 1s 99us/sample - loss: 0.2648 - acc: 0.9718\n",
            "Epoch 26/50\n",
            "8949/8949 [==============================] - 1s 100us/sample - loss: 0.2531 - acc: 0.9768\n",
            "Epoch 27/50\n",
            "8949/8949 [==============================] - 1s 98us/sample - loss: 0.2513 - acc: 0.9715\n",
            "Epoch 28/50\n",
            "8949/8949 [==============================] - 1s 98us/sample - loss: 0.2424 - acc: 0.9750\n",
            "Epoch 29/50\n",
            "8949/8949 [==============================] - 1s 97us/sample - loss: 0.2319 - acc: 0.9752\n",
            "Epoch 30/50\n",
            "8949/8949 [==============================] - 1s 98us/sample - loss: 0.2315 - acc: 0.9751\n",
            "Epoch 31/50\n",
            "8949/8949 [==============================] - 1s 99us/sample - loss: 0.2279 - acc: 0.9763\n",
            "Epoch 32/50\n",
            "8949/8949 [==============================] - 1s 98us/sample - loss: 0.2241 - acc: 0.9758\n",
            "Epoch 33/50\n",
            "8949/8949 [==============================] - 1s 97us/sample - loss: 0.2225 - acc: 0.9782\n",
            "Epoch 34/50\n",
            "8949/8949 [==============================] - 1s 99us/sample - loss: 0.2114 - acc: 0.9785\n",
            "Epoch 35/50\n",
            "8949/8949 [==============================] - 1s 99us/sample - loss: 0.2047 - acc: 0.9815\n",
            "Epoch 36/50\n",
            "8949/8949 [==============================] - 1s 99us/sample - loss: 0.1971 - acc: 0.9816\n",
            "Epoch 37/50\n",
            "8949/8949 [==============================] - 1s 98us/sample - loss: 0.1963 - acc: 0.9818\n",
            "Epoch 38/50\n",
            "8949/8949 [==============================] - 1s 98us/sample - loss: 0.1784 - acc: 0.9846\n",
            "Epoch 39/50\n",
            "8949/8949 [==============================] - 1s 98us/sample - loss: 0.1701 - acc: 0.9855\n",
            "Epoch 40/50\n",
            "8949/8949 [==============================] - 1s 98us/sample - loss: 0.1673 - acc: 0.9860\n",
            "Epoch 41/50\n",
            "8949/8949 [==============================] - 1s 98us/sample - loss: 0.1620 - acc: 0.9848\n",
            "Epoch 42/50\n",
            "8949/8949 [==============================] - 1s 98us/sample - loss: 0.1515 - acc: 0.9868\n",
            "Epoch 43/50\n",
            "8949/8949 [==============================] - 1s 97us/sample - loss: 0.1458 - acc: 0.9877\n",
            "Epoch 44/50\n",
            "8949/8949 [==============================] - 1s 98us/sample - loss: 0.1405 - acc: 0.9890\n",
            "Epoch 45/50\n",
            "8949/8949 [==============================] - 1s 98us/sample - loss: 0.1332 - acc: 0.9888\n",
            "Epoch 46/50\n",
            "8949/8949 [==============================] - 1s 98us/sample - loss: 0.1369 - acc: 0.9869\n",
            "Epoch 47/50\n",
            "8949/8949 [==============================] - 1s 101us/sample - loss: 0.1474 - acc: 0.9849\n",
            "Epoch 48/50\n",
            "8949/8949 [==============================] - 1s 98us/sample - loss: 0.1375 - acc: 0.9879\n",
            "Epoch 49/50\n",
            "8949/8949 [==============================] - 1s 98us/sample - loss: 0.1431 - acc: 0.9854\n",
            "Epoch 50/50\n",
            "8949/8949 [==============================] - 1s 97us/sample - loss: 0.1382 - acc: 0.9875\n",
            "====================================== END TRAINING TIME 2 ==============================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================== PERFORMANCE METRICS AT TRAINING TIME 2==============================\n",
            "accuracy: 0.9146559428060769, specificity: 0.9231456657730116, sensitivity: 0.9061662198391421, precision= 0.9218181818181819, mcc-score: 0.8294314574325516 \n",
            "====================================== START TRAINING TIME 3 ==============================\n",
            "Train on 8950 samples\n",
            "Epoch 1/50\n",
            "8950/8950 [==============================] - 2s 184us/sample - loss: 17.8530 - acc: 0.5067\n",
            "Epoch 2/50\n",
            "8950/8950 [==============================] - 1s 99us/sample - loss: 11.0481 - acc: 0.5269\n",
            "Epoch 3/50\n",
            "8950/8950 [==============================] - 1s 98us/sample - loss: 7.2880 - acc: 0.5449\n",
            "Epoch 4/50\n",
            "8950/8950 [==============================] - 1s 98us/sample - loss: 5.0876 - acc: 0.5651\n",
            "Epoch 5/50\n",
            "8950/8950 [==============================] - 1s 99us/sample - loss: 3.7250 - acc: 0.6008\n",
            "Epoch 6/50\n",
            "8950/8950 [==============================] - 1s 98us/sample - loss: 2.8369 - acc: 0.6409\n",
            "Epoch 7/50\n",
            "8950/8950 [==============================] - 1s 99us/sample - loss: 2.2267 - acc: 0.6878\n",
            "Epoch 8/50\n",
            "8950/8950 [==============================] - 1s 99us/sample - loss: 1.7874 - acc: 0.7344\n",
            "Epoch 9/50\n",
            "8950/8950 [==============================] - 1s 99us/sample - loss: 1.4634 - acc: 0.7855\n",
            "Epoch 10/50\n",
            "8950/8950 [==============================] - 1s 98us/sample - loss: 1.2188 - acc: 0.8231\n",
            "Epoch 11/50\n",
            "8950/8950 [==============================] - 1s 100us/sample - loss: 1.0401 - acc: 0.8518\n",
            "Epoch 12/50\n",
            "8950/8950 [==============================] - 1s 99us/sample - loss: 0.8746 - acc: 0.8847\n",
            "Epoch 13/50\n",
            "8950/8950 [==============================] - 1s 100us/sample - loss: 0.7549 - acc: 0.9001\n",
            "Epoch 14/50\n",
            "8950/8950 [==============================] - 1s 102us/sample - loss: 0.6577 - acc: 0.9155\n",
            "Epoch 15/50\n",
            "8950/8950 [==============================] - 1s 111us/sample - loss: 0.5736 - acc: 0.9263\n",
            "Epoch 16/50\n",
            "8950/8950 [==============================] - 1s 99us/sample - loss: 0.5083 - acc: 0.9360\n",
            "Epoch 17/50\n",
            "8950/8950 [==============================] - 1s 103us/sample - loss: 0.4658 - acc: 0.9387\n",
            "Epoch 18/50\n",
            "8950/8950 [==============================] - 1s 100us/sample - loss: 0.4292 - acc: 0.9410\n",
            "Epoch 19/50\n",
            "8950/8950 [==============================] - 1s 99us/sample - loss: 0.3950 - acc: 0.9479\n",
            "Epoch 20/50\n",
            "8950/8950 [==============================] - 1s 99us/sample - loss: 0.3640 - acc: 0.9545\n",
            "Epoch 21/50\n",
            "8950/8950 [==============================] - 1s 99us/sample - loss: 0.3405 - acc: 0.9618\n",
            "Epoch 22/50\n",
            "8950/8950 [==============================] - 1s 99us/sample - loss: 0.3133 - acc: 0.9644\n",
            "Epoch 23/50\n",
            "8950/8950 [==============================] - 1s 99us/sample - loss: 0.3013 - acc: 0.9648\n",
            "Epoch 24/50\n",
            "8950/8950 [==============================] - 1s 102us/sample - loss: 0.2776 - acc: 0.9702\n",
            "Epoch 25/50\n",
            "8950/8950 [==============================] - 1s 99us/sample - loss: 0.2696 - acc: 0.9716\n",
            "Epoch 26/50\n",
            "8950/8950 [==============================] - 1s 99us/sample - loss: 0.2569 - acc: 0.9709\n",
            "Epoch 27/50\n",
            "8950/8950 [==============================] - 1s 99us/sample - loss: 0.2553 - acc: 0.9722\n",
            "Epoch 28/50\n",
            "8950/8950 [==============================] - 1s 100us/sample - loss: 0.2479 - acc: 0.9743\n",
            "Epoch 29/50\n",
            "8950/8950 [==============================] - 1s 100us/sample - loss: 0.2492 - acc: 0.9714\n",
            "Epoch 30/50\n",
            "8950/8950 [==============================] - 1s 98us/sample - loss: 0.2373 - acc: 0.9762\n",
            "Epoch 31/50\n",
            "8950/8950 [==============================] - 1s 100us/sample - loss: 0.2253 - acc: 0.9760\n",
            "Epoch 32/50\n",
            "8950/8950 [==============================] - 1s 98us/sample - loss: 0.2156 - acc: 0.9789\n",
            "Epoch 33/50\n",
            "8950/8950 [==============================] - 1s 101us/sample - loss: 0.2081 - acc: 0.9794\n",
            "Epoch 34/50\n",
            "8950/8950 [==============================] - 1s 100us/sample - loss: 0.2064 - acc: 0.9803\n",
            "Epoch 35/50\n",
            "8950/8950 [==============================] - 1s 102us/sample - loss: 0.1933 - acc: 0.9825\n",
            "Epoch 36/50\n",
            "8950/8950 [==============================] - 1s 103us/sample - loss: 0.1939 - acc: 0.9809\n",
            "Epoch 37/50\n",
            "8950/8950 [==============================] - 1s 102us/sample - loss: 0.1892 - acc: 0.9818\n",
            "Epoch 38/50\n",
            "8950/8950 [==============================] - 1s 100us/sample - loss: 0.1732 - acc: 0.9840\n",
            "Epoch 39/50\n",
            "8950/8950 [==============================] - 1s 99us/sample - loss: 0.1751 - acc: 0.9813\n",
            "Epoch 40/50\n",
            "8950/8950 [==============================] - 1s 101us/sample - loss: 0.1799 - acc: 0.9812\n",
            "Epoch 41/50\n",
            "8950/8950 [==============================] - 1s 100us/sample - loss: 0.1764 - acc: 0.9802\n",
            "Epoch 42/50\n",
            "8950/8950 [==============================] - 1s 101us/sample - loss: 0.1622 - acc: 0.9839\n",
            "Epoch 43/50\n",
            "8950/8950 [==============================] - 1s 105us/sample - loss: 0.1564 - acc: 0.9851\n",
            "Epoch 44/50\n",
            "8950/8950 [==============================] - 1s 99us/sample - loss: 0.1568 - acc: 0.9844\n",
            "Epoch 45/50\n",
            "8950/8950 [==============================] - 1s 101us/sample - loss: 0.1565 - acc: 0.9858\n",
            "Epoch 46/50\n",
            "8950/8950 [==============================] - 1s 101us/sample - loss: 0.1463 - acc: 0.9874\n",
            "Epoch 47/50\n",
            "8950/8950 [==============================] - 1s 100us/sample - loss: 0.1446 - acc: 0.9878\n",
            "Epoch 48/50\n",
            "8950/8950 [==============================] - 1s 101us/sample - loss: 0.1371 - acc: 0.9878\n",
            "Epoch 49/50\n",
            "8950/8950 [==============================] - 1s 99us/sample - loss: 0.1271 - acc: 0.9903\n",
            "Epoch 50/50\n",
            "8950/8950 [==============================] - 1s 99us/sample - loss: 0.1233 - acc: 0.9902\n",
            "====================================== END TRAINING TIME 3 ==============================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================== PERFORMANCE METRICS AT TRAINING TIME 3==============================\n",
            "accuracy: 0.9217702279839071, specificity: 0.934763181411975, sensitivity: 0.9087656529516994, precision= 0.9329660238751147, mcc-score: 0.843822290965836 \n",
            "====================================== START TRAINING TIME 4 ==============================\n",
            "Train on 8950 samples\n",
            "Epoch 1/50\n",
            "8950/8950 [==============================] - 2s 188us/sample - loss: 17.8135 - acc: 0.5046\n",
            "Epoch 2/50\n",
            "8950/8950 [==============================] - 1s 101us/sample - loss: 11.0245 - acc: 0.5219\n",
            "Epoch 3/50\n",
            "8950/8950 [==============================] - 1s 99us/sample - loss: 7.2730 - acc: 0.5369\n",
            "Epoch 4/50\n",
            "8950/8950 [==============================] - 1s 99us/sample - loss: 5.0770 - acc: 0.5699\n",
            "Epoch 5/50\n",
            "8950/8950 [==============================] - 1s 101us/sample - loss: 3.7150 - acc: 0.6003\n",
            "Epoch 6/50\n",
            "8950/8950 [==============================] - 1s 99us/sample - loss: 2.8269 - acc: 0.6449\n",
            "Epoch 7/50\n",
            "8950/8950 [==============================] - 1s 99us/sample - loss: 2.2212 - acc: 0.6874\n",
            "Epoch 8/50\n",
            "8950/8950 [==============================] - 1s 99us/sample - loss: 1.7783 - acc: 0.7484\n",
            "Epoch 9/50\n",
            "8950/8950 [==============================] - 1s 99us/sample - loss: 1.4496 - acc: 0.7912\n",
            "Epoch 10/50\n",
            "8950/8950 [==============================] - 1s 99us/sample - loss: 1.1992 - acc: 0.8361\n",
            "Epoch 11/50\n",
            "8950/8950 [==============================] - 1s 100us/sample - loss: 1.0105 - acc: 0.8684\n",
            "Epoch 12/50\n",
            "8950/8950 [==============================] - 1s 99us/sample - loss: 0.8546 - acc: 0.8944\n",
            "Epoch 13/50\n",
            "8950/8950 [==============================] - 1s 100us/sample - loss: 0.7329 - acc: 0.9114\n",
            "Epoch 14/50\n",
            "8950/8950 [==============================] - 1s 100us/sample - loss: 0.6443 - acc: 0.9188\n",
            "Epoch 15/50\n",
            "8950/8950 [==============================] - 1s 100us/sample - loss: 0.5668 - acc: 0.9292\n",
            "Epoch 16/50\n",
            "8950/8950 [==============================] - 1s 101us/sample - loss: 0.5125 - acc: 0.9369\n",
            "Epoch 17/50\n",
            "8950/8950 [==============================] - 1s 98us/sample - loss: 0.4614 - acc: 0.9451\n",
            "Epoch 18/50\n",
            "8950/8950 [==============================] - 1s 99us/sample - loss: 0.4129 - acc: 0.9543\n",
            "Epoch 19/50\n",
            "8950/8950 [==============================] - 1s 98us/sample - loss: 0.3803 - acc: 0.9561\n",
            "Epoch 20/50\n",
            "8950/8950 [==============================] - 1s 98us/sample - loss: 0.3657 - acc: 0.9546\n",
            "Epoch 21/50\n",
            "8950/8950 [==============================] - 1s 99us/sample - loss: 0.3350 - acc: 0.9602\n",
            "Epoch 22/50\n",
            "8950/8950 [==============================] - 1s 99us/sample - loss: 0.3100 - acc: 0.9677\n",
            "Epoch 23/50\n",
            "8950/8950 [==============================] - 1s 102us/sample - loss: 0.2999 - acc: 0.9669\n",
            "Epoch 24/50\n",
            "8950/8950 [==============================] - 1s 101us/sample - loss: 0.2815 - acc: 0.9715\n",
            "Epoch 25/50\n",
            "8950/8950 [==============================] - 1s 98us/sample - loss: 0.2733 - acc: 0.9698\n",
            "Epoch 26/50\n",
            "8950/8950 [==============================] - 1s 98us/sample - loss: 0.2647 - acc: 0.9726\n",
            "Epoch 27/50\n",
            "8950/8950 [==============================] - 1s 101us/sample - loss: 0.2530 - acc: 0.9747\n",
            "Epoch 28/50\n",
            "8950/8950 [==============================] - 1s 99us/sample - loss: 0.2442 - acc: 0.9754\n",
            "Epoch 29/50\n",
            "8950/8950 [==============================] - 1s 100us/sample - loss: 0.2255 - acc: 0.9780\n",
            "Epoch 30/50\n",
            "8950/8950 [==============================] - 1s 99us/sample - loss: 0.2236 - acc: 0.9782\n",
            "Epoch 31/50\n",
            "8950/8950 [==============================] - 1s 103us/sample - loss: 0.2137 - acc: 0.9775\n",
            "Epoch 32/50\n",
            "8950/8950 [==============================] - 1s 98us/sample - loss: 0.2037 - acc: 0.9800\n",
            "Epoch 33/50\n",
            "8950/8950 [==============================] - 1s 98us/sample - loss: 0.1956 - acc: 0.9808\n",
            "Epoch 34/50\n",
            "8950/8950 [==============================] - 1s 99us/sample - loss: 0.1980 - acc: 0.9785\n",
            "Epoch 35/50\n",
            "8950/8950 [==============================] - 1s 99us/sample - loss: 0.1936 - acc: 0.9801\n",
            "Epoch 36/50\n",
            "8950/8950 [==============================] - 1s 99us/sample - loss: 0.1849 - acc: 0.9813\n",
            "Epoch 37/50\n",
            "8950/8950 [==============================] - 1s 98us/sample - loss: 0.1852 - acc: 0.9809\n",
            "Epoch 38/50\n",
            "8950/8950 [==============================] - 1s 98us/sample - loss: 0.1929 - acc: 0.9773\n",
            "Epoch 39/50\n",
            "8950/8950 [==============================] - 1s 98us/sample - loss: 0.1714 - acc: 0.9846\n",
            "Epoch 40/50\n",
            "8950/8950 [==============================] - 1s 99us/sample - loss: 0.1678 - acc: 0.9837\n",
            "Epoch 41/50\n",
            "8950/8950 [==============================] - 1s 99us/sample - loss: 0.1689 - acc: 0.9829\n",
            "Epoch 42/50\n",
            "8950/8950 [==============================] - 1s 98us/sample - loss: 0.1690 - acc: 0.9826\n",
            "Epoch 43/50\n",
            "8950/8950 [==============================] - 1s 99us/sample - loss: 0.1690 - acc: 0.9819\n",
            "Epoch 44/50\n",
            "8950/8950 [==============================] - 1s 99us/sample - loss: 0.1579 - acc: 0.9863\n",
            "Epoch 45/50\n",
            "8950/8950 [==============================] - 1s 100us/sample - loss: 0.1494 - acc: 0.9857\n",
            "Epoch 46/50\n",
            "8950/8950 [==============================] - 1s 99us/sample - loss: 0.1477 - acc: 0.9876\n",
            "Epoch 47/50\n",
            "8950/8950 [==============================] - 1s 98us/sample - loss: 0.1485 - acc: 0.9858\n",
            "Epoch 48/50\n",
            "8950/8950 [==============================] - 1s 98us/sample - loss: 0.1390 - acc: 0.9875\n",
            "Epoch 49/50\n",
            "8950/8950 [==============================] - 1s 99us/sample - loss: 0.1399 - acc: 0.9870\n",
            "Epoch 50/50\n",
            "8950/8950 [==============================] - 1s 99us/sample - loss: 0.1413 - acc: 0.9891\n",
            "====================================== END TRAINING TIME 4 ==============================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================== PERFORMANCE METRICS AT TRAINING TIME 4==============================\n",
            "accuracy: 0.907912382655342, specificity: 0.938337801608579, sensitivity: 0.8774597495527728, precision= 0.9342857142857143, mcc-score: 0.8173316813901916 \n",
            "====================================== START TRAINING TIME 5 ==============================\n",
            "Train on 8950 samples\n",
            "Epoch 1/50\n",
            "8950/8950 [==============================] - 2s 189us/sample - loss: 17.8391 - acc: 0.5012\n",
            "Epoch 2/50\n",
            "3136/8950 [=========>....................] - ETA: 0s - loss: 12.6775 - acc: 0.5096"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extract last hidden layer of FSNN for LGBM classfier input"
      ],
      "metadata": {
        "id": "32c-jPrr0eFk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ERM1r-YYyvE",
        "outputId": "635a68d8-1f0f-4722-c47b-f583a658ef86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        }
      ],
      "source": [
        "################################Intermediate Layer prediction (Abstraction features extraction)######################################\n",
        "intermediate_layer_model = Model(inputs=model.input,outputs=model.get_layer('Merged_feature_1').output)\n",
        "\n",
        "# Use intermediate layer to transform pairs matrix\n",
        "intermediate_output_p1 = intermediate_layer_model.predict([X1_train,X2_train])  \n",
        "p_merge=pd.DataFrame(intermediate_output_p1)    \n",
        "\n",
        "# create dataframe use transformed pairs matrix outputs and labels\n",
        "X_train_feat=pd.concat((p_merge,pd.DataFrame(pd.DataFrame(Trainlabels))),axis=1,ignore_index=True)\n",
        "\n",
        "# write to file dataframe of transformed pairs matrix and labels\n",
        "X_train_feat.to_csv('X_train.csv',header=False, index=False)\n",
        "\n",
        "# read dataframe of transformed pairs matrix and labels\n",
        "Train=pd.read_csv(\"X_train.csv\",header=None)\n",
        "Train=Train.sample(frac=1)\n",
        "\n",
        "X=Train.iloc[:,0:64].values\n",
        "y=Train.iloc[:,64:].values\n",
        "\n",
        "extracted_df=X_train_feat\n",
        "\n",
        "scaler=RobustScaler()\n",
        "# scaler=MinMaxScaler()\n",
        "X=scaler.fit_transform(X)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### FSNN-LGBM cross-validation"
      ],
      "metadata": {
        "id": "WcgtJJqW50Uk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2U0tm0dKavPi",
        "outputId": "474c34a1-701f-4d44-f2ee-8aaab7b3aa92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========================== START TRAINING TIME 1 ===========================\n",
            "========================== PERFOMANCE METRICS AT TRAINING TIME 1 ===========================\n",
            "accuracy: 0.9740840035746202, specificity: 0.9758713136729222, sensitivity: 0.9722966934763181, precision= 0.9757847533632287, recall: 0.9722966934763181 \n",
            "========================== START TRAINING TIME 2 ===========================\n",
            "========================== PERFOMANCE METRICS AT TRAINING TIME 2 ===========================\n",
            "accuracy: 0.9789991063449508, specificity: 0.9812332439678284, sensitivity: 0.9767649687220733, precision= 0.981149012567325, recall: 0.9767649687220733 \n",
            "========================== START TRAINING TIME 3 ===========================\n",
            "========================== PERFOMANCE METRICS AT TRAINING TIME 3 ===========================\n",
            "accuracy: 0.9807778274474743, specificity: 0.9838998211091234, sensitivity: 0.9776586237712243, precision= 0.9838129496402878, recall: 0.9776586237712243 \n",
            "========================== START TRAINING TIME 4 ===========================\n",
            "========================== PERFOMANCE METRICS AT TRAINING TIME 4 ===========================\n",
            "accuracy: 0.981671881984801, specificity: 0.9874888293118856, sensitivity: 0.9758497316636852, precision= 0.9873303167420815, recall: 0.9758497316636852 \n",
            "========================== START TRAINING TIME 5 ===========================\n",
            "========================== PERFOMANCE METRICS AT TRAINING TIME 5 ===========================\n",
            "accuracy: 0.9718372820742065, specificity: 0.9767649687220733, sensitivity: 0.9669051878354203, precision= 0.976513098464318, recall: 0.9669051878354203 \n",
            "============MAX ACCURACY  0.981671881984801================\n",
            "Accuracy:0.9774740202852106 Â± 0.003850405252993588\n",
            "Accuracy_Var:0.9774740202852106 Â± 1.482562061228062e-05\n",
            "Specificity:0.9810516353567665 Â± 0.0043541670470587665\n",
            "Sensitivity:0.9738950410937444 Â± 0.0039404217557343565\n",
            "Precison:0.9809180261554482 Â± 0.004365897039473107\n",
            "Recall:0.9738950410937444 Â± 0.0039404217557343565\n",
            "MCC:0.9549770026687586 Â± 0.007704641938909037\n",
            "AUC:0.9923361591586012\n"
          ]
        }
      ],
      "source": [
        "\n",
        "##################################### Five-fold Cross-Validation ##########################################################\n",
        "\n",
        "kf=StratifiedKFold(n_splits=5)\n",
        "\n",
        "\n",
        "accuracy = []\n",
        "specificity = []\n",
        "sensitivity = []\n",
        "precision=[]\n",
        "recall=[]\n",
        "m_coef=[]\n",
        "\n",
        "auc_list=[]\n",
        "xgb_fpr_list=[]\n",
        "xgb_tpr_list=[]\n",
        "o=0\n",
        "max_accuracy=float(\"-inf\")\n",
        "xgb_fpr=None\n",
        "xgb_tpr=None\n",
        "\n",
        "y = y.reshape(-1, )\n",
        "training_time = 1\n",
        "for train, test in kf.split(X,y):\n",
        "    o=o+1\n",
        "    model_=LGBMClassifier(learning_rate=.2, gamma=0, max_depth=10, n_estimators=1000)\n",
        "    # model_=XGBClassifier(n_estimators=100)\n",
        "    print(f\"========================== START TRAINING TIME {training_time} ===========================\")\n",
        "    hist=model_.fit(X[train], y[train],eval_set=[(X[test], y[test])],verbose=False)\n",
        "    # hist = lgb.train(param, train_data, num_round, valid_sets=[validation_data])\n",
        "    y_score=model_.predict_proba(X[test])\n",
        "    y_test=tf.keras.utils.to_categorical(y[test]) \n",
        "    \n",
        "    fpr, tpr, _ = roc_curve(y_test[:,0].ravel(), y_score[:,0].ravel())\n",
        "    auc = metrics.roc_auc_score(y_test, y_score)\n",
        "    auc_list.append(auc)\n",
        "    coef=matthews_corrcoef(y_test.argmax(axis=1), y_score.argmax(axis=1), sample_weight=None)\n",
        "    m_coef.append(coef)\n",
        "    \n",
        "    cm1=confusion_matrix(y_test.argmax(axis=1), y_score.argmax(axis=1))\n",
        "    acc = (cm1[0,0]+cm1[1,1])/(cm1[0,0]+cm1[0,1]+cm1[1,0]+cm1[1,1])\n",
        "    spec= (cm1[0,0])/(cm1[0,0]+cm1[0,1])\n",
        "    sens = (cm1[1,1])/(cm1[1,0]+cm1[1,1])\n",
        "    prec=cm1[1,1]/(cm1[1,1]+cm1[0,1])\n",
        "    rec=cm1[1,1]/(cm1[1,1]+cm1[1,0])\n",
        "    sensitivity.append(sens)\n",
        "    specificity.append(spec)\n",
        "    accuracy.append(acc)\n",
        "    precision.append(prec)\n",
        "    recall.append(rec)\n",
        "    xgb_fpr_list.append(fpr)\n",
        "    xgb_tpr_list.append(tpr)\n",
        "    print(f\"========================== PERFOMANCE METRICS AT TRAINING TIME {training_time} ===========================\")\n",
        "    print(f\"accuracy: {acc}, specificity: {spec}, sensitivity: {sens}, precision= {prec}, recall: {rec} \")\n",
        "    if max_accuracy<acc:\n",
        "        max_accuracy=acc\n",
        "        xgb_fpr=fpr\n",
        "        xgb_tpr=tpr\n",
        "    training_time += 1\n",
        "print(f'============MAX ACCURACY  {max_accuracy}================')\n",
        "        \n",
        "\n",
        "xgb_fpr=pd.DataFrame(xgb_fpr)\n",
        "xgb_tpr=pd.DataFrame(xgb_tpr)\n",
        "\n",
        "xgb_fpr.to_csv('fprdnn_xgb.csv',header=False, index=False)\n",
        "xgb_tpr.to_csv('tprdnn_xgb.csv',header=False, index=False)   \n",
        " \n",
        "mean_acc=np.mean(accuracy)\n",
        "std_acc=np.std(accuracy)\n",
        "var_acc=np.var(accuracy)\n",
        "print(\"Accuracy:\"+str(mean_acc)+\" Â± \"+str(std_acc))\n",
        "print(\"Accuracy_Var:\"+str(mean_acc)+\" Â± \"+str(var_acc))\n",
        "mean_spec=np.mean(specificity)\n",
        "std_spec=np.std(specificity)\n",
        "print(\"Specificity:\"+str(mean_spec)+\" Â± \"+str(std_spec))\n",
        "mean_sens=np.mean(sensitivity)\n",
        "std_sens=np.std(sensitivity)\n",
        "print(\"Sensitivity:\"+str(mean_sens)+\" Â± \"+str(std_sens))\n",
        "mean_prec=np.mean(precision)\n",
        "std_prec=np.std(precision)\n",
        "print(\"Precison:\"+str(mean_prec)+\" Â± \"+str(std_prec))\n",
        "mean_rec=np.mean(recall)\n",
        "std_rec=np.std(recall)\n",
        "print(\"Recall:\"+str(mean_rec)+\" Â± \"+str(std_rec))\n",
        "mean_coef=np.mean(m_coef)\n",
        "std_coef=np.std(m_coef)\n",
        "print(\"MCC:\"+str(mean_coef)+\" Â± \"+str(std_coef))\n",
        "\n",
        "print(\"AUC:\"+str(np.mean(auc_list)))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### t-sne visualization"
      ],
      "metadata": {
        "id": "o2HHTynX6A-g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "######################################## ROC Curve plot ###############################################\n",
        "\n",
        "def ROC_dnn():\n",
        "    plt.figure(figsize=(3,2),dpi=300)\n",
        "    \n",
        "    plt.plot(dnn_fpr.values,dnn_tpr.values)\n",
        "    plt.legend()\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')        \n",
        "    \n",
        "    plt.title(\"ROC Curve for DNN\")\n",
        "    plt.show()     \n",
        "\n",
        "def ROC_dnn_xgb(): # Enter ROC_dnn_xgb() in console to see the roc-auc plot for XGB Classifier\n",
        "    plt.figure(figsize=(3,2),dpi=300)\n",
        "    plt.plot(xgb_fpr.values,xgb_tpr.values)   \n",
        "    plt.legend()\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')           \n",
        "    plt.title(\"ROC Curve for DNN_XGB Classifier\")\n",
        "    plt.show()\n",
        "    \n",
        "\n",
        "######################################## TSNE plot ###############################################\n",
        "\n",
        "def TSNE_raw():\n",
        "    global raw_data\n",
        "    # raw_data= pd.concat([df_pos,df_neg])\n",
        "    # raw_data=raw_data.iloc[:,:-1]\n",
        "    raw_data = np.hstack([seq_tensor[seq_index1], seq_tensor[seq_index2]])\n",
        "    t=TSNE(n_components=2).fit_transform(raw_data)\n",
        "    pos_t=t[:int(len(t)/2),:]\n",
        "    neg_t=t[int(len(t)/2):,:]\n",
        "    plt.scatter(pos_t[:,0],pos_t[:,1],label=\"Positive\",s=4)\n",
        "    plt.scatter(neg_t[:,0],neg_t[:,1],label=\"Negative\",s=4)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "TSNE_raw()\n",
        "\n",
        "def TSNE_extracted():\n",
        "    \n",
        "    pos=extracted_df[extracted_df.iloc[:,-1]==1]\n",
        "    neg=extracted_df[extracted_df.iloc[:,-1]==0]\n",
        "    X_feat=pd.concat([pos,neg])\n",
        "    X_feat=X_feat.iloc[:,:-1]\n",
        "    t=TSNE(n_components=2).fit_transform(X_feat)\n",
        "    pos_t=t[:int(len(t)/2),:]\n",
        "    neg_t=t[int(len(t)/2):,:]\n",
        "    plt.scatter(pos_t[:,0],pos_t[:,1],label=\"Positive\",s=4)\n",
        "    plt.scatter(neg_t[:,0],neg_t[:,1],label=\"Negative\",s=4)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "TSNE_extracted()"
      ],
      "metadata": {
        "id": "_2Wxv1G26Ali"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "FSNN-LGBM.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPFW0DOwkdac6quUjYIpLbN",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}