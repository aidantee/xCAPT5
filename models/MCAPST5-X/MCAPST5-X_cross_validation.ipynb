{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5D-q9BYzoVvO"
   },
   "source": [
    "### Check GPU hardware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RRoY6MKt1aeI",
    "outputId": "a583466a-d3b1-4857-c9ef-214cb43dc409"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Jun  3 07:36:37 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM...  On   | 00000000:9B:00.0 Off |                    0 |\n",
      "| N/A   28C    P0    67W / 400W |   7411MiB / 81920MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gnuZk-s1piRk"
   },
   "source": [
    "### Install requirements for ProtBert embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QyQ4uyiWphmv",
    "outputId": "200ec83b-fa7d-4890-f6f8-3733e41a18e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (1.9.0+cu111)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.29.2)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.8/dist-packages (0.1.99)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.8/dist-packages (3.8.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch) (4.5.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2023.5.5)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.12.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.15.1)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.5.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers) (2019.11.28)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.1.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->transformers) (1.25.8)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch transformers sentencepiece h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uQZ5i52i3oMH",
    "outputId": "18767b02-4ddf-4a09-fb21-7d307d2623cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Requirement already satisfied: torch==1.9.0+cu111 in /usr/local/lib/python3.8/dist-packages (1.9.0+cu111)\n",
      "Requirement already satisfied: torchvision==0.10.0+cu111 in /usr/local/lib/python3.8/dist-packages (0.10.0+cu111)\n",
      "Requirement already satisfied: torchaudio==0.9.0 in /usr/local/lib/python3.8/dist-packages (0.9.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch==1.9.0+cu111) (4.5.0)\n",
      "Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision==0.10.0+cu111) (9.5.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision==0.10.0+cu111) (1.23.5)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch==1.9.0+cu111 torchvision==0.10.0+cu111 torchaudio==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FBsAQxOcpxj-"
   },
   "source": [
    "### Set up embedding directories "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v7xhz60CprsI",
    "outputId": "4eab0270-04b0-4394-e17c-7591a2bfa031"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-06-03 07:36:45--  https://rostlab.org/~deepppi/example_seqs.fasta\n",
      "Resolving rostlab.org (rostlab.org)... 131.159.28.73\n",
      "Connecting to rostlab.org (rostlab.org)|131.159.28.73|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 627\n",
      "Saving to: ‘protT5/example_seqs.fasta’\n",
      "\n",
      "example_seqs.fasta  100%[===================>]     627  --.-KB/s    in 0s      \n",
      "\n",
      "2023-06-03 07:36:46 (235 MB/s) - ‘protT5/example_seqs.fasta’ saved [627/627]\n",
      "\n",
      "--2023-06-03 07:36:46--  http://data.bioembeddings.com/public/embeddings/feature_models/t5/secstruct_checkpoint.pt\n",
      "Resolving data.bioembeddings.com (data.bioembeddings.com)... 143.95.108.236\n",
      "Connecting to data.bioembeddings.com (data.bioembeddings.com)|143.95.108.236|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3727951 (3.6M)\n",
      "Saving to: ‘protT5/sec_struct_checkpoint/secstruct_checkpoint.pt’\n",
      "\n",
      "secstruct_checkpoin 100%[===================>]   3.55M  2.21MB/s    in 1.6s    \n",
      "\n",
      "2023-06-03 07:36:48 (2.21 MB/s) - ‘protT5/sec_struct_checkpoint/secstruct_checkpoint.pt’ saved [3727951/3727951]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create directory for storing model weights (2.3GB) and example sequences.\n",
    "# Here we use the encoder-part of ProtT5-XL-U50 in half-precision (fp16) as \n",
    "# it performed best in our benchmarks (also outperforming ProtBERT-BFD).\n",
    "# Also download secondary structure prediction checkpoint to show annotation extraction from embeddings\n",
    "!mkdir protT5 # root directory for storing checkpoints, results etc\n",
    "!mkdir protT5/protT5_checkpoint # directory holding the ProtT5 checkpoint\n",
    "!mkdir protT5/sec_struct_checkpoint # directory storing the supervised classifier's checkpoint\n",
    "!mkdir protT5/output # directory for storing your embeddings & predictions\n",
    "!wget -nc -P protT5/ https://rostlab.org/~deepppi/example_seqs.fasta\n",
    "# Huge kudos to the bio_embeddings team here! We will integrate the new encoder, half-prec ProtT5 checkpoint soon\n",
    "!wget -nc -P protT5/sec_struct_checkpoint http://data.bioembeddings.com/public/embeddings/feature_models/t5/secstruct_checkpoint.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qpVKvCU2sfge"
   },
   "source": [
    "### Download interaction, dictionary/fasta file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1cTGEFmip9Na",
    "outputId": "3440cd3c-ecf4-4323-b3e4-258da3f85435"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-06-03 07:36:54--  https://raw.githubusercontent.com/anhvt00/MCAPS/master/data/Golden-standard-datasets/Guo-2008/guo_dict.tsv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1415079 (1.3M) [text/plain]\n",
      "Saving to: ‘guo_dict.tsv’\n",
      "\n",
      "guo_dict.tsv        100%[===================>]   1.35M  6.46MB/s    in 0.2s    \n",
      "\n",
      "2023-06-03 07:36:55 (6.46 MB/s) - ‘guo_dict.tsv’ saved [1415079/1415079]\n",
      "\n",
      "--2023-06-03 07:36:55--  https://raw.githubusercontent.com/anhvt00/MCAPS/master/data/Golden-standard-datasets/Guo-2008/guo_pairs.tsv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 179008 (175K) [text/plain]\n",
      "Saving to: ‘guo_pairs.tsv’\n",
      "\n",
      "guo_pairs.tsv       100%[===================>] 174.81K  --.-KB/s    in 0.09s   \n",
      "\n",
      "2023-06-03 07:36:56 (1.83 MB/s) - ‘guo_pairs.tsv’ saved [179008/179008]\n",
      "\n",
      "--2023-06-03 07:36:56--  https://raw.githubusercontent.com/anhvt00/MCAPS/master/data/Golden-standard-datasets/Guo-2008/guo.fasta\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1417576 (1.4M) [text/plain]\n",
      "Saving to: ‘guo.fasta’\n",
      "\n",
      "guo.fasta           100%[===================>]   1.35M  7.24MB/s    in 0.2s    \n",
      "\n",
      "2023-06-03 07:36:57 (7.24 MB/s) - ‘guo.fasta’ saved [1417576/1417576]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Yous should change for the dataset you want to test\n",
    "!wget https://raw.githubusercontent.com/anhvt00/MCAPS/master/data/Golden-standard-datasets/Guo-2008/guo_dict.tsv\n",
    "!wget https://raw.githubusercontent.com/anhvt00/MCAPS/master/data/Golden-standard-datasets/Guo-2008/guo_pairs.tsv\n",
    "!wget https://raw.githubusercontent.com/anhvt00/MCAPS/master/data/Golden-standard-datasets/Guo-2008/guo.fasta\n",
    "    \n",
    "pair_dataset_path = 'guo_pairs.tsv'\n",
    "seq_path = 'guo.fasta'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qUrwC7uGp22f"
   },
   "source": [
    "### Choose sequence FASTA file for embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "u0bzK4mbqCyF"
   },
   "outputs": [],
   "source": [
    "# In the following you can define your desired output. Current options:\n",
    "# per_residue embeddings\n",
    "# per_protein embeddings\n",
    "# secondary structure predictions\n",
    "\n",
    "# Replace this file with your own (multi-)FASTA\n",
    "# Headers are expected to start with \">\";\n",
    "# seq_path = \"guo.fasta\"\n",
    "\n",
    "# whether to retrieve embeddings for each residue in a protein \n",
    "# --> Lx1024 matrix per protein with L being the protein's length\n",
    "# as a rule of thumb: 1k proteins require around 1GB RAM/disk\n",
    "per_residue = True \n",
    "per_residue_path = \"./protT5/output/per_residue_embeddings.h5\" # where to store the embeddings\n",
    "\n",
    "# whether to retrieve per-protein embeddings \n",
    "# --> only one 1024-d vector per protein, irrespective of its length\n",
    "per_protein = False\n",
    "per_protein_path = \"./protT5/output/per_protein_embeddings.h5\" # where to store the embeddings\n",
    "\n",
    "# whether to retrieve secondary structure predictions\n",
    "# This can be replaced by your method after being trained on ProtT5 embeddings\n",
    "sec_struct = False\n",
    "sec_struct_path = \"./protT5/output/ss3_preds.fasta\" # file for storing predictions\n",
    "\n",
    "# make sure that either per-residue or per-protein embeddings are stored\n",
    "assert per_protein is True or per_residue is True or sec_struct is True, print(\n",
    "    \"Minimally, you need to active per_residue, per_protein or sec_struct. (or any combination)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ckj3UjxyrEdm"
   },
   "source": [
    "### Read FASTA file function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "BQ9zpehyqrtv"
   },
   "outputs": [],
   "source": [
    "def read_fasta( fasta_path, split_char=\"!\", id_field=0):\n",
    "    '''\n",
    "        Reads in fasta file containing multiple sequences.\n",
    "        Split_char and id_field allow to control identifier extraction from header.\n",
    "        E.g.: set split_char=\"|\" and id_field=1 for SwissProt/UniProt Headers.\n",
    "        Returns dictionary holding multiple sequences or only single \n",
    "        sequence, depending on input file.\n",
    "    '''\n",
    "    \n",
    "    seqs = dict()\n",
    "    with open( fasta_path, 'r' ) as fasta_f:\n",
    "        for line in fasta_f:\n",
    "            # get uniprot ID from header and create new entry\n",
    "            if line.startswith('>'):\n",
    "                uniprot_id = line.replace('>', '').strip().split(split_char)[id_field]\n",
    "                # replace tokens that are mis-interpreted when loading h5\n",
    "                uniprot_id = uniprot_id.replace(\"/\",\"_\").replace(\".\",\"_\")\n",
    "                seqs[ uniprot_id ] = ''\n",
    "            else:\n",
    "                # repl. all whie-space chars and join seqs spanning multiple lines, drop gaps and cast to upper-case\n",
    "                seq= ''.join( line.split() ).upper().replace(\"-\",\"\")\n",
    "                # repl. all non-standard AAs and map them to unknown/X\n",
    "                seq = seq.replace('U','X').replace('Z','X').replace('O','X')\n",
    "                seqs[ uniprot_id ] += seq \n",
    "    example_id=next(iter(seqs))\n",
    "    print(\"Read {} sequences.\".format(len(seqs)))\n",
    "    print(\"Example:\\n{}\\n{}\".format(example_id,seqs[example_id]))\n",
    "\n",
    "    return seqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vEL1Q_TkqR2Y"
   },
   "source": [
    "### Import ProtBert embedder model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CnnYiMSSqQ_G",
    "outputId": "6fcd0967-f2ab-4ff1-d287-5f7d4fe2b0fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5EncoderModel, T5Tokenizer\n",
    "import torch\n",
    "import numpy as np\n",
    "import h5py\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using {}\".format(device))\n",
    "\n",
    "def get_T5_model():\n",
    "    model = T5EncoderModel.from_pretrained(\"Rostlab/prot_t5_xl_half_uniref50-enc\")\n",
    "    model = model.to(device) # move model to GPU\n",
    "    model = model.eval() # set model to evaluation model\n",
    "    tokenizer = T5Tokenizer.from_pretrained('Rostlab/prot_t5_xl_half_uniref50-enc', do_lower_case=False)\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lF-_BqblrOPq"
   },
   "source": [
    "### Generate embeddings function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "FaAQNEs8qZw4"
   },
   "outputs": [],
   "source": [
    "# Generate embeddings via batch-processing\n",
    "# per_residue indicates that embeddings for each residue in a protein should be returned.\n",
    "# per_protein indicates that embeddings for a whole protein should be returned (average-pooling)\n",
    "# max_residues gives the upper limit of residues within one batch\n",
    "# max_seq_len gives the upper sequences length for applying batch-processing\n",
    "# max_batch gives the upper number of sequences per batch\n",
    "def get_embeddings( model, tokenizer, seqs, per_residue, per_protein, sec_struct, \n",
    "                   max_residues=4000, max_seq_len=1000, max_batch=100 ):\n",
    "\n",
    "    if sec_struct:\n",
    "      sec_struct_model = load_sec_struct_model()\n",
    "\n",
    "    results = {\"residue_embs\" : dict(), \n",
    "               \"protein_embs\" : dict(),\n",
    "               \"sec_structs\" : dict() \n",
    "               }\n",
    "\n",
    "    # sort sequences according to length (reduces unnecessary padding --> speeds up embedding)\n",
    "    seq_dict   = sorted( seqs.items(), key=lambda kv: len( seqs[kv[0]] ), reverse=True )\n",
    "    start = time.time()\n",
    "    batch = list()\n",
    "    for seq_idx, (pdb_id, seq) in tqdm(enumerate(seq_dict,1)):\n",
    "        seq = seq\n",
    "        seq_len = len(seq)\n",
    "        seq = ' '.join(list(seq))\n",
    "        batch.append((pdb_id,seq,seq_len))\n",
    "\n",
    "        # count residues in current batch and add the last sequence length to\n",
    "        # avoid that batches with (n_res_batch > max_residues) get processed \n",
    "        n_res_batch = sum([ s_len for  _, _, s_len in batch ]) + seq_len \n",
    "        if len(batch) >= max_batch or n_res_batch>=max_residues or seq_idx==len(seq_dict) or seq_len>max_seq_len:\n",
    "            pdb_ids, seqs, seq_lens = zip(*batch)\n",
    "            batch = list()\n",
    "\n",
    "            # add_special_tokens adds extra token at the end of each sequence\n",
    "            token_encoding = tokenizer.batch_encode_plus(seqs, add_special_tokens=True, padding=\"longest\")\n",
    "            input_ids      = torch.tensor(token_encoding['input_ids']).to(device)\n",
    "            attention_mask = torch.tensor(token_encoding['attention_mask']).to(device)\n",
    "            \n",
    "            try:\n",
    "                with torch.no_grad():\n",
    "                    # returns: ( batch-size x max_seq_len_in_minibatch x embedding_dim )\n",
    "                    embedding_repr = model(input_ids, attention_mask=attention_mask)\n",
    "            except RuntimeError:\n",
    "                print(\"RuntimeError during embedding for {} (L={})\".format(pdb_id, seq_len))\n",
    "                continue\n",
    "\n",
    "            if sec_struct: # in case you want to predict secondary structure from embeddings\n",
    "              d3_Yhat, d8_Yhat, diso_Yhat = sec_struct_model(embedding_repr.last_hidden_state)\n",
    "\n",
    "\n",
    "            for batch_idx, identifier in enumerate(pdb_ids): # for each protein in the current mini-batch\n",
    "                s_len = seq_lens[batch_idx]\n",
    "                # slice off padding --> batch-size x seq_len x embedding_dim  \n",
    "                emb = embedding_repr.last_hidden_state[batch_idx,:s_len]\n",
    "                if sec_struct: # get classification results\n",
    "                    results[\"sec_structs\"][identifier] = torch.max( d3_Yhat[batch_idx,:s_len], dim=1 )[1].detach().cpu().numpy().squeeze()\n",
    "                if per_residue: # store per-residue embeddings (Lx1024)\n",
    "                    results[\"residue_embs\"][ identifier ] = emb.detach().cpu().numpy().squeeze()\n",
    "                if per_protein: # apply average-pooling to derive per-protein embeddings (1024-d)\n",
    "                    protein_emb = emb.mean(dim=0)\n",
    "                    results[\"protein_embs\"][identifier] = protein_emb.detach().cpu().numpy().squeeze()\n",
    "\n",
    "\n",
    "    passed_time=time.time()-start\n",
    "    avg_time = passed_time/len(results[\"residue_embs\"]) if per_residue else passed_time/len(results[\"protein_embs\"])\n",
    "    print('\\n############# EMBEDDING STATS #############')\n",
    "    print('Total number of per-residue embeddings: {}'.format(len(results[\"residue_embs\"])))\n",
    "    print('Total number of per-protein embeddings: {}'.format(len(results[\"protein_embs\"])))\n",
    "    print(\"Time for generating embeddings: {:.1f}[m] ({:.3f}[s/protein])\".format(\n",
    "        passed_time/60, avg_time ))\n",
    "    print('\\n############# END #############')\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "16chLbRYrbmn"
   },
   "source": [
    "### Write embeddings to disk function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "0WsvghwRrZdf"
   },
   "outputs": [],
   "source": [
    "def save_embeddings(emb_dict,out_path):\n",
    "    with h5py.File(str(out_path), \"w\") as hf:\n",
    "        for sequence_id, embedding in emb_dict.items():\n",
    "            # noinspection PyUnboundLocalVariable\n",
    "            hf.create_dataset(sequence_id, data=embedding)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ee18ICHsryMg"
   },
   "source": [
    "### Generate embedding for FASTA file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "CMYqWvA7roEf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 2497 sequences.\n",
      "Example:\n",
      "O13297\n",
      "MSYTDNPPQTKRALSLDDLVNHDENEKVKLQKLSEAANGSRPFAENLESDINQTETGQAAPIDNYKESTGHGSHSQKPKSRKSSNDDEETDTDDEMGASGEINFDSEMDFDYDKQHRNLLSNGSPPMNDGSDANAKLEKPSDDSIHQNSKSDEEQRIPKQGNEGNIASNYITQVPLQKQKQTEKKIAGNAVGSVVKKEEEANAAVDNIFEEKATLQSKKNNIKRDLEVLNEISASSKPSKYKNVPIWAQKWKPTIKALQSINVKDLKIDPSFLNIIPDDDLTKSVQDWVYATIYSIAPELRSFIELEMKFGVIIDAKGPDRVNPPVSSQCVFTELDAHLTPNIDASLFKELSKYIRGISEVTENTGKFSIIESQTRDSVYRVGLSTQRPRFLRMSTDIKTGRVGQFIEKRHVAQLLLYSPKDSYDVKISLNLELPVPDNDPPEKYKSQSPISERTKDRVSYIHNDSCTRIDITKVENHNQNSKSRQSETTHEVELEINTPALLNAFDNITNDSKEYASLIRTFLNNGTIIRRKLSSLSYEIFEGSKKVM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2497it [01:27, 28.54it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "############# EMBEDDING STATS #############\n",
      "Total number of per-residue embeddings: 2497\n",
      "Total number of per-protein embeddings: 0\n",
      "Time for generating embeddings: 1.5[m] (0.035[s/protein])\n",
      "\n",
      "############# END #############\n"
     ]
    }
   ],
   "source": [
    "# Load the encoder part of ProtT5-XL-U50 in half-precision (recommended)\n",
    "model, tokenizer = get_T5_model()\n",
    "\n",
    "# Load example fasta.\n",
    "seqs = read_fasta( seq_path )\n",
    "\n",
    "for id, seq in seqs.items():\n",
    "    if len(seq) > 1200:\n",
    "        seqs[id] = seq[:1200]\n",
    "\n",
    "\n",
    "# Compute embeddings and/or secondary structure predictions\n",
    "results = get_embeddings( model, tokenizer, seqs,\n",
    "                         per_residue, per_protein, sec_struct)\n",
    "\n",
    "# Store per-residue embeddings\n",
    "if per_residue:\n",
    "  save_embeddings(results[\"residue_embs\"], per_residue_path)\n",
    "if per_protein:\n",
    "  save_embeddings(results[\"protein_embs\"], per_protein_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8lxQjjPnoY02"
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U1QbuwzCoao2",
    "outputId": "0a69579a-ac80-409f-d656-3d6027dae3c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in /usr/local/lib/python3.8/dist-packages (1.7.5)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from xgboost) (1.23.5)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from xgboost) (1.10.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (3.7.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (1.0.7)\n",
      "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (1.23.5)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (5.12.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (9.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (23.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (4.39.4)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from importlib-resources>=3.2.0->matplotlib) (3.15.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.14.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "wget is already the newest version (1.20.3-1ubuntu2).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 46 not upgraded.\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (1.2.2)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (2.0.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.23.5)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.8/dist-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.8/dist-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.14.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: gdown in /usr/local/lib/python3.8/dist-packages (4.7.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.8/dist-packages (from gdown) (4.12.2)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from gdown) (1.14.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from gdown) (4.65.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from gdown) (3.12.0)\n",
      "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.8/dist-packages (from gdown) (2.31.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.8/dist-packages (from beautifulsoup4->gdown) (2.4.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests[socks]->gdown) (2019.11.28)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests[socks]->gdown) (1.25.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests[socks]->gdown) (2.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (3.1.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-03 07:39:44.150366: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.8/dist-packages (0.20.0)\n",
      "Requirement already satisfied: typeguard<3.0.0,>=2.7 in /usr/local/lib/python3.8/dist-packages (from tensorflow-addons) (2.13.3)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow-addons) (23.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "2023-06-03 07:39:49.319426: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 70636 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:9b:00.0, compute capability: 8.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n",
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA A100-SXM4-80GB, compute capability 8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost\n",
    "!pip install matplotlib\n",
    "!apt install wget\n",
    "!pip install scikit-learn pandas \n",
    "!pip install gdown\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "import math\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout, BatchNormalization, ReLU, LeakyReLU, Conv1D, GlobalMaxPooling1D, AveragePooling1D, MaxPooling1D, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import concatenate, multiply, Bidirectional, LSTM, GRU, Flatten, PReLU, add, SpatialDropout1D\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import l2, l1_l2\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import matthews_corrcoef,accuracy_score, precision_score,recall_score\n",
    "from sklearn.manifold import TSNE\n",
    "import os\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras import mixed_precision\n",
    "!pip install tensorflow-addons\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "import time\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "### Setting RAM GPU for training growth \n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "\n",
    "# Disables caching (when set to 1) or enables caching (when set to 0) for just-in-time-compilation. When disabled,\n",
    "# no binary code is added to or retrieved from the cache.\n",
    "os.environ['CUDA_CACHE_DISABLE'] = '0' # orig is 0\n",
    "\n",
    "# When set to 1, forces the device driver to ignore any binary code embedded in an application \n",
    "# (see Application Compatibility) and to just-in-time compile embedded PTX code instead.\n",
    "# If a kernel does not have embedded PTX code, it will fail to load. This environment variable can be used to\n",
    "# validate that PTX code is embedded in an application and that its just-in-time compilation works as expected to guarantee application \n",
    "# forward compatibility with future architectures.\n",
    "os.environ['CUDA_FORCE_PTX_JIT'] = '1'# no orig\n",
    "\n",
    "\n",
    "os.environ['HOROVOD_GPU_ALLREDUCE'] = 'NCCL'\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "os.environ['TF_GPU_THREAD_MODE'] = 'gpu_private'\n",
    "os.environ['TF_GPU_THREAD_COUNT']='1'\n",
    "\n",
    "os.environ['TF_USE_CUDNN_BATCHNORM_SPATIAL_PERSISTENT'] = '1'\n",
    "\n",
    "os.environ['TF_ADJUST_HUE_FUSED'] = '1'\n",
    "os.environ['TF_ADJUST_SATURATION_FUSED'] = '1'\n",
    "os.environ['TF_ENABLE_WINOGRAD_NONFUSED'] = '1'\n",
    "\n",
    "os.environ['TF_SYNC_ON_FINISH'] = '0'\n",
    "os.environ['TF_AUTOTUNE_THRESHOLD'] = '2'\n",
    "os.environ['TF_DISABLE_NVTX_RANGES'] = '1'\n",
    "os.environ[\"TF_ENABLE_AUTO_MIXED_PRECISION_GRAPH_REWRITE\"] = \"1\"\n",
    "\n",
    "\n",
    "\n",
    "# =================================================\n",
    "mixed_precision.set_global_policy('mixed_float16')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j1DKoGKXsQvK"
   },
   "source": [
    "### Read interaction dataset to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "_5XAIYkKsXPW",
    "outputId": "77ae6601-015c-44a5-d0d1-bb230ad962ca"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p1</th>\n",
       "      <th>p2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q12236</td>\n",
       "      <td>P36094</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P39730</td>\n",
       "      <td>P00899</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P32389</td>\n",
       "      <td>P09734</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P35817</td>\n",
       "      <td>P25502</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P40956</td>\n",
       "      <td>P15705</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11183</th>\n",
       "      <td>Q12477</td>\n",
       "      <td>Q07540</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11184</th>\n",
       "      <td>P20459</td>\n",
       "      <td>P19454</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11185</th>\n",
       "      <td>P40465</td>\n",
       "      <td>P30771</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11186</th>\n",
       "      <td>P36224</td>\n",
       "      <td>P30619</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11187</th>\n",
       "      <td>P53297</td>\n",
       "      <td>P38353</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11188 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           p1      p2  label\n",
       "0      Q12236  P36094    1.0\n",
       "1      P39730  P00899    1.0\n",
       "2      P32389  P09734    0.0\n",
       "3      P35817  P25502    1.0\n",
       "4      P40956  P15705    1.0\n",
       "...       ...     ...    ...\n",
       "11183  Q12477  Q07540    0.0\n",
       "11184  P20459  P19454    1.0\n",
       "11185  P40465  P30771    1.0\n",
       "11186  P36224  P30619    1.0\n",
       "11187  P53297  P38353    0.0\n",
       "\n",
       "[11188 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_dataframe = pd.read_csv(pair_dataset_path, sep='\\t', header=None)\n",
    "pair_array  = pair_dataframe.to_numpy()\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(pair_array)\n",
    "pair_dataframe = pd.DataFrame(pair_array)\n",
    "pair_dataframe = pd.DataFrame(pair_array, columns = ['p1', 'p2', 'label'])\n",
    "pair_dataframe['label'] = pair_dataframe['label'].astype('float16') \n",
    "pair_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kWnhEFD5xXNl"
   },
   "source": [
    "### Padding function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "koiIBavgWWeT"
   },
   "outputs": [],
   "source": [
    "def pad(rst, length=1200, dim=1024):\n",
    "    if len(rst) > length:\n",
    "        return rst[:length]\n",
    "    elif len(rst) < length:\n",
    "        return np.concatenate((rst, np.zeros((length - len(rst), dim))))\n",
    "    return rst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4-mBPjEe1P_X"
   },
   "source": [
    "### Read embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "nUl-7v161P_x"
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "embedding_matrix= h5py.File(per_residue_path, 'r')\n",
    "protein_keys = list(embedding_matrix.keys())\n",
    "embedding_dict = dict()\n",
    "\n",
    "for key in protein_keys:\n",
    "  embedding_dict[key] = np.array(embedding_matrix[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AqIYpouvtZNN"
   },
   "source": [
    "### Split indicies for cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "BAYXEC_PGR_w"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, ShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "kf = KFold(n_splits=5, random_state=42, shuffle=True)\n",
    "train_test = []\n",
    "for train, test in kf.split(pair_dataframe['label']):\n",
    "  train_test.append((train, test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QMoeBQnUCK_E"
   },
   "source": [
    "### Architecture of MCAPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iY1Qm9AjmIaO",
    "outputId": "6e0d8050-74b0-4c3f-a652-491480b44501"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.8/dist-packages (0.20.0)\n",
      "Requirement already satisfied: typeguard<3.0.0,>=2.7 in /usr/local/lib/python3.8/dist-packages (from tensorflow-addons) (2.13.3)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow-addons) (23.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import get_custom_objects\n",
    "def leaky_relu(x, alpha = .2):\n",
    "   return tf.keras.backend.maximum(alpha*x, x)\n",
    "!pip install tensorflow-addons\n",
    "import tensorflow_addons as tfa \n",
    "get_custom_objects().update({'leaky_relu': leaky_relu})\n",
    "get_custom_objects().update({'mish': tfa.activations.mish})\n",
    "get_custom_objects().update({'lisht': tfa.activations.lisht})\n",
    "get_custom_objects().update({'rrelu': tfa.activations.rrelu})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "UwUkDkHjsJ1c",
    "outputId": "64108f8a-a88b-40fb-b74a-96a149a48a15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " seq1 (InputLayer)              [(None, 1200, 1024)  0           []                               \n",
      "                                ]                                                                 \n",
      "                                                                                                  \n",
      " seq2 (InputLayer)              [(None, 1200, 1024)  0           []                               \n",
      "                                ]                                                                 \n",
      "                                                                                                  \n",
      " conv1d_45 (Conv1D)             (None, 1200, 50)     102450      ['seq1[0][0]',                   \n",
      "                                                                  'seq2[0][0]']                   \n",
      "                                                                                                  \n",
      " conv1d_50 (Conv1D)             (None, 1200, 50)     153650      ['seq1[0][0]',                   \n",
      "                                                                  'seq2[0][0]']                   \n",
      "                                                                                                  \n",
      " conv1d_55 (Conv1D)             (None, 1200, 50)     204850      ['seq1[0][0]',                   \n",
      "                                                                  'seq2[0][0]']                   \n",
      "                                                                                                  \n",
      " max_pooling1d_90 (MaxPooling1D  (None, 300, 50)     0           ['conv1d_45[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " average_pooling1d_90 (AverageP  (None, 300, 50)     0           ['conv1d_45[0][0]']              \n",
      " ooling1D)                                                                                        \n",
      "                                                                                                  \n",
      " max_pooling1d_100 (MaxPooling1  (None, 300, 50)     0           ['conv1d_50[0][0]']              \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " average_pooling1d_100 (Average  (None, 300, 50)     0           ['conv1d_50[0][0]']              \n",
      " Pooling1D)                                                                                       \n",
      "                                                                                                  \n",
      " max_pooling1d_110 (MaxPooling1  (None, 300, 50)     0           ['conv1d_55[0][0]']              \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " average_pooling1d_110 (Average  (None, 300, 50)     0           ['conv1d_55[0][0]']              \n",
      " Pooling1D)                                                                                       \n",
      "                                                                                                  \n",
      " max_pooling1d_91 (MaxPooling1D  (None, 300, 50)     0           ['conv1d_45[1][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " average_pooling1d_91 (AverageP  (None, 300, 50)     0           ['conv1d_45[1][0]']              \n",
      " ooling1D)                                                                                        \n",
      "                                                                                                  \n",
      " max_pooling1d_101 (MaxPooling1  (None, 300, 50)     0           ['conv1d_50[1][0]']              \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " average_pooling1d_101 (Average  (None, 300, 50)     0           ['conv1d_50[1][0]']              \n",
      " Pooling1D)                                                                                       \n",
      "                                                                                                  \n",
      " max_pooling1d_111 (MaxPooling1  (None, 300, 50)     0           ['conv1d_55[1][0]']              \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " average_pooling1d_111 (Average  (None, 300, 50)     0           ['conv1d_55[1][0]']              \n",
      " Pooling1D)                                                                                       \n",
      "                                                                                                  \n",
      " concatenate_191 (Concatenate)  (None, 300, 100)     0           ['max_pooling1d_90[0][0]',       \n",
      "                                                                  'average_pooling1d_90[0][0]']   \n",
      "                                                                                                  \n",
      " concatenate_211 (Concatenate)  (None, 300, 100)     0           ['max_pooling1d_100[0][0]',      \n",
      "                                                                  'average_pooling1d_100[0][0]']  \n",
      "                                                                                                  \n",
      " concatenate_231 (Concatenate)  (None, 300, 100)     0           ['max_pooling1d_110[0][0]',      \n",
      "                                                                  'average_pooling1d_110[0][0]']  \n",
      "                                                                                                  \n",
      " concatenate_192 (Concatenate)  (None, 300, 100)     0           ['max_pooling1d_91[0][0]',       \n",
      "                                                                  'average_pooling1d_91[0][0]']   \n",
      "                                                                                                  \n",
      " concatenate_212 (Concatenate)  (None, 300, 100)     0           ['max_pooling1d_101[0][0]',      \n",
      "                                                                  'average_pooling1d_101[0][0]']  \n",
      "                                                                                                  \n",
      " concatenate_232 (Concatenate)  (None, 300, 100)     0           ['max_pooling1d_111[0][0]',      \n",
      "                                                                  'average_pooling1d_111[0][0]']  \n",
      "                                                                                                  \n",
      " spatial_dropout1d_90 (SpatialD  (None, 300, 100)    0           ['concatenate_191[0][0]']        \n",
      " ropout1D)                                                                                        \n",
      "                                                                                                  \n",
      " spatial_dropout1d_100 (Spatial  (None, 300, 100)    0           ['concatenate_211[0][0]']        \n",
      " Dropout1D)                                                                                       \n",
      "                                                                                                  \n",
      " spatial_dropout1d_110 (Spatial  (None, 300, 100)    0           ['concatenate_231[0][0]']        \n",
      " Dropout1D)                                                                                       \n",
      "                                                                                                  \n",
      " spatial_dropout1d_91 (SpatialD  (None, 300, 100)    0           ['concatenate_192[0][0]']        \n",
      " ropout1D)                                                                                        \n",
      "                                                                                                  \n",
      " spatial_dropout1d_101 (Spatial  (None, 300, 100)    0           ['concatenate_212[0][0]']        \n",
      " Dropout1D)                                                                                       \n",
      "                                                                                                  \n",
      " spatial_dropout1d_111 (Spatial  (None, 300, 100)    0           ['concatenate_232[0][0]']        \n",
      " Dropout1D)                                                                                       \n",
      "                                                                                                  \n",
      " conv1d_46 (Conv1D)             (None, 300, 50)      10050       ['spatial_dropout1d_90[0][0]',   \n",
      "                                                                  'spatial_dropout1d_91[0][0]']   \n",
      "                                                                                                  \n",
      " conv1d_51 (Conv1D)             (None, 300, 50)      15050       ['spatial_dropout1d_100[0][0]',  \n",
      "                                                                  'spatial_dropout1d_101[0][0]']  \n",
      "                                                                                                  \n",
      " conv1d_56 (Conv1D)             (None, 300, 50)      20050       ['spatial_dropout1d_110[0][0]',  \n",
      "                                                                  'spatial_dropout1d_111[0][0]']  \n",
      "                                                                                                  \n",
      " max_pooling1d_92 (MaxPooling1D  (None, 75, 50)      0           ['conv1d_46[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " average_pooling1d_92 (AverageP  (None, 75, 50)      0           ['conv1d_46[0][0]']              \n",
      " ooling1D)                                                                                        \n",
      "                                                                                                  \n",
      " max_pooling1d_102 (MaxPooling1  (None, 75, 50)      0           ['conv1d_51[0][0]']              \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " average_pooling1d_102 (Average  (None, 75, 50)      0           ['conv1d_51[0][0]']              \n",
      " Pooling1D)                                                                                       \n",
      "                                                                                                  \n",
      " max_pooling1d_112 (MaxPooling1  (None, 75, 50)      0           ['conv1d_56[0][0]']              \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " average_pooling1d_112 (Average  (None, 75, 50)      0           ['conv1d_56[0][0]']              \n",
      " Pooling1D)                                                                                       \n",
      "                                                                                                  \n",
      " max_pooling1d_93 (MaxPooling1D  (None, 75, 50)      0           ['conv1d_46[1][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " average_pooling1d_93 (AverageP  (None, 75, 50)      0           ['conv1d_46[1][0]']              \n",
      " ooling1D)                                                                                        \n",
      "                                                                                                  \n",
      " max_pooling1d_103 (MaxPooling1  (None, 75, 50)      0           ['conv1d_51[1][0]']              \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " average_pooling1d_103 (Average  (None, 75, 50)      0           ['conv1d_51[1][0]']              \n",
      " Pooling1D)                                                                                       \n",
      "                                                                                                  \n",
      " max_pooling1d_113 (MaxPooling1  (None, 75, 50)      0           ['conv1d_56[1][0]']              \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " average_pooling1d_113 (Average  (None, 75, 50)      0           ['conv1d_56[1][0]']              \n",
      " Pooling1D)                                                                                       \n",
      "                                                                                                  \n",
      " concatenate_195 (Concatenate)  (None, 75, 100)      0           ['max_pooling1d_92[0][0]',       \n",
      "                                                                  'average_pooling1d_92[0][0]']   \n",
      "                                                                                                  \n",
      " concatenate_215 (Concatenate)  (None, 75, 100)      0           ['max_pooling1d_102[0][0]',      \n",
      "                                                                  'average_pooling1d_102[0][0]']  \n",
      "                                                                                                  \n",
      " concatenate_235 (Concatenate)  (None, 75, 100)      0           ['max_pooling1d_112[0][0]',      \n",
      "                                                                  'average_pooling1d_112[0][0]']  \n",
      "                                                                                                  \n",
      " concatenate_196 (Concatenate)  (None, 75, 100)      0           ['max_pooling1d_93[0][0]',       \n",
      "                                                                  'average_pooling1d_93[0][0]']   \n",
      "                                                                                                  \n",
      " concatenate_216 (Concatenate)  (None, 75, 100)      0           ['max_pooling1d_103[0][0]',      \n",
      "                                                                  'average_pooling1d_103[0][0]']  \n",
      "                                                                                                  \n",
      " concatenate_236 (Concatenate)  (None, 75, 100)      0           ['max_pooling1d_113[0][0]',      \n",
      "                                                                  'average_pooling1d_113[0][0]']  \n",
      "                                                                                                  \n",
      " spatial_dropout1d_92 (SpatialD  (None, 75, 100)     0           ['concatenate_195[0][0]']        \n",
      " ropout1D)                                                                                        \n",
      "                                                                                                  \n",
      " spatial_dropout1d_102 (Spatial  (None, 75, 100)     0           ['concatenate_215[0][0]']        \n",
      " Dropout1D)                                                                                       \n",
      "                                                                                                  \n",
      " spatial_dropout1d_112 (Spatial  (None, 75, 100)     0           ['concatenate_235[0][0]']        \n",
      " Dropout1D)                                                                                       \n",
      "                                                                                                  \n",
      " spatial_dropout1d_93 (SpatialD  (None, 75, 100)     0           ['concatenate_196[0][0]']        \n",
      " ropout1D)                                                                                        \n",
      "                                                                                                  \n",
      " spatial_dropout1d_103 (Spatial  (None, 75, 100)     0           ['concatenate_216[0][0]']        \n",
      " Dropout1D)                                                                                       \n",
      "                                                                                                  \n",
      " spatial_dropout1d_113 (Spatial  (None, 75, 100)     0           ['concatenate_236[0][0]']        \n",
      " Dropout1D)                                                                                       \n",
      "                                                                                                  \n",
      " conv1d_47 (Conv1D)             (None, 75, 50)       10050       ['spatial_dropout1d_92[0][0]',   \n",
      "                                                                  'spatial_dropout1d_93[0][0]']   \n",
      "                                                                                                  \n",
      " conv1d_52 (Conv1D)             (None, 75, 50)       15050       ['spatial_dropout1d_102[0][0]',  \n",
      "                                                                  'spatial_dropout1d_103[0][0]']  \n",
      "                                                                                                  \n",
      " conv1d_57 (Conv1D)             (None, 75, 50)       20050       ['spatial_dropout1d_112[0][0]',  \n",
      "                                                                  'spatial_dropout1d_113[0][0]']  \n",
      "                                                                                                  \n",
      " max_pooling1d_94 (MaxPooling1D  (None, 18, 50)      0           ['conv1d_47[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " average_pooling1d_94 (AverageP  (None, 18, 50)      0           ['conv1d_47[0][0]']              \n",
      " ooling1D)                                                                                        \n",
      "                                                                                                  \n",
      " max_pooling1d_104 (MaxPooling1  (None, 18, 50)      0           ['conv1d_52[0][0]']              \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " average_pooling1d_104 (Average  (None, 18, 50)      0           ['conv1d_52[0][0]']              \n",
      " Pooling1D)                                                                                       \n",
      "                                                                                                  \n",
      " max_pooling1d_114 (MaxPooling1  (None, 18, 50)      0           ['conv1d_57[0][0]']              \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " average_pooling1d_114 (Average  (None, 18, 50)      0           ['conv1d_57[0][0]']              \n",
      " Pooling1D)                                                                                       \n",
      "                                                                                                  \n",
      " max_pooling1d_95 (MaxPooling1D  (None, 18, 50)      0           ['conv1d_47[1][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " average_pooling1d_95 (AverageP  (None, 18, 50)      0           ['conv1d_47[1][0]']              \n",
      " ooling1D)                                                                                        \n",
      "                                                                                                  \n",
      " max_pooling1d_105 (MaxPooling1  (None, 18, 50)      0           ['conv1d_52[1][0]']              \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " average_pooling1d_105 (Average  (None, 18, 50)      0           ['conv1d_52[1][0]']              \n",
      " Pooling1D)                                                                                       \n",
      "                                                                                                  \n",
      " max_pooling1d_115 (MaxPooling1  (None, 18, 50)      0           ['conv1d_57[1][0]']              \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " average_pooling1d_115 (Average  (None, 18, 50)      0           ['conv1d_57[1][0]']              \n",
      " Pooling1D)                                                                                       \n",
      "                                                                                                  \n",
      " concatenate_199 (Concatenate)  (None, 18, 100)      0           ['max_pooling1d_94[0][0]',       \n",
      "                                                                  'average_pooling1d_94[0][0]']   \n",
      "                                                                                                  \n",
      " concatenate_219 (Concatenate)  (None, 18, 100)      0           ['max_pooling1d_104[0][0]',      \n",
      "                                                                  'average_pooling1d_104[0][0]']  \n",
      "                                                                                                  \n",
      " concatenate_239 (Concatenate)  (None, 18, 100)      0           ['max_pooling1d_114[0][0]',      \n",
      "                                                                  'average_pooling1d_114[0][0]']  \n",
      "                                                                                                  \n",
      " concatenate_200 (Concatenate)  (None, 18, 100)      0           ['max_pooling1d_95[0][0]',       \n",
      "                                                                  'average_pooling1d_95[0][0]']   \n",
      "                                                                                                  \n",
      " concatenate_220 (Concatenate)  (None, 18, 100)      0           ['max_pooling1d_105[0][0]',      \n",
      "                                                                  'average_pooling1d_105[0][0]']  \n",
      "                                                                                                  \n",
      " concatenate_240 (Concatenate)  (None, 18, 100)      0           ['max_pooling1d_115[0][0]',      \n",
      "                                                                  'average_pooling1d_115[0][0]']  \n",
      "                                                                                                  \n",
      " spatial_dropout1d_94 (SpatialD  (None, 18, 100)     0           ['concatenate_199[0][0]']        \n",
      " ropout1D)                                                                                        \n",
      "                                                                                                  \n",
      " spatial_dropout1d_104 (Spatial  (None, 18, 100)     0           ['concatenate_219[0][0]']        \n",
      " Dropout1D)                                                                                       \n",
      "                                                                                                  \n",
      " spatial_dropout1d_114 (Spatial  (None, 18, 100)     0           ['concatenate_239[0][0]']        \n",
      " Dropout1D)                                                                                       \n",
      "                                                                                                  \n",
      " spatial_dropout1d_95 (SpatialD  (None, 18, 100)     0           ['concatenate_200[0][0]']        \n",
      " ropout1D)                                                                                        \n",
      "                                                                                                  \n",
      " spatial_dropout1d_105 (Spatial  (None, 18, 100)     0           ['concatenate_220[0][0]']        \n",
      " Dropout1D)                                                                                       \n",
      "                                                                                                  \n",
      " spatial_dropout1d_115 (Spatial  (None, 18, 100)     0           ['concatenate_240[0][0]']        \n",
      " Dropout1D)                                                                                       \n",
      "                                                                                                  \n",
      " conv1d_48 (Conv1D)             (None, 18, 50)       10050       ['spatial_dropout1d_94[0][0]',   \n",
      "                                                                  'spatial_dropout1d_95[0][0]']   \n",
      "                                                                                                  \n",
      " conv1d_53 (Conv1D)             (None, 18, 50)       15050       ['spatial_dropout1d_104[0][0]',  \n",
      "                                                                  'spatial_dropout1d_105[0][0]']  \n",
      "                                                                                                  \n",
      " conv1d_58 (Conv1D)             (None, 18, 50)       20050       ['spatial_dropout1d_114[0][0]',  \n",
      "                                                                  'spatial_dropout1d_115[0][0]']  \n",
      "                                                                                                  \n",
      " max_pooling1d_96 (MaxPooling1D  (None, 4, 50)       0           ['conv1d_48[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " average_pooling1d_96 (AverageP  (None, 4, 50)       0           ['conv1d_48[0][0]']              \n",
      " ooling1D)                                                                                        \n",
      "                                                                                                  \n",
      " max_pooling1d_106 (MaxPooling1  (None, 4, 50)       0           ['conv1d_53[0][0]']              \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " average_pooling1d_106 (Average  (None, 4, 50)       0           ['conv1d_53[0][0]']              \n",
      " Pooling1D)                                                                                       \n",
      "                                                                                                  \n",
      " max_pooling1d_116 (MaxPooling1  (None, 4, 50)       0           ['conv1d_58[0][0]']              \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " average_pooling1d_116 (Average  (None, 4, 50)       0           ['conv1d_58[0][0]']              \n",
      " Pooling1D)                                                                                       \n",
      "                                                                                                  \n",
      " max_pooling1d_97 (MaxPooling1D  (None, 4, 50)       0           ['conv1d_48[1][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " average_pooling1d_97 (AverageP  (None, 4, 50)       0           ['conv1d_48[1][0]']              \n",
      " ooling1D)                                                                                        \n",
      "                                                                                                  \n",
      " max_pooling1d_107 (MaxPooling1  (None, 4, 50)       0           ['conv1d_53[1][0]']              \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " average_pooling1d_107 (Average  (None, 4, 50)       0           ['conv1d_53[1][0]']              \n",
      " Pooling1D)                                                                                       \n",
      "                                                                                                  \n",
      " max_pooling1d_117 (MaxPooling1  (None, 4, 50)       0           ['conv1d_58[1][0]']              \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " average_pooling1d_117 (Average  (None, 4, 50)       0           ['conv1d_58[1][0]']              \n",
      " Pooling1D)                                                                                       \n",
      "                                                                                                  \n",
      " concatenate_203 (Concatenate)  (None, 4, 100)       0           ['max_pooling1d_96[0][0]',       \n",
      "                                                                  'average_pooling1d_96[0][0]']   \n",
      "                                                                                                  \n",
      " concatenate_223 (Concatenate)  (None, 4, 100)       0           ['max_pooling1d_106[0][0]',      \n",
      "                                                                  'average_pooling1d_106[0][0]']  \n",
      "                                                                                                  \n",
      " concatenate_243 (Concatenate)  (None, 4, 100)       0           ['max_pooling1d_116[0][0]',      \n",
      "                                                                  'average_pooling1d_116[0][0]']  \n",
      "                                                                                                  \n",
      " concatenate_204 (Concatenate)  (None, 4, 100)       0           ['max_pooling1d_97[0][0]',       \n",
      "                                                                  'average_pooling1d_97[0][0]']   \n",
      "                                                                                                  \n",
      " concatenate_224 (Concatenate)  (None, 4, 100)       0           ['max_pooling1d_107[0][0]',      \n",
      "                                                                  'average_pooling1d_107[0][0]']  \n",
      "                                                                                                  \n",
      " concatenate_244 (Concatenate)  (None, 4, 100)       0           ['max_pooling1d_117[0][0]',      \n",
      "                                                                  'average_pooling1d_117[0][0]']  \n",
      "                                                                                                  \n",
      " spatial_dropout1d_96 (SpatialD  (None, 4, 100)      0           ['concatenate_203[0][0]']        \n",
      " ropout1D)                                                                                        \n",
      "                                                                                                  \n",
      " spatial_dropout1d_106 (Spatial  (None, 4, 100)      0           ['concatenate_223[0][0]']        \n",
      " Dropout1D)                                                                                       \n",
      "                                                                                                  \n",
      " spatial_dropout1d_116 (Spatial  (None, 4, 100)      0           ['concatenate_243[0][0]']        \n",
      " Dropout1D)                                                                                       \n",
      "                                                                                                  \n",
      " spatial_dropout1d_97 (SpatialD  (None, 4, 100)      0           ['concatenate_204[0][0]']        \n",
      " ropout1D)                                                                                        \n",
      "                                                                                                  \n",
      " spatial_dropout1d_107 (Spatial  (None, 4, 100)      0           ['concatenate_224[0][0]']        \n",
      " Dropout1D)                                                                                       \n",
      "                                                                                                  \n",
      " spatial_dropout1d_117 (Spatial  (None, 4, 100)      0           ['concatenate_244[0][0]']        \n",
      " Dropout1D)                                                                                       \n",
      "                                                                                                  \n",
      " conv1d_49 (Conv1D)             (None, 4, 50)        10050       ['spatial_dropout1d_96[0][0]',   \n",
      "                                                                  'spatial_dropout1d_97[0][0]']   \n",
      "                                                                                                  \n",
      " conv1d_54 (Conv1D)             (None, 4, 50)        15050       ['spatial_dropout1d_106[0][0]',  \n",
      "                                                                  'spatial_dropout1d_107[0][0]']  \n",
      "                                                                                                  \n",
      " conv1d_59 (Conv1D)             (None, 4, 50)        20050       ['spatial_dropout1d_116[0][0]',  \n",
      "                                                                  'spatial_dropout1d_117[0][0]']  \n",
      "                                                                                                  \n",
      " global_max_pooling1d_90 (Globa  (None, 50)          0           ['conv1d_45[0][0]']              \n",
      " lMaxPooling1D)                                                                                   \n",
      "                                                                                                  \n",
      " global_average_pooling1d_180 (  (None, 50)          0           ['conv1d_45[0][0]']              \n",
      " GlobalAveragePooling1D)                                                                          \n",
      "                                                                                                  \n",
      " global_max_pooling1d_92 (Globa  (None, 50)          0           ['conv1d_46[0][0]']              \n",
      " lMaxPooling1D)                                                                                   \n",
      "                                                                                                  \n",
      " global_average_pooling1d_184 (  (None, 50)          0           ['conv1d_46[0][0]']              \n",
      " GlobalAveragePooling1D)                                                                          \n",
      "                                                                                                  \n",
      " global_max_pooling1d_94 (Globa  (None, 50)          0           ['conv1d_47[0][0]']              \n",
      " lMaxPooling1D)                                                                                   \n",
      "                                                                                                  \n",
      " global_average_pooling1d_188 (  (None, 50)          0           ['conv1d_47[0][0]']              \n",
      " GlobalAveragePooling1D)                                                                          \n",
      "                                                                                                  \n",
      " global_max_pooling1d_96 (Globa  (None, 50)          0           ['conv1d_48[0][0]']              \n",
      " lMaxPooling1D)                                                                                   \n",
      "                                                                                                  \n",
      " global_average_pooling1d_192 (  (None, 50)          0           ['conv1d_48[0][0]']              \n",
      " GlobalAveragePooling1D)                                                                          \n",
      "                                                                                                  \n",
      " global_max_pooling1d_98 (Globa  (None, 50)          0           ['conv1d_49[0][0]']              \n",
      " lMaxPooling1D)                                                                                   \n",
      "                                                                                                  \n",
      " global_average_pooling1d_196 (  (None, 50)          0           ['conv1d_49[0][0]']              \n",
      " GlobalAveragePooling1D)                                                                          \n",
      "                                                                                                  \n",
      " global_max_pooling1d_100 (Glob  (None, 50)          0           ['conv1d_50[0][0]']              \n",
      " alMaxPooling1D)                                                                                  \n",
      "                                                                                                  \n",
      " global_average_pooling1d_200 (  (None, 50)          0           ['conv1d_50[0][0]']              \n",
      " GlobalAveragePooling1D)                                                                          \n",
      "                                                                                                  \n",
      " global_max_pooling1d_102 (Glob  (None, 50)          0           ['conv1d_51[0][0]']              \n",
      " alMaxPooling1D)                                                                                  \n",
      "                                                                                                  \n",
      " global_average_pooling1d_204 (  (None, 50)          0           ['conv1d_51[0][0]']              \n",
      " GlobalAveragePooling1D)                                                                          \n",
      "                                                                                                  \n",
      " global_max_pooling1d_104 (Glob  (None, 50)          0           ['conv1d_52[0][0]']              \n",
      " alMaxPooling1D)                                                                                  \n",
      "                                                                                                  \n",
      " global_average_pooling1d_208 (  (None, 50)          0           ['conv1d_52[0][0]']              \n",
      " GlobalAveragePooling1D)                                                                          \n",
      "                                                                                                  \n",
      " global_max_pooling1d_106 (Glob  (None, 50)          0           ['conv1d_53[0][0]']              \n",
      " alMaxPooling1D)                                                                                  \n",
      "                                                                                                  \n",
      " global_average_pooling1d_212 (  (None, 50)          0           ['conv1d_53[0][0]']              \n",
      " GlobalAveragePooling1D)                                                                          \n",
      "                                                                                                  \n",
      " global_max_pooling1d_108 (Glob  (None, 50)          0           ['conv1d_54[0][0]']              \n",
      " alMaxPooling1D)                                                                                  \n",
      "                                                                                                  \n",
      " global_average_pooling1d_216 (  (None, 50)          0           ['conv1d_54[0][0]']              \n",
      " GlobalAveragePooling1D)                                                                          \n",
      "                                                                                                  \n",
      " global_max_pooling1d_110 (Glob  (None, 50)          0           ['conv1d_55[0][0]']              \n",
      " alMaxPooling1D)                                                                                  \n",
      "                                                                                                  \n",
      " global_average_pooling1d_220 (  (None, 50)          0           ['conv1d_55[0][0]']              \n",
      " GlobalAveragePooling1D)                                                                          \n",
      "                                                                                                  \n",
      " global_max_pooling1d_112 (Glob  (None, 50)          0           ['conv1d_56[0][0]']              \n",
      " alMaxPooling1D)                                                                                  \n",
      "                                                                                                  \n",
      " global_average_pooling1d_224 (  (None, 50)          0           ['conv1d_56[0][0]']              \n",
      " GlobalAveragePooling1D)                                                                          \n",
      "                                                                                                  \n",
      " global_max_pooling1d_114 (Glob  (None, 50)          0           ['conv1d_57[0][0]']              \n",
      " alMaxPooling1D)                                                                                  \n",
      "                                                                                                  \n",
      " global_average_pooling1d_228 (  (None, 50)          0           ['conv1d_57[0][0]']              \n",
      " GlobalAveragePooling1D)                                                                          \n",
      "                                                                                                  \n",
      " global_max_pooling1d_116 (Glob  (None, 50)          0           ['conv1d_58[0][0]']              \n",
      " alMaxPooling1D)                                                                                  \n",
      "                                                                                                  \n",
      " global_average_pooling1d_232 (  (None, 50)          0           ['conv1d_58[0][0]']              \n",
      " GlobalAveragePooling1D)                                                                          \n",
      "                                                                                                  \n",
      " global_max_pooling1d_118 (Glob  (None, 50)          0           ['conv1d_59[0][0]']              \n",
      " alMaxPooling1D)                                                                                  \n",
      "                                                                                                  \n",
      " global_average_pooling1d_236 (  (None, 50)          0           ['conv1d_59[0][0]']              \n",
      " GlobalAveragePooling1D)                                                                          \n",
      "                                                                                                  \n",
      " global_max_pooling1d_91 (Globa  (None, 50)          0           ['conv1d_45[1][0]']              \n",
      " lMaxPooling1D)                                                                                   \n",
      "                                                                                                  \n",
      " global_average_pooling1d_182 (  (None, 50)          0           ['conv1d_45[1][0]']              \n",
      " GlobalAveragePooling1D)                                                                          \n",
      "                                                                                                  \n",
      " global_max_pooling1d_93 (Globa  (None, 50)          0           ['conv1d_46[1][0]']              \n",
      " lMaxPooling1D)                                                                                   \n",
      "                                                                                                  \n",
      " global_average_pooling1d_186 (  (None, 50)          0           ['conv1d_46[1][0]']              \n",
      " GlobalAveragePooling1D)                                                                          \n",
      "                                                                                                  \n",
      " global_max_pooling1d_95 (Globa  (None, 50)          0           ['conv1d_47[1][0]']              \n",
      " lMaxPooling1D)                                                                                   \n",
      "                                                                                                  \n",
      " global_average_pooling1d_190 (  (None, 50)          0           ['conv1d_47[1][0]']              \n",
      " GlobalAveragePooling1D)                                                                          \n",
      "                                                                                                  \n",
      " global_max_pooling1d_97 (Globa  (None, 50)          0           ['conv1d_48[1][0]']              \n",
      " lMaxPooling1D)                                                                                   \n",
      "                                                                                                  \n",
      " global_average_pooling1d_194 (  (None, 50)          0           ['conv1d_48[1][0]']              \n",
      " GlobalAveragePooling1D)                                                                          \n",
      "                                                                                                  \n",
      " global_max_pooling1d_99 (Globa  (None, 50)          0           ['conv1d_49[1][0]']              \n",
      " lMaxPooling1D)                                                                                   \n",
      "                                                                                                  \n",
      " global_average_pooling1d_198 (  (None, 50)          0           ['conv1d_49[1][0]']              \n",
      " GlobalAveragePooling1D)                                                                          \n",
      "                                                                                                  \n",
      " global_max_pooling1d_101 (Glob  (None, 50)          0           ['conv1d_50[1][0]']              \n",
      " alMaxPooling1D)                                                                                  \n",
      "                                                                                                  \n",
      " global_average_pooling1d_202 (  (None, 50)          0           ['conv1d_50[1][0]']              \n",
      " GlobalAveragePooling1D)                                                                          \n",
      "                                                                                                  \n",
      " global_max_pooling1d_103 (Glob  (None, 50)          0           ['conv1d_51[1][0]']              \n",
      " alMaxPooling1D)                                                                                  \n",
      "                                                                                                  \n",
      " global_average_pooling1d_206 (  (None, 50)          0           ['conv1d_51[1][0]']              \n",
      " GlobalAveragePooling1D)                                                                          \n",
      "                                                                                                  \n",
      " global_max_pooling1d_105 (Glob  (None, 50)          0           ['conv1d_52[1][0]']              \n",
      " alMaxPooling1D)                                                                                  \n",
      "                                                                                                  \n",
      " global_average_pooling1d_210 (  (None, 50)          0           ['conv1d_52[1][0]']              \n",
      " GlobalAveragePooling1D)                                                                          \n",
      "                                                                                                  \n",
      " global_max_pooling1d_107 (Glob  (None, 50)          0           ['conv1d_53[1][0]']              \n",
      " alMaxPooling1D)                                                                                  \n",
      "                                                                                                  \n",
      " global_average_pooling1d_214 (  (None, 50)          0           ['conv1d_53[1][0]']              \n",
      " GlobalAveragePooling1D)                                                                          \n",
      "                                                                                                  \n",
      " global_max_pooling1d_109 (Glob  (None, 50)          0           ['conv1d_54[1][0]']              \n",
      " alMaxPooling1D)                                                                                  \n",
      "                                                                                                  \n",
      " global_average_pooling1d_218 (  (None, 50)          0           ['conv1d_54[1][0]']              \n",
      " GlobalAveragePooling1D)                                                                          \n",
      "                                                                                                  \n",
      " global_max_pooling1d_111 (Glob  (None, 50)          0           ['conv1d_55[1][0]']              \n",
      " alMaxPooling1D)                                                                                  \n",
      "                                                                                                  \n",
      " global_average_pooling1d_222 (  (None, 50)          0           ['conv1d_55[1][0]']              \n",
      " GlobalAveragePooling1D)                                                                          \n",
      "                                                                                                  \n",
      " global_max_pooling1d_113 (Glob  (None, 50)          0           ['conv1d_56[1][0]']              \n",
      " alMaxPooling1D)                                                                                  \n",
      "                                                                                                  \n",
      " global_average_pooling1d_226 (  (None, 50)          0           ['conv1d_56[1][0]']              \n",
      " GlobalAveragePooling1D)                                                                          \n",
      "                                                                                                  \n",
      " global_max_pooling1d_115 (Glob  (None, 50)          0           ['conv1d_57[1][0]']              \n",
      " alMaxPooling1D)                                                                                  \n",
      "                                                                                                  \n",
      " global_average_pooling1d_230 (  (None, 50)          0           ['conv1d_57[1][0]']              \n",
      " GlobalAveragePooling1D)                                                                          \n",
      "                                                                                                  \n",
      " global_max_pooling1d_117 (Glob  (None, 50)          0           ['conv1d_58[1][0]']              \n",
      " alMaxPooling1D)                                                                                  \n",
      "                                                                                                  \n",
      " global_average_pooling1d_234 (  (None, 50)          0           ['conv1d_58[1][0]']              \n",
      " GlobalAveragePooling1D)                                                                          \n",
      "                                                                                                  \n",
      " global_max_pooling1d_119 (Glob  (None, 50)          0           ['conv1d_59[1][0]']              \n",
      " alMaxPooling1D)                                                                                  \n",
      "                                                                                                  \n",
      " global_average_pooling1d_238 (  (None, 50)          0           ['conv1d_59[1][0]']              \n",
      " GlobalAveragePooling1D)                                                                          \n",
      "                                                                                                  \n",
      " concatenate_189 (Concatenate)  (None, 100)          0           ['global_max_pooling1d_90[0][0]',\n",
      "                                                                  'global_average_pooling1d_180[0]\n",
      "                                                                 [0]']                            \n",
      "                                                                                                  \n",
      " concatenate_193 (Concatenate)  (None, 100)          0           ['global_max_pooling1d_92[0][0]',\n",
      "                                                                  'global_average_pooling1d_184[0]\n",
      "                                                                 [0]']                            \n",
      "                                                                                                  \n",
      " concatenate_197 (Concatenate)  (None, 100)          0           ['global_max_pooling1d_94[0][0]',\n",
      "                                                                  'global_average_pooling1d_188[0]\n",
      "                                                                 [0]']                            \n",
      "                                                                                                  \n",
      " concatenate_201 (Concatenate)  (None, 100)          0           ['global_max_pooling1d_96[0][0]',\n",
      "                                                                  'global_average_pooling1d_192[0]\n",
      "                                                                 [0]']                            \n",
      "                                                                                                  \n",
      " concatenate_205 (Concatenate)  (None, 100)          0           ['global_max_pooling1d_98[0][0]',\n",
      "                                                                  'global_average_pooling1d_196[0]\n",
      "                                                                 [0]']                            \n",
      "                                                                                                  \n",
      " concatenate_209 (Concatenate)  (None, 100)          0           ['global_max_pooling1d_100[0][0]'\n",
      "                                                                 , 'global_average_pooling1d_200[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " concatenate_213 (Concatenate)  (None, 100)          0           ['global_max_pooling1d_102[0][0]'\n",
      "                                                                 , 'global_average_pooling1d_204[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " concatenate_217 (Concatenate)  (None, 100)          0           ['global_max_pooling1d_104[0][0]'\n",
      "                                                                 , 'global_average_pooling1d_208[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " concatenate_221 (Concatenate)  (None, 100)          0           ['global_max_pooling1d_106[0][0]'\n",
      "                                                                 , 'global_average_pooling1d_212[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " concatenate_225 (Concatenate)  (None, 100)          0           ['global_max_pooling1d_108[0][0]'\n",
      "                                                                 , 'global_average_pooling1d_216[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " concatenate_229 (Concatenate)  (None, 100)          0           ['global_max_pooling1d_110[0][0]'\n",
      "                                                                 , 'global_average_pooling1d_220[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " concatenate_233 (Concatenate)  (None, 100)          0           ['global_max_pooling1d_112[0][0]'\n",
      "                                                                 , 'global_average_pooling1d_224[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " concatenate_237 (Concatenate)  (None, 100)          0           ['global_max_pooling1d_114[0][0]'\n",
      "                                                                 , 'global_average_pooling1d_228[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " concatenate_241 (Concatenate)  (None, 100)          0           ['global_max_pooling1d_116[0][0]'\n",
      "                                                                 , 'global_average_pooling1d_232[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " concatenate_245 (Concatenate)  (None, 100)          0           ['global_max_pooling1d_118[0][0]'\n",
      "                                                                 , 'global_average_pooling1d_236[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " concatenate_190 (Concatenate)  (None, 100)          0           ['global_max_pooling1d_91[0][0]',\n",
      "                                                                  'global_average_pooling1d_182[0]\n",
      "                                                                 [0]']                            \n",
      "                                                                                                  \n",
      " concatenate_194 (Concatenate)  (None, 100)          0           ['global_max_pooling1d_93[0][0]',\n",
      "                                                                  'global_average_pooling1d_186[0]\n",
      "                                                                 [0]']                            \n",
      "                                                                                                  \n",
      " concatenate_198 (Concatenate)  (None, 100)          0           ['global_max_pooling1d_95[0][0]',\n",
      "                                                                  'global_average_pooling1d_190[0]\n",
      "                                                                 [0]']                            \n",
      "                                                                                                  \n",
      " concatenate_202 (Concatenate)  (None, 100)          0           ['global_max_pooling1d_97[0][0]',\n",
      "                                                                  'global_average_pooling1d_194[0]\n",
      "                                                                 [0]']                            \n",
      "                                                                                                  \n",
      " concatenate_206 (Concatenate)  (None, 100)          0           ['global_max_pooling1d_99[0][0]',\n",
      "                                                                  'global_average_pooling1d_198[0]\n",
      "                                                                 [0]']                            \n",
      "                                                                                                  \n",
      " concatenate_210 (Concatenate)  (None, 100)          0           ['global_max_pooling1d_101[0][0]'\n",
      "                                                                 , 'global_average_pooling1d_202[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " concatenate_214 (Concatenate)  (None, 100)          0           ['global_max_pooling1d_103[0][0]'\n",
      "                                                                 , 'global_average_pooling1d_206[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " concatenate_218 (Concatenate)  (None, 100)          0           ['global_max_pooling1d_105[0][0]'\n",
      "                                                                 , 'global_average_pooling1d_210[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " concatenate_222 (Concatenate)  (None, 100)          0           ['global_max_pooling1d_107[0][0]'\n",
      "                                                                 , 'global_average_pooling1d_214[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " concatenate_226 (Concatenate)  (None, 100)          0           ['global_max_pooling1d_109[0][0]'\n",
      "                                                                 , 'global_average_pooling1d_218[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " concatenate_230 (Concatenate)  (None, 100)          0           ['global_max_pooling1d_111[0][0]'\n",
      "                                                                 , 'global_average_pooling1d_222[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " concatenate_234 (Concatenate)  (None, 100)          0           ['global_max_pooling1d_113[0][0]'\n",
      "                                                                 , 'global_average_pooling1d_226[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " concatenate_238 (Concatenate)  (None, 100)          0           ['global_max_pooling1d_115[0][0]'\n",
      "                                                                 , 'global_average_pooling1d_230[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " concatenate_242 (Concatenate)  (None, 100)          0           ['global_max_pooling1d_117[0][0]'\n",
      "                                                                 , 'global_average_pooling1d_234[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " concatenate_246 (Concatenate)  (None, 100)          0           ['global_max_pooling1d_119[0][0]'\n",
      "                                                                 , 'global_average_pooling1d_238[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " dropout_120 (Dropout)          (None, 100)          0           ['concatenate_189[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_122 (Dropout)          (None, 100)          0           ['concatenate_193[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_124 (Dropout)          (None, 100)          0           ['concatenate_197[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_126 (Dropout)          (None, 100)          0           ['concatenate_201[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_128 (Dropout)          (None, 100)          0           ['concatenate_205[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_130 (Dropout)          (None, 100)          0           ['concatenate_209[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_132 (Dropout)          (None, 100)          0           ['concatenate_213[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_134 (Dropout)          (None, 100)          0           ['concatenate_217[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_136 (Dropout)          (None, 100)          0           ['concatenate_221[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_138 (Dropout)          (None, 100)          0           ['concatenate_225[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_140 (Dropout)          (None, 100)          0           ['concatenate_229[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_142 (Dropout)          (None, 100)          0           ['concatenate_233[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_144 (Dropout)          (None, 100)          0           ['concatenate_237[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_146 (Dropout)          (None, 100)          0           ['concatenate_241[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_148 (Dropout)          (None, 100)          0           ['concatenate_245[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_121 (Dropout)          (None, 100)          0           ['concatenate_190[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_123 (Dropout)          (None, 100)          0           ['concatenate_194[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_125 (Dropout)          (None, 100)          0           ['concatenate_198[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_127 (Dropout)          (None, 100)          0           ['concatenate_202[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_129 (Dropout)          (None, 100)          0           ['concatenate_206[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_131 (Dropout)          (None, 100)          0           ['concatenate_210[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_133 (Dropout)          (None, 100)          0           ['concatenate_214[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_135 (Dropout)          (None, 100)          0           ['concatenate_218[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_137 (Dropout)          (None, 100)          0           ['concatenate_222[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_139 (Dropout)          (None, 100)          0           ['concatenate_226[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_141 (Dropout)          (None, 100)          0           ['concatenate_230[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_143 (Dropout)          (None, 100)          0           ['concatenate_234[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_145 (Dropout)          (None, 100)          0           ['concatenate_238[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_147 (Dropout)          (None, 100)          0           ['concatenate_242[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_149 (Dropout)          (None, 100)          0           ['concatenate_246[0][0]']        \n",
      "                                                                                                  \n",
      " concatenate_249 (Concatenate)  (None, 1500)         0           ['dropout_120[0][0]',            \n",
      "                                                                  'dropout_122[0][0]',            \n",
      "                                                                  'dropout_124[0][0]',            \n",
      "                                                                  'dropout_126[0][0]',            \n",
      "                                                                  'dropout_128[0][0]',            \n",
      "                                                                  'dropout_130[0][0]',            \n",
      "                                                                  'dropout_132[0][0]',            \n",
      "                                                                  'dropout_134[0][0]',            \n",
      "                                                                  'dropout_136[0][0]',            \n",
      "                                                                  'dropout_138[0][0]',            \n",
      "                                                                  'dropout_140[0][0]',            \n",
      "                                                                  'dropout_142[0][0]',            \n",
      "                                                                  'dropout_144[0][0]',            \n",
      "                                                                  'dropout_146[0][0]',            \n",
      "                                                                  'dropout_148[0][0]']            \n",
      "                                                                                                  \n",
      " concatenate_250 (Concatenate)  (None, 1500)         0           ['dropout_121[0][0]',            \n",
      "                                                                  'dropout_123[0][0]',            \n",
      "                                                                  'dropout_125[0][0]',            \n",
      "                                                                  'dropout_127[0][0]',            \n",
      "                                                                  'dropout_129[0][0]',            \n",
      "                                                                  'dropout_131[0][0]',            \n",
      "                                                                  'dropout_133[0][0]',            \n",
      "                                                                  'dropout_135[0][0]',            \n",
      "                                                                  'dropout_137[0][0]',            \n",
      "                                                                  'dropout_139[0][0]',            \n",
      "                                                                  'dropout_141[0][0]',            \n",
      "                                                                  'dropout_143[0][0]',            \n",
      "                                                                  'dropout_145[0][0]',            \n",
      "                                                                  'dropout_147[0][0]',            \n",
      "                                                                  'dropout_149[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 1500)        6000        ['concatenate_249[0][0]']        \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 1500)        6000        ['concatenate_250[0][0]']        \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_150 (Dropout)          (None, 1500)         0           ['batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_151 (Dropout)          (None, 1500)         0           ['batch_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " dense_18 (Dense)               (None, 744)          1116744     ['dropout_150[0][0]',            \n",
      "                                                                  'dropout_151[0][0]']            \n",
      "                                                                                                  \n",
      " dropout_152 (Dropout)          (None, 744)          0           ['dense_18[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_153 (Dropout)          (None, 744)          0           ['dense_18[1][0]']               \n",
      "                                                                                                  \n",
      " dense_19 (Dense)               (None, 372)          277140      ['dropout_152[0][0]',            \n",
      "                                                                  'dropout_153[0][0]']            \n",
      "                                                                                                  \n",
      " dropout_154 (Dropout)          (None, 372)          0           ['dense_19[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_155 (Dropout)          (None, 372)          0           ['dense_19[1][0]']               \n",
      "                                                                                                  \n",
      " dense_20 (Dense)               (None, 186)          69378       ['dropout_154[0][0]',            \n",
      "                                                                  'dropout_155[0][0]']            \n",
      "                                                                                                  \n",
      " dropout_156 (Dropout)          (None, 186)          0           ['dense_20[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_157 (Dropout)          (None, 186)          0           ['dense_20[1][0]']               \n",
      "                                                                                                  \n",
      " concatenate_251 (Concatenate)  (None, 372)          0           ['dropout_156[0][0]',            \n",
      "                                                                  'dropout_157[0][0]']            \n",
      "                                                                                                  \n",
      " dense_21 (Dense)               (None, 328)          122344      ['concatenate_251[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_158 (Dropout)          (None, 328)          0           ['dense_21[0][0]']               \n",
      "                                                                                                  \n",
      " dense_22 (Dense)               (None, 164)          53956       ['dropout_158[0][0]']            \n",
      "                                                                                                  \n",
      " dropout_159 (Dropout)          (None, 164)          0           ['dense_22[0][0]']               \n",
      "                                                                                                  \n",
      " dense_23 (Dense)               (None, 1)            165         ['dropout_159[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,293,277\n",
      "Trainable params: 2,287,277\n",
      "Non-trainable params: 6,000\n",
      "__________________________________________________________________________________________________\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "seq_size = 1200\n",
    "dim = 1024\n",
    "def multi_cnn():\n",
    "    DEPTH = 5\n",
    "    WIDTH = 3\n",
    "    POOLING_SIZE = 4\n",
    "    FILTERS = 50\n",
    "    KERNEL_SIZE = 2\n",
    "    DEPTH_DENSE1 = 3\n",
    "    DEPTH_DENSE2 = 2\n",
    "    DROPOUT = DROPOUT1 = DROPOUT2 = 0.05\n",
    "    DROPOUT_SPATIAL= 0.15\n",
    "    ACTIVATION = 'swish'\n",
    "    ACTIVATION_CNN = 'swish'\n",
    "    INITIALIZER = 'glorot_normal'\n",
    "    \n",
    "    def BlockCNN_single(KERNEL_SIZE, POOLING_SIZE, FILTERS, LAYER_IN1, LAYER_IN2):\n",
    "        c1 = Conv1D(filters=FILTERS, kernel_size=KERNEL_SIZE, activation=ACTIVATION_CNN, padding='same')\n",
    "        x1 = c1(LAYER_IN1)\n",
    "        x2 = c1(LAYER_IN2)\n",
    "\n",
    "        g1 = Dropout(DROPOUT)(concatenate([GlobalMaxPooling1D()(x1),GlobalAveragePooling1D()(x1)]))\n",
    "        a1 = GlobalAveragePooling1D()(x1)\n",
    "        g2 = Dropout(DROPOUT)(concatenate([GlobalMaxPooling1D()(x2),GlobalAveragePooling1D()(x2)]))\n",
    "        a2 = GlobalAveragePooling1D()(x1)\n",
    "\n",
    "        x1 = SpatialDropout1D(DROPOUT_SPATIAL)(concatenate([MaxPooling1D(POOLING_SIZE)(x1), AveragePooling1D(POOLING_SIZE)(x1)]))\n",
    "        x2 = SpatialDropout1D(DROPOUT_SPATIAL)(concatenate([MaxPooling1D(POOLING_SIZE)(x2), AveragePooling1D(POOLING_SIZE)(x2)]))\n",
    "\n",
    "        return x1, x2, g1, g2, a1, a2\n",
    "\n",
    "    def BlockCNN_multi(POOLING_SIZE, FILTERS, LAYER_IN1, LAYER_IN2, WIDTH):\n",
    "      X1 = []\n",
    "      X2 = []\n",
    "      G1 = []\n",
    "      G2 = []\n",
    "      A1 = []\n",
    "      A2 = []\n",
    "      for i in range(2, 2+WIDTH):\n",
    "        x1, x2, g1, g2, a1, a2 = BlockCNN_single(i, POOLING_SIZE, FILTERS, LAYER_IN1, LAYER_IN2)\n",
    "        X1.append(x1)\n",
    "        X2.append(x2)\n",
    "        G1.append(g1)\n",
    "        G2.append(g2)\n",
    "        A1.append(a1)\n",
    "        A2.append(a2)\n",
    "      x1 = concatenate(X1)\n",
    "      x2 = concatenate(X2)\n",
    "      g1 = GlobalMaxPooling1D()(x1)\n",
    "      g2 = GlobalMaxPooling1D()(x2)\n",
    "      return x1, x2, g1, g2\n",
    "\n",
    "    def BlockCNN_single_deep(KERNEL_SIZE, POOLING_SIZE, DEPTH, FILTERS, LAYER_IN1, LAYER_IN2):\n",
    "      X1 = []\n",
    "      X2 = []\n",
    "      G1 = []\n",
    "      G2 = []\n",
    "      A1 = []\n",
    "      A2 = []\n",
    "      x1 = LAYER_IN1\n",
    "      x2 = LAYER_IN2\n",
    "      for i in range(DEPTH):\n",
    "        x1, x2, g1, g2, a1, a2 = BlockCNN_single(KERNEL_SIZE, POOLING_SIZE, FILTERS, x1, x2)\n",
    "        X1.append(x1)\n",
    "        X2.append(x2)\n",
    "        G1.append(g1)\n",
    "        G2.append(g2)\n",
    "        A1.append(a1)\n",
    "        A2.append(a2)\n",
    "\n",
    "      return X1, X2, G1, G2, A1, A2\n",
    "\n",
    "    input1 = Input(shape=(seq_size, dim), name=\"seq1\")\n",
    "    input2 = Input(shape=(seq_size, dim), name=\"seq2\")\n",
    "    \n",
    "\n",
    "\n",
    "    X1 = dict()\n",
    "    X2 = dict()\n",
    "    G1 = dict()\n",
    "    G2 = dict()\n",
    "    A1 = dict()\n",
    "    A2 = dict()\n",
    "\n",
    "    for i in range(KERNEL_SIZE, KERNEL_SIZE+WIDTH):\n",
    "      X1[f'{i}'], X2[f'{i}'], G1[f'{i}'], G2[f'{i}'], A1[f'{i}'], A2[f'{i}'] = BlockCNN_single_deep(i, POOLING_SIZE, DEPTH, FILTERS, input1, input2)\n",
    "\n",
    "    s1 = []\n",
    "    s2 = []\n",
    "    for i in range(KERNEL_SIZE, KERNEL_SIZE+WIDTH):\n",
    "      s1.extend(G1[f'{i}'])\n",
    "      s2.extend(G2[f'{i}'])\n",
    "\n",
    "    s1 = concatenate(s1)\n",
    "    s2 = concatenate(s2)\n",
    "    \n",
    "    s1 = BatchNormalization(momentum=.9)(s1)\n",
    "    s2 = BatchNormalization(momentum=.9)(s2)\n",
    "\n",
    "    s1 = Dropout(DROPOUT1)(s1)\n",
    "    s2 = Dropout(DROPOUT1)(s2)\n",
    "    \n",
    "    s1_shape = s1.shape[-1]\n",
    "    DENSE1 = 744 \n",
    "    d1 = []\n",
    "    for i in range(DEPTH_DENSE1):\n",
    "        d1.append(Dense(int(DENSE1*(1/2)**i), kernel_initializer=INITIALIZER, activation=ACTIVATION))\n",
    "\n",
    "    for i in range(DEPTH_DENSE1):\n",
    "        s1 = d1[i](s1)\n",
    "        s2 = d1[i](s2)\n",
    "        s1 = Dropout(DROPOUT1)(s1)\n",
    "        s2 = Dropout(DROPOUT1)(s2)\n",
    "        \n",
    "    s = concatenate([s1, s2])\n",
    "\n",
    "    \n",
    "    s_shape = s.shape[-1]\n",
    "    DENSE2 = 328\n",
    "        \n",
    "    d2 = []\n",
    "    for i in range(DEPTH_DENSE2):\n",
    "        d2.append(Dense(int(DENSE2*(1/2)**i), kernel_initializer=INITIALIZER, activation=ACTIVATION))\n",
    "\n",
    "    for i in range(DEPTH_DENSE2):\n",
    "        s = d2[i](s)\n",
    "        s = Dropout(DROPOUT2)(s)\n",
    "\n",
    "    output = Dense(1, activation='sigmoid')(s)\n",
    "    model = Model(inputs=[input1, input2], outputs=[output])\n",
    "    \n",
    "    adam = Adam(learning_rate=1e-3, amsgrad=True, epsilon=1e-6)\n",
    "    model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "model = multi_cnn()\n",
    "model.summary()\n",
    "tf.keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MShfiXzf8wTy"
   },
   "source": [
    "### Function for mapping in dataset pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "tkVGXUOtWkny"
   },
   "outputs": [],
   "source": [
    "def func(i):\n",
    "    i = i.numpy() # Decoding from the EagerTensor object\n",
    "    x1= pad(embedding_dict[pair_dataframe['p1'][i]])\n",
    "    x2= pad(embedding_dict[pair_dataframe['p2'][i]])\n",
    "    y = pair_dataframe['label'][i]\n",
    "    return x1, x2, y\n",
    "\n",
    "def _fixup_shape(x1, x2, y):\n",
    "    x1.set_shape((seq_size, dim))\n",
    "    x2.set_shape((seq_size, dim)) \n",
    "    y.set_shape(()) \n",
    "\n",
    "    return (x1, x2), y\n",
    "\n",
    "def pad(rst, length=1200, dim=1024):\n",
    "    if len(rst) > length:\n",
    "        return rst[:length]\n",
    "    elif len(rst) < length:\n",
    "        return np.concatenate((rst, np.zeros((length - len(rst), dim))))\n",
    "    return rst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zg4LqOpKtUj9"
   },
   "source": [
    "### 5-fold cross validation DL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "s9PfppUh2sFI"
   },
   "outputs": [],
   "source": [
    "# Use intermediate layer to transform pairs matrix\n",
    "BATCH_SIZE = 64\n",
    "pred_dataset = tf.data.Dataset.from_generator(lambda: range(len(pair_dataframe)), tf.uint32)\n",
    "\n",
    "pred_dataset = pred_dataset.map(lambda i: tf.py_function(func=func, \n",
    "                                              inp=[i], \n",
    "                                              Tout=[tf.float16,\n",
    "                                                    tf.float16, tf.float16]\n",
    "                                              ), \n",
    "                      num_parallel_calls=tf.data.AUTOTUNE).map(_fixup_shape).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pMR7FC3XHLiX",
    "outputId": "329dc163-0965-453e-f313-52051490015d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================== FOLD 1 ==================================\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-02 02:40:19.067480: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2023-06-02 02:40:19.067715: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2023-06-02 02:40:30.556788: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x1777ad7a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-06-02 02:40:30.556859: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): NVIDIA GeForce RTX 4090, Compute Capability 8.9\n",
      "2023-06-02 02:40:30.592118: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-06-02 02:40:30.803883: I ./tensorflow/compiler/jit/device_compiler.h:180] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    140/Unknown - 51s 248ms/step - loss: 0.5720 - accuracy: 0.6914"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-02 02:41:10.399904: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2023-06-02 02:41:10.400127: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 0.45518, saving model to my_best_model_fold_1.hdf5\n",
      "140/140 [==============================] - 61s 318ms/step - loss: 0.5720 - accuracy: 0.6914 - val_loss: 0.4552 - val_accuracy: 0.8088\n",
      "Epoch 2/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.3209 - accuracy: 0.8682\n",
      "Epoch 2: val_loss improved from 0.45518 to 0.23755, saving model to my_best_model_fold_1.hdf5\n",
      "140/140 [==============================] - 44s 317ms/step - loss: 0.3209 - accuracy: 0.8682 - val_loss: 0.2375 - val_accuracy: 0.9138\n",
      "Epoch 3/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.2176 - accuracy: 0.9102\n",
      "Epoch 3: val_loss improved from 0.23755 to 0.16428, saving model to my_best_model_fold_1.hdf5\n",
      "140/140 [==============================] - 44s 314ms/step - loss: 0.2176 - accuracy: 0.9102 - val_loss: 0.1643 - val_accuracy: 0.9321\n",
      "Epoch 4/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.1844 - accuracy: 0.9215\n",
      "Epoch 4: val_loss improved from 0.16428 to 0.15727, saving model to my_best_model_fold_1.hdf5\n",
      "140/140 [==============================] - 45s 318ms/step - loss: 0.1844 - accuracy: 0.9215 - val_loss: 0.1573 - val_accuracy: 0.9231\n",
      "Epoch 5/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.1374 - accuracy: 0.9435\n",
      "Epoch 5: val_loss improved from 0.15727 to 0.11899, saving model to my_best_model_fold_1.hdf5\n",
      "140/140 [==============================] - 44s 315ms/step - loss: 0.1374 - accuracy: 0.9435 - val_loss: 0.1190 - val_accuracy: 0.9504\n",
      "Epoch 6/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.1219 - accuracy: 0.9502\n",
      "Epoch 6: val_loss improved from 0.11899 to 0.10863, saving model to my_best_model_fold_1.hdf5\n",
      "140/140 [==============================] - 44s 315ms/step - loss: 0.1219 - accuracy: 0.9502 - val_loss: 0.1086 - val_accuracy: 0.9531\n",
      "Epoch 7/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0978 - accuracy: 0.9606\n",
      "Epoch 7: val_loss did not improve from 0.10863\n",
      "140/140 [==============================] - 44s 314ms/step - loss: 0.0978 - accuracy: 0.9606 - val_loss: 0.1202 - val_accuracy: 0.9616\n",
      "Epoch 8/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0972 - accuracy: 0.9618\n",
      "Epoch 8: val_loss did not improve from 0.10863\n",
      "140/140 [==============================] - 44s 313ms/step - loss: 0.0972 - accuracy: 0.9618 - val_loss: 0.1102 - val_accuracy: 0.9678\n",
      "Epoch 9/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0909 - accuracy: 0.9650\n",
      "Epoch 9: val_loss did not improve from 0.10863\n",
      "140/140 [==============================] - 44s 311ms/step - loss: 0.0909 - accuracy: 0.9650 - val_loss: 0.1132 - val_accuracy: 0.9660\n",
      "Epoch 10/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0741 - accuracy: 0.9725\n",
      "Epoch 10: val_loss improved from 0.10863 to 0.09490, saving model to my_best_model_fold_1.hdf5\n",
      "140/140 [==============================] - 44s 316ms/step - loss: 0.0741 - accuracy: 0.9725 - val_loss: 0.0949 - val_accuracy: 0.9629\n",
      "Epoch 11/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0690 - accuracy: 0.9755\n",
      "Epoch 11: val_loss did not improve from 0.09490\n",
      "140/140 [==============================] - 44s 315ms/step - loss: 0.0690 - accuracy: 0.9755 - val_loss: 0.0967 - val_accuracy: 0.9651\n",
      "Epoch 12/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0586 - accuracy: 0.9789\n",
      "Epoch 12: val_loss improved from 0.09490 to 0.08523, saving model to my_best_model_fold_1.hdf5\n",
      "140/140 [==============================] - 43s 310ms/step - loss: 0.0586 - accuracy: 0.9789 - val_loss: 0.0852 - val_accuracy: 0.9683\n",
      "Epoch 13/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0564 - accuracy: 0.9764\n",
      "Epoch 13: val_loss improved from 0.08523 to 0.07384, saving model to my_best_model_fold_1.hdf5\n",
      "140/140 [==============================] - 44s 315ms/step - loss: 0.0564 - accuracy: 0.9764 - val_loss: 0.0738 - val_accuracy: 0.9754\n",
      "Epoch 14/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0512 - accuracy: 0.9809\n",
      "Epoch 14: val_loss improved from 0.07384 to 0.06548, saving model to my_best_model_fold_1.hdf5\n",
      "140/140 [==============================] - 44s 317ms/step - loss: 0.0512 - accuracy: 0.9809 - val_loss: 0.0655 - val_accuracy: 0.9777\n",
      "Epoch 15/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0490 - accuracy: 0.9812\n",
      "Epoch 15: val_loss did not improve from 0.06548\n",
      "140/140 [==============================] - 44s 312ms/step - loss: 0.0490 - accuracy: 0.9812 - val_loss: 0.0668 - val_accuracy: 0.9786\n",
      "Epoch 16/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0416 - accuracy: 0.9864\n",
      "Epoch 16: val_loss improved from 0.06548 to 0.04999, saving model to my_best_model_fold_1.hdf5\n",
      "140/140 [==============================] - 44s 313ms/step - loss: 0.0416 - accuracy: 0.9864 - val_loss: 0.0500 - val_accuracy: 0.9839\n",
      "Epoch 17/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0479 - accuracy: 0.9829\n",
      "Epoch 17: val_loss did not improve from 0.04999\n",
      "140/140 [==============================] - 44s 312ms/step - loss: 0.0479 - accuracy: 0.9829 - val_loss: 0.0837 - val_accuracy: 0.9750\n",
      "Epoch 18/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0434 - accuracy: 0.9849\n",
      "Epoch 18: val_loss did not improve from 0.04999\n",
      "140/140 [==============================] - 44s 313ms/step - loss: 0.0434 - accuracy: 0.9849 - val_loss: 0.0706 - val_accuracy: 0.9777\n",
      "Epoch 19/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0442 - accuracy: 0.9857\n",
      "Epoch 19: val_loss did not improve from 0.04999\n",
      "140/140 [==============================] - 44s 312ms/step - loss: 0.0442 - accuracy: 0.9857 - val_loss: 0.0711 - val_accuracy: 0.9772\n",
      "Epoch 20/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0381 - accuracy: 0.9867\n",
      "Epoch 20: val_loss did not improve from 0.04999\n",
      "140/140 [==============================] - 44s 311ms/step - loss: 0.0381 - accuracy: 0.9867 - val_loss: 0.0589 - val_accuracy: 0.9848\n",
      "Epoch 21/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0380 - accuracy: 0.9878\n",
      "Epoch 21: val_loss did not improve from 0.04999\n",
      "140/140 [==============================] - 44s 312ms/step - loss: 0.0380 - accuracy: 0.9878 - val_loss: 0.0592 - val_accuracy: 0.9817\n",
      "Epoch 22/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0314 - accuracy: 0.9892\n",
      "Epoch 22: val_loss did not improve from 0.04999\n",
      "140/140 [==============================] - 44s 316ms/step - loss: 0.0314 - accuracy: 0.9892 - val_loss: 0.0572 - val_accuracy: 0.9826\n",
      "Epoch 23/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0298 - accuracy: 0.9904\n",
      "Epoch 23: val_loss did not improve from 0.04999\n",
      "140/140 [==============================] - 44s 312ms/step - loss: 0.0298 - accuracy: 0.9904 - val_loss: 0.0616 - val_accuracy: 0.9884\n",
      "Epoch 24/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0307 - accuracy: 0.9897\n",
      "Epoch 24: val_loss did not improve from 0.04999\n",
      "140/140 [==============================] - 43s 309ms/step - loss: 0.0307 - accuracy: 0.9897 - val_loss: 0.0514 - val_accuracy: 0.9866\n",
      "Epoch 25/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0318 - accuracy: 0.9883\n",
      "Epoch 25: val_loss did not improve from 0.04999\n",
      "140/140 [==============================] - 44s 314ms/step - loss: 0.0318 - accuracy: 0.9883 - val_loss: 0.0672 - val_accuracy: 0.9848\n",
      "Epoch 26/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0314 - accuracy: 0.9897\n",
      "Epoch 26: val_loss did not improve from 0.04999\n",
      "140/140 [==============================] - 44s 314ms/step - loss: 0.0314 - accuracy: 0.9897 - val_loss: 0.0719 - val_accuracy: 0.9839\n",
      "Epoch 27/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0255 - accuracy: 0.9909\n",
      "Epoch 27: val_loss did not improve from 0.04999\n",
      "140/140 [==============================] - 44s 311ms/step - loss: 0.0255 - accuracy: 0.9909 - val_loss: 0.0566 - val_accuracy: 0.9879\n",
      "Epoch 28/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0177 - accuracy: 0.9933\n",
      "Epoch 28: val_loss did not improve from 0.04999\n",
      "140/140 [==============================] - 44s 312ms/step - loss: 0.0177 - accuracy: 0.9933 - val_loss: 0.0665 - val_accuracy: 0.9879\n",
      "Epoch 29/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0247 - accuracy: 0.9923\n",
      "Epoch 29: val_loss did not improve from 0.04999\n",
      "140/140 [==============================] - 44s 312ms/step - loss: 0.0247 - accuracy: 0.9923 - val_loss: 0.0606 - val_accuracy: 0.9848\n",
      "Epoch 30/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0221 - accuracy: 0.9922\n",
      "Epoch 30: val_loss did not improve from 0.04999\n",
      "140/140 [==============================] - 43s 310ms/step - loss: 0.0221 - accuracy: 0.9922 - val_loss: 0.0917 - val_accuracy: 0.9830\n",
      "Epoch 31/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0232 - accuracy: 0.9935\n",
      "Epoch 31: val_loss did not improve from 0.04999\n",
      "140/140 [==============================] - 44s 311ms/step - loss: 0.0232 - accuracy: 0.9935 - val_loss: 0.0643 - val_accuracy: 0.9853\n",
      "Epoch 32/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0196 - accuracy: 0.9936\n",
      "Epoch 32: val_loss did not improve from 0.04999\n",
      "140/140 [==============================] - 44s 313ms/step - loss: 0.0196 - accuracy: 0.9936 - val_loss: 0.0722 - val_accuracy: 0.9857\n",
      "Epoch 33/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0191 - accuracy: 0.9936\n",
      "Epoch 33: val_loss did not improve from 0.04999\n",
      "140/140 [==============================] - 44s 313ms/step - loss: 0.0191 - accuracy: 0.9936 - val_loss: 0.0709 - val_accuracy: 0.9830\n",
      "Epoch 34/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0154 - accuracy: 0.9940\n",
      "Epoch 34: val_loss did not improve from 0.04999\n",
      "140/140 [==============================] - 44s 310ms/step - loss: 0.0154 - accuracy: 0.9940 - val_loss: 0.0777 - val_accuracy: 0.9866\n",
      "Epoch 35/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0172 - accuracy: 0.9944\n",
      "Epoch 35: val_loss did not improve from 0.04999\n",
      "140/140 [==============================] - 44s 312ms/step - loss: 0.0172 - accuracy: 0.9944 - val_loss: 0.0687 - val_accuracy: 0.9870\n",
      "Epoch 36/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0125 - accuracy: 0.9950\n",
      "Epoch 36: val_loss did not improve from 0.04999\n",
      "140/140 [==============================] - 44s 313ms/step - loss: 0.0125 - accuracy: 0.9950 - val_loss: 0.0920 - val_accuracy: 0.9835\n",
      "Epoch 37/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0135 - accuracy: 0.9953\n",
      "Epoch 37: val_loss did not improve from 0.04999\n",
      "140/140 [==============================] - 44s 313ms/step - loss: 0.0135 - accuracy: 0.9953 - val_loss: 0.0764 - val_accuracy: 0.9879\n",
      "Epoch 38/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0145 - accuracy: 0.9958\n",
      "Epoch 38: val_loss did not improve from 0.04999\n",
      "140/140 [==============================] - 44s 312ms/step - loss: 0.0145 - accuracy: 0.9958 - val_loss: 0.0704 - val_accuracy: 0.9897\n",
      "Epoch 39/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0219 - accuracy: 0.9941\n",
      "Epoch 39: val_loss did not improve from 0.04999\n",
      "140/140 [==============================] - 44s 313ms/step - loss: 0.0219 - accuracy: 0.9941 - val_loss: 0.0643 - val_accuracy: 0.9893\n",
      "Epoch 40/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0083 - accuracy: 0.9969\n",
      "Epoch 40: val_loss did not improve from 0.04999\n",
      "140/140 [==============================] - 43s 310ms/step - loss: 0.0083 - accuracy: 0.9969 - val_loss: 0.0650 - val_accuracy: 0.9893\n",
      "Epoch 41/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0135 - accuracy: 0.9951\n",
      "Epoch 41: val_loss did not improve from 0.04999\n",
      "140/140 [==============================] - 44s 315ms/step - loss: 0.0135 - accuracy: 0.9951 - val_loss: 0.0636 - val_accuracy: 0.9884\n",
      "Epoch 42/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0131 - accuracy: 0.9958\n",
      "Epoch 42: val_loss did not improve from 0.04999\n",
      "140/140 [==============================] - 44s 312ms/step - loss: 0.0131 - accuracy: 0.9958 - val_loss: 0.0727 - val_accuracy: 0.9826\n",
      "Epoch 43/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0174 - accuracy: 0.9955\n",
      "Epoch 43: val_loss did not improve from 0.04999\n",
      "140/140 [==============================] - 44s 312ms/step - loss: 0.0174 - accuracy: 0.9955 - val_loss: 0.0663 - val_accuracy: 0.9884\n",
      "Epoch 44/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0087 - accuracy: 0.9975\n",
      "Epoch 44: val_loss did not improve from 0.04999\n",
      "140/140 [==============================] - 44s 312ms/step - loss: 0.0087 - accuracy: 0.9975 - val_loss: 0.0837 - val_accuracy: 0.9888\n",
      "Epoch 45/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0114 - accuracy: 0.9966\n",
      "Epoch 45: val_loss did not improve from 0.04999\n",
      "140/140 [==============================] - 44s 314ms/step - loss: 0.0114 - accuracy: 0.9966 - val_loss: 0.0881 - val_accuracy: 0.9866\n",
      "Epoch 46/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0140 - accuracy: 0.9954\n",
      "Epoch 46: val_loss did not improve from 0.04999\n",
      "140/140 [==============================] - 44s 311ms/step - loss: 0.0140 - accuracy: 0.9954 - val_loss: 0.0702 - val_accuracy: 0.9884\n",
      "Epoch 47/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0103 - accuracy: 0.9965\n",
      "Epoch 47: val_loss did not improve from 0.04999\n",
      "140/140 [==============================] - 44s 312ms/step - loss: 0.0103 - accuracy: 0.9965 - val_loss: 0.0947 - val_accuracy: 0.9888\n",
      "Epoch 48/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0136 - accuracy: 0.9964\n",
      "Epoch 48: val_loss did not improve from 0.04999\n",
      "140/140 [==============================] - 44s 315ms/step - loss: 0.0136 - accuracy: 0.9964 - val_loss: 0.0714 - val_accuracy: 0.9902\n",
      "Epoch 49/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0079 - accuracy: 0.9979\n",
      "Epoch 49: val_loss did not improve from 0.04999\n",
      "140/140 [==============================] - 43s 309ms/step - loss: 0.0079 - accuracy: 0.9979 - val_loss: 0.0781 - val_accuracy: 0.9906\n",
      "Epoch 50/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0095 - accuracy: 0.9979\n",
      "Epoch 50: val_loss did not improve from 0.04999\n",
      "140/140 [==============================] - 43s 310ms/step - loss: 0.0095 - accuracy: 0.9979 - val_loss: 0.0810 - val_accuracy: 0.9902\n",
      "Epoch 51/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0135 - accuracy: 0.9970\n",
      "Epoch 51: val_loss did not improve from 0.04999\n",
      "140/140 [==============================] - 44s 314ms/step - loss: 0.0135 - accuracy: 0.9970 - val_loss: 0.0739 - val_accuracy: 0.9888\n",
      "Epoch 52/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0083 - accuracy: 0.9973\n",
      "Epoch 52: val_loss did not improve from 0.04999\n",
      "140/140 [==============================] - 43s 310ms/step - loss: 0.0083 - accuracy: 0.9973 - val_loss: 0.0827 - val_accuracy: 0.9893\n",
      "Epoch 53/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0085 - accuracy: 0.9974\n",
      "Epoch 53: val_loss did not improve from 0.04999\n",
      "140/140 [==============================] - 44s 311ms/step - loss: 0.0085 - accuracy: 0.9974 - val_loss: 0.0817 - val_accuracy: 0.9884\n",
      "Epoch 54/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0054 - accuracy: 0.9983\n",
      "Epoch 54: val_loss did not improve from 0.04999\n",
      "140/140 [==============================] - 44s 313ms/step - loss: 0.0054 - accuracy: 0.9983 - val_loss: 0.0839 - val_accuracy: 0.9911\n",
      "Epoch 55/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0141 - accuracy: 0.9959\n",
      "Epoch 55: val_loss did not improve from 0.04999\n",
      "140/140 [==============================] - 43s 310ms/step - loss: 0.0141 - accuracy: 0.9959 - val_loss: 0.0685 - val_accuracy: 0.9897\n",
      "Epoch 56/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0080 - accuracy: 0.9975\n",
      "Epoch 56: val_loss did not improve from 0.04999\n",
      "140/140 [==============================] - 44s 312ms/step - loss: 0.0080 - accuracy: 0.9975 - val_loss: 0.0899 - val_accuracy: 0.9893\n",
      "Epoch 57/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0117 - accuracy: 0.9961\n",
      "Epoch 57: val_loss did not improve from 0.04999\n",
      "140/140 [==============================] - 44s 312ms/step - loss: 0.0117 - accuracy: 0.9961 - val_loss: 0.0777 - val_accuracy: 0.9870\n",
      "Epoch 58/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0089 - accuracy: 0.9981\n",
      "Epoch 58: val_loss did not improve from 0.04999\n",
      "140/140 [==============================] - 44s 311ms/step - loss: 0.0089 - accuracy: 0.9981 - val_loss: 0.0903 - val_accuracy: 0.9893\n",
      "Epoch 59/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0119 - accuracy: 0.9968\n",
      "Epoch 59: val_loss did not improve from 0.04999\n",
      "140/140 [==============================] - 43s 310ms/step - loss: 0.0119 - accuracy: 0.9968 - val_loss: 0.0926 - val_accuracy: 0.9888\n",
      "Epoch 60/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0097 - accuracy: 0.9970\n",
      "Epoch 60: val_loss did not improve from 0.04999\n",
      "140/140 [==============================] - 44s 311ms/step - loss: 0.0097 - accuracy: 0.9970 - val_loss: 0.0916 - val_accuracy: 0.9884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-02 03:24:25.107172: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2023-06-02 03:24:25.107388: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - 9s 243ms/step\n",
      "======================= manual compute ==============\n",
      "accuracy: 0.9883824843610366, precision: 0.9849955869373345, recall: 0.992, specificity: 0.9847259658580413, mcc: 0.9767883762137096 ,f1-score: 0.9884853852967228, auc: 0.9941623240491165, prc: 0.9919457508555357\n",
      "=========================== FOLD 2 ==================================\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-02 03:24:35.579923: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2023-06-02 03:24:35.580144: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    140/Unknown - 50s 246ms/step - loss: 0.5828 - accuracy: 0.6864"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-02 03:25:25.669879: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2023-06-02 03:25:25.670157: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 0.35669, saving model to my_best_model_fold_2.hdf5\n",
      "140/140 [==============================] - 60s 315ms/step - loss: 0.5828 - accuracy: 0.6864 - val_loss: 0.3567 - val_accuracy: 0.8467\n",
      "Epoch 2/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.3356 - accuracy: 0.8625\n",
      "Epoch 2: val_loss improved from 0.35669 to 0.22452, saving model to my_best_model_fold_2.hdf5\n",
      "140/140 [==============================] - 44s 311ms/step - loss: 0.3356 - accuracy: 0.8625 - val_loss: 0.2245 - val_accuracy: 0.9124\n",
      "Epoch 3/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.2132 - accuracy: 0.9107\n",
      "Epoch 3: val_loss improved from 0.22452 to 0.17985, saving model to my_best_model_fold_2.hdf5\n",
      "140/140 [==============================] - 44s 312ms/step - loss: 0.2132 - accuracy: 0.9107 - val_loss: 0.1799 - val_accuracy: 0.9361\n",
      "Epoch 4/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.1757 - accuracy: 0.9254\n",
      "Epoch 4: val_loss improved from 0.17985 to 0.12671, saving model to my_best_model_fold_2.hdf5\n",
      "140/140 [==============================] - 44s 313ms/step - loss: 0.1757 - accuracy: 0.9254 - val_loss: 0.1267 - val_accuracy: 0.9401\n",
      "Epoch 5/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.1445 - accuracy: 0.9422\n",
      "Epoch 5: val_loss improved from 0.12671 to 0.11503, saving model to my_best_model_fold_2.hdf5\n",
      "140/140 [==============================] - 44s 311ms/step - loss: 0.1445 - accuracy: 0.9422 - val_loss: 0.1150 - val_accuracy: 0.9482\n",
      "Epoch 6/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.1246 - accuracy: 0.9502\n",
      "Epoch 6: val_loss improved from 0.11503 to 0.08788, saving model to my_best_model_fold_2.hdf5\n",
      "140/140 [==============================] - 44s 313ms/step - loss: 0.1246 - accuracy: 0.9502 - val_loss: 0.0879 - val_accuracy: 0.9611\n",
      "Epoch 7/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.1056 - accuracy: 0.9593\n",
      "Epoch 7: val_loss did not improve from 0.08788\n",
      "140/140 [==============================] - 44s 311ms/step - loss: 0.1056 - accuracy: 0.9593 - val_loss: 0.0891 - val_accuracy: 0.9638\n",
      "Epoch 8/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.1047 - accuracy: 0.9603\n",
      "Epoch 8: val_loss improved from 0.08788 to 0.08099, saving model to my_best_model_fold_2.hdf5\n",
      "140/140 [==============================] - 44s 311ms/step - loss: 0.1047 - accuracy: 0.9603 - val_loss: 0.0810 - val_accuracy: 0.9705\n",
      "Epoch 9/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0837 - accuracy: 0.9659\n",
      "Epoch 9: val_loss improved from 0.08099 to 0.07145, saving model to my_best_model_fold_2.hdf5\n",
      "140/140 [==============================] - 44s 312ms/step - loss: 0.0837 - accuracy: 0.9659 - val_loss: 0.0714 - val_accuracy: 0.9736\n",
      "Epoch 10/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0673 - accuracy: 0.9735\n",
      "Epoch 10: val_loss improved from 0.07145 to 0.06403, saving model to my_best_model_fold_2.hdf5\n",
      "140/140 [==============================] - 44s 311ms/step - loss: 0.0673 - accuracy: 0.9735 - val_loss: 0.0640 - val_accuracy: 0.9710\n",
      "Epoch 11/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0616 - accuracy: 0.9763\n",
      "Epoch 11: val_loss did not improve from 0.06403\n",
      "140/140 [==============================] - 44s 311ms/step - loss: 0.0616 - accuracy: 0.9763 - val_loss: 0.0790 - val_accuracy: 0.9727\n",
      "Epoch 12/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0611 - accuracy: 0.9760\n",
      "Epoch 12: val_loss did not improve from 0.06403\n",
      "140/140 [==============================] - 44s 311ms/step - loss: 0.0611 - accuracy: 0.9760 - val_loss: 0.0779 - val_accuracy: 0.9759\n",
      "Epoch 13/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0518 - accuracy: 0.9800\n",
      "Epoch 13: val_loss did not improve from 0.06403\n",
      "140/140 [==============================] - 43s 310ms/step - loss: 0.0518 - accuracy: 0.9800 - val_loss: 0.0650 - val_accuracy: 0.9803\n",
      "Epoch 14/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0400 - accuracy: 0.9846\n",
      "Epoch 14: val_loss did not improve from 0.06403\n",
      "140/140 [==============================] - 44s 311ms/step - loss: 0.0400 - accuracy: 0.9846 - val_loss: 0.0683 - val_accuracy: 0.9812\n",
      "Epoch 15/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0505 - accuracy: 0.9808\n",
      "Epoch 15: val_loss did not improve from 0.06403\n",
      "140/140 [==============================] - 43s 310ms/step - loss: 0.0505 - accuracy: 0.9808 - val_loss: 0.0745 - val_accuracy: 0.9799\n",
      "Epoch 16/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0500 - accuracy: 0.9834\n",
      "Epoch 16: val_loss did not improve from 0.06403\n",
      "140/140 [==============================] - 43s 310ms/step - loss: 0.0500 - accuracy: 0.9834 - val_loss: 0.0847 - val_accuracy: 0.9763\n",
      "Epoch 17/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0527 - accuracy: 0.9819\n",
      "Epoch 17: val_loss did not improve from 0.06403\n",
      "140/140 [==============================] - 44s 312ms/step - loss: 0.0527 - accuracy: 0.9819 - val_loss: 0.0776 - val_accuracy: 0.9799\n",
      "Epoch 18/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0469 - accuracy: 0.9823\n",
      "Epoch 18: val_loss did not improve from 0.06403\n",
      "140/140 [==============================] - 44s 312ms/step - loss: 0.0469 - accuracy: 0.9823 - val_loss: 0.0712 - val_accuracy: 0.9803\n",
      "Epoch 19/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0329 - accuracy: 0.9882\n",
      "Epoch 19: val_loss did not improve from 0.06403\n",
      "140/140 [==============================] - 43s 310ms/step - loss: 0.0329 - accuracy: 0.9882 - val_loss: 0.0859 - val_accuracy: 0.9808\n",
      "Epoch 20/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0461 - accuracy: 0.9854\n",
      "Epoch 20: val_loss did not improve from 0.06403\n",
      "140/140 [==============================] - 44s 312ms/step - loss: 0.0461 - accuracy: 0.9854 - val_loss: 0.0795 - val_accuracy: 0.9848\n",
      "Epoch 21/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0296 - accuracy: 0.9895\n",
      "Epoch 21: val_loss did not improve from 0.06403\n",
      "140/140 [==============================] - 43s 309ms/step - loss: 0.0296 - accuracy: 0.9895 - val_loss: 0.0856 - val_accuracy: 0.9830\n",
      "Epoch 22/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0334 - accuracy: 0.9891\n",
      "Epoch 22: val_loss did not improve from 0.06403\n",
      "140/140 [==============================] - 43s 310ms/step - loss: 0.0334 - accuracy: 0.9891 - val_loss: 0.0689 - val_accuracy: 0.9830\n",
      "Epoch 23/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0314 - accuracy: 0.9901\n",
      "Epoch 23: val_loss did not improve from 0.06403\n",
      "140/140 [==============================] - 44s 313ms/step - loss: 0.0314 - accuracy: 0.9901 - val_loss: 0.0742 - val_accuracy: 0.9812\n",
      "Epoch 24/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0365 - accuracy: 0.9869\n",
      "Epoch 24: val_loss did not improve from 0.06403\n",
      "140/140 [==============================] - 43s 310ms/step - loss: 0.0365 - accuracy: 0.9869 - val_loss: 0.0885 - val_accuracy: 0.9808\n",
      "Epoch 25/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0258 - accuracy: 0.9916\n",
      "Epoch 25: val_loss improved from 0.06403 to 0.05737, saving model to my_best_model_fold_2.hdf5\n",
      "140/140 [==============================] - 44s 313ms/step - loss: 0.0258 - accuracy: 0.9916 - val_loss: 0.0574 - val_accuracy: 0.9857\n",
      "Epoch 26/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0224 - accuracy: 0.9924\n",
      "Epoch 26: val_loss did not improve from 0.05737\n",
      "140/140 [==============================] - 44s 311ms/step - loss: 0.0224 - accuracy: 0.9924 - val_loss: 0.0717 - val_accuracy: 0.9857\n",
      "Epoch 27/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0331 - accuracy: 0.9904\n",
      "Epoch 27: val_loss did not improve from 0.05737\n",
      "140/140 [==============================] - 44s 311ms/step - loss: 0.0331 - accuracy: 0.9904 - val_loss: 0.0792 - val_accuracy: 0.9812\n",
      "Epoch 28/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0289 - accuracy: 0.9906\n",
      "Epoch 28: val_loss did not improve from 0.05737\n",
      "140/140 [==============================] - 43s 309ms/step - loss: 0.0289 - accuracy: 0.9906 - val_loss: 0.0710 - val_accuracy: 0.9848\n",
      "Epoch 29/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0188 - accuracy: 0.9933\n",
      "Epoch 29: val_loss did not improve from 0.05737\n",
      "140/140 [==============================] - 44s 310ms/step - loss: 0.0188 - accuracy: 0.9933 - val_loss: 0.0911 - val_accuracy: 0.9844\n",
      "Epoch 30/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0266 - accuracy: 0.9908\n",
      "Epoch 30: val_loss did not improve from 0.05737\n",
      "140/140 [==============================] - 43s 310ms/step - loss: 0.0266 - accuracy: 0.9908 - val_loss: 0.0899 - val_accuracy: 0.9844\n",
      "Epoch 31/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0164 - accuracy: 0.9943\n",
      "Epoch 31: val_loss did not improve from 0.05737\n",
      "140/140 [==============================] - 43s 310ms/step - loss: 0.0164 - accuracy: 0.9943 - val_loss: 0.0926 - val_accuracy: 0.9866\n",
      "Epoch 32/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0154 - accuracy: 0.9945\n",
      "Epoch 32: val_loss did not improve from 0.05737\n",
      "140/140 [==============================] - 43s 308ms/step - loss: 0.0154 - accuracy: 0.9945 - val_loss: 0.1072 - val_accuracy: 0.9857\n",
      "Epoch 33/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0138 - accuracy: 0.9959\n",
      "Epoch 33: val_loss did not improve from 0.05737\n",
      "140/140 [==============================] - 43s 310ms/step - loss: 0.0138 - accuracy: 0.9959 - val_loss: 0.0927 - val_accuracy: 0.9875\n",
      "Epoch 34/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0120 - accuracy: 0.9962\n",
      "Epoch 34: val_loss did not improve from 0.05737\n",
      "140/140 [==============================] - 43s 309ms/step - loss: 0.0120 - accuracy: 0.9962 - val_loss: 0.0888 - val_accuracy: 0.9866\n",
      "Epoch 35/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0143 - accuracy: 0.9960\n",
      "Epoch 35: val_loss did not improve from 0.05737\n",
      "140/140 [==============================] - 43s 308ms/step - loss: 0.0143 - accuracy: 0.9960 - val_loss: 0.0767 - val_accuracy: 0.9875\n",
      "Epoch 36/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0202 - accuracy: 0.9939\n",
      "Epoch 36: val_loss did not improve from 0.05737\n",
      "140/140 [==============================] - 43s 310ms/step - loss: 0.0202 - accuracy: 0.9939 - val_loss: 0.0906 - val_accuracy: 0.9866\n",
      "Epoch 37/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0080 - accuracy: 0.9973\n",
      "Epoch 37: val_loss did not improve from 0.05737\n",
      "140/140 [==============================] - 43s 309ms/step - loss: 0.0080 - accuracy: 0.9973 - val_loss: 0.1190 - val_accuracy: 0.9861\n",
      "Epoch 38/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0098 - accuracy: 0.9971\n",
      "Epoch 38: val_loss did not improve from 0.05737\n",
      "140/140 [==============================] - 43s 310ms/step - loss: 0.0098 - accuracy: 0.9971 - val_loss: 0.1070 - val_accuracy: 0.9861\n",
      "Epoch 39/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0138 - accuracy: 0.9960\n",
      "Epoch 39: val_loss did not improve from 0.05737\n",
      "140/140 [==============================] - 44s 313ms/step - loss: 0.0138 - accuracy: 0.9960 - val_loss: 0.1146 - val_accuracy: 0.9839\n",
      "Epoch 40/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0139 - accuracy: 0.9950\n",
      "Epoch 40: val_loss did not improve from 0.05737\n",
      "140/140 [==============================] - 43s 310ms/step - loss: 0.0139 - accuracy: 0.9950 - val_loss: 0.1143 - val_accuracy: 0.9839\n",
      "Epoch 41/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0207 - accuracy: 0.9939\n",
      "Epoch 41: val_loss did not improve from 0.05737\n",
      "140/140 [==============================] - 43s 310ms/step - loss: 0.0207 - accuracy: 0.9939 - val_loss: 0.0984 - val_accuracy: 0.9853\n",
      "Epoch 42/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0145 - accuracy: 0.9953\n",
      "Epoch 42: val_loss did not improve from 0.05737\n",
      "140/140 [==============================] - 43s 310ms/step - loss: 0.0145 - accuracy: 0.9953 - val_loss: 0.1257 - val_accuracy: 0.9839\n",
      "Epoch 43/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0114 - accuracy: 0.9966\n",
      "Epoch 43: val_loss did not improve from 0.05737\n",
      "140/140 [==============================] - 43s 308ms/step - loss: 0.0114 - accuracy: 0.9966 - val_loss: 0.1031 - val_accuracy: 0.9857\n",
      "Epoch 44/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0119 - accuracy: 0.9954\n",
      "Epoch 44: val_loss did not improve from 0.05737\n",
      "140/140 [==============================] - 44s 310ms/step - loss: 0.0119 - accuracy: 0.9954 - val_loss: 0.1163 - val_accuracy: 0.9884\n",
      "Epoch 45/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0069 - accuracy: 0.9973\n",
      "Epoch 45: val_loss did not improve from 0.05737\n",
      "140/140 [==============================] - 44s 311ms/step - loss: 0.0069 - accuracy: 0.9973 - val_loss: 0.1267 - val_accuracy: 0.9839\n",
      "Epoch 46/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0063 - accuracy: 0.9975\n",
      "Epoch 46: val_loss did not improve from 0.05737\n",
      "140/140 [==============================] - 44s 311ms/step - loss: 0.0063 - accuracy: 0.9975 - val_loss: 0.1147 - val_accuracy: 0.9848\n",
      "Epoch 47/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0121 - accuracy: 0.9951\n",
      "Epoch 47: val_loss did not improve from 0.05737\n",
      "140/140 [==============================] - 43s 307ms/step - loss: 0.0121 - accuracy: 0.9951 - val_loss: 0.1126 - val_accuracy: 0.9844\n",
      "Epoch 48/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0078 - accuracy: 0.9969\n",
      "Epoch 48: val_loss did not improve from 0.05737\n",
      "140/140 [==============================] - 44s 311ms/step - loss: 0.0078 - accuracy: 0.9969 - val_loss: 0.1123 - val_accuracy: 0.9844\n",
      "Epoch 49/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0142 - accuracy: 0.9955\n",
      "Epoch 49: val_loss did not improve from 0.05737\n",
      "140/140 [==============================] - 43s 309ms/step - loss: 0.0142 - accuracy: 0.9955 - val_loss: 0.1012 - val_accuracy: 0.9861\n",
      "Epoch 50/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0104 - accuracy: 0.9960\n",
      "Epoch 50: val_loss did not improve from 0.05737\n",
      "140/140 [==============================] - 43s 310ms/step - loss: 0.0104 - accuracy: 0.9960 - val_loss: 0.1049 - val_accuracy: 0.9839\n",
      "Epoch 51/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0091 - accuracy: 0.9968\n",
      "Epoch 51: val_loss did not improve from 0.05737\n",
      "140/140 [==============================] - 44s 311ms/step - loss: 0.0091 - accuracy: 0.9968 - val_loss: 0.1245 - val_accuracy: 0.9870\n",
      "Epoch 52/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0100 - accuracy: 0.9974\n",
      "Epoch 52: val_loss did not improve from 0.05737\n",
      "140/140 [==============================] - 43s 309ms/step - loss: 0.0100 - accuracy: 0.9974 - val_loss: 0.1167 - val_accuracy: 0.9870\n",
      "Epoch 53/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0066 - accuracy: 0.9984\n",
      "Epoch 53: val_loss did not improve from 0.05737\n",
      "140/140 [==============================] - 43s 310ms/step - loss: 0.0066 - accuracy: 0.9984 - val_loss: 0.1449 - val_accuracy: 0.9875\n",
      "Epoch 54/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0088 - accuracy: 0.9972\n",
      "Epoch 54: val_loss did not improve from 0.05737\n",
      "140/140 [==============================] - 44s 312ms/step - loss: 0.0088 - accuracy: 0.9972 - val_loss: 0.1016 - val_accuracy: 0.9870\n",
      "Epoch 55/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0089 - accuracy: 0.9972\n",
      "Epoch 55: val_loss did not improve from 0.05737\n",
      "140/140 [==============================] - 43s 309ms/step - loss: 0.0089 - accuracy: 0.9972 - val_loss: 0.1030 - val_accuracy: 0.9861\n",
      "Epoch 56/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0067 - accuracy: 0.9978\n",
      "Epoch 56: val_loss did not improve from 0.05737\n",
      "140/140 [==============================] - 43s 309ms/step - loss: 0.0067 - accuracy: 0.9978 - val_loss: 0.1198 - val_accuracy: 0.9861\n",
      "Epoch 57/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0097 - accuracy: 0.9979\n",
      "Epoch 57: val_loss did not improve from 0.05737\n",
      "140/140 [==============================] - 44s 311ms/step - loss: 0.0097 - accuracy: 0.9979 - val_loss: 0.1090 - val_accuracy: 0.9879\n",
      "Epoch 58/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0065 - accuracy: 0.9974\n",
      "Epoch 58: val_loss did not improve from 0.05737\n",
      "140/140 [==============================] - 43s 308ms/step - loss: 0.0065 - accuracy: 0.9974 - val_loss: 0.1083 - val_accuracy: 0.9870\n",
      "Epoch 59/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0075 - accuracy: 0.9977\n",
      "Epoch 59: val_loss did not improve from 0.05737\n",
      "140/140 [==============================] - 43s 308ms/step - loss: 0.0075 - accuracy: 0.9977 - val_loss: 0.1026 - val_accuracy: 0.9861\n",
      "Epoch 60/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0065 - accuracy: 0.9983\n",
      "Epoch 60: val_loss did not improve from 0.05737\n",
      "140/140 [==============================] - 43s 309ms/step - loss: 0.0065 - accuracy: 0.9983 - val_loss: 0.1018 - val_accuracy: 0.9879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-02 04:08:22.202607: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2023-06-02 04:08:22.202830: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - 10s 243ms/step\n",
      "======================= manual compute ==============\n",
      "accuracy: 0.9879356568364611, precision: 0.9885159010600707, recall: 0.9876434245366285, specificity: 0.9882352941176471, mcc: 0.9758681958763895 ,f1-score: 0.9880794701986756, auc: 0.9935497398090201, prc: 0.9894701413816813\n",
      "=========================== FOLD 3 ==================================\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-02 04:08:33.228548: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2023-06-02 04:08:33.228780: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    140/Unknown - 49s 245ms/step - loss: 0.5818 - accuracy: 0.6821"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-02 04:09:22.379356: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2023-06-02 04:09:22.379587: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 0.44871, saving model to my_best_model_fold_3.hdf5\n",
      "140/140 [==============================] - 59s 314ms/step - loss: 0.5818 - accuracy: 0.6821 - val_loss: 0.4487 - val_accuracy: 0.8181\n",
      "Epoch 2/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.3442 - accuracy: 0.8636\n",
      "Epoch 2: val_loss improved from 0.44871 to 0.33884, saving model to my_best_model_fold_3.hdf5\n",
      "140/140 [==============================] - 43s 310ms/step - loss: 0.3442 - accuracy: 0.8636 - val_loss: 0.3388 - val_accuracy: 0.8633\n",
      "Epoch 3/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.2371 - accuracy: 0.9046\n",
      "Epoch 3: val_loss improved from 0.33884 to 0.19216, saving model to my_best_model_fold_3.hdf5\n",
      "140/140 [==============================] - 44s 312ms/step - loss: 0.2371 - accuracy: 0.9046 - val_loss: 0.1922 - val_accuracy: 0.9173\n",
      "Epoch 4/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.1790 - accuracy: 0.9248\n",
      "Epoch 4: val_loss improved from 0.19216 to 0.15427, saving model to my_best_model_fold_3.hdf5\n",
      "140/140 [==============================] - 44s 312ms/step - loss: 0.1790 - accuracy: 0.9248 - val_loss: 0.1543 - val_accuracy: 0.9352\n",
      "Epoch 5/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.1488 - accuracy: 0.9339\n",
      "Epoch 5: val_loss improved from 0.15427 to 0.11396, saving model to my_best_model_fold_3.hdf5\n",
      "140/140 [==============================] - 44s 312ms/step - loss: 0.1488 - accuracy: 0.9339 - val_loss: 0.1140 - val_accuracy: 0.9491\n",
      "Epoch 6/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.1380 - accuracy: 0.9402\n",
      "Epoch 6: val_loss improved from 0.11396 to 0.10045, saving model to my_best_model_fold_3.hdf5\n",
      "140/140 [==============================] - 44s 314ms/step - loss: 0.1380 - accuracy: 0.9402 - val_loss: 0.1004 - val_accuracy: 0.9517\n",
      "Epoch 7/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.1152 - accuracy: 0.9535\n",
      "Epoch 7: val_loss did not improve from 0.10045\n",
      "140/140 [==============================] - 43s 310ms/step - loss: 0.1152 - accuracy: 0.9535 - val_loss: 0.1095 - val_accuracy: 0.9526\n",
      "Epoch 8/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.1050 - accuracy: 0.9584\n",
      "Epoch 8: val_loss improved from 0.10045 to 0.09033, saving model to my_best_model_fold_3.hdf5\n",
      "140/140 [==============================] - 44s 313ms/step - loss: 0.1050 - accuracy: 0.9584 - val_loss: 0.0903 - val_accuracy: 0.9544\n",
      "Epoch 9/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0893 - accuracy: 0.9649\n",
      "Epoch 9: val_loss did not improve from 0.09033\n",
      "140/140 [==============================] - 44s 311ms/step - loss: 0.0893 - accuracy: 0.9649 - val_loss: 0.1028 - val_accuracy: 0.9580\n",
      "Epoch 10/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0844 - accuracy: 0.9682\n",
      "Epoch 10: val_loss did not improve from 0.09033\n",
      "140/140 [==============================] - 44s 312ms/step - loss: 0.0844 - accuracy: 0.9682 - val_loss: 0.1141 - val_accuracy: 0.9571\n",
      "Epoch 11/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0743 - accuracy: 0.9713\n",
      "Epoch 11: val_loss improved from 0.09033 to 0.08113, saving model to my_best_model_fold_3.hdf5\n",
      "140/140 [==============================] - 44s 312ms/step - loss: 0.0743 - accuracy: 0.9713 - val_loss: 0.0811 - val_accuracy: 0.9687\n",
      "Epoch 12/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0718 - accuracy: 0.9727\n",
      "Epoch 12: val_loss improved from 0.08113 to 0.07322, saving model to my_best_model_fold_3.hdf5\n",
      "140/140 [==============================] - 44s 312ms/step - loss: 0.0718 - accuracy: 0.9727 - val_loss: 0.0732 - val_accuracy: 0.9687\n",
      "Epoch 13/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0612 - accuracy: 0.9784\n",
      "Epoch 13: val_loss improved from 0.07322 to 0.07275, saving model to my_best_model_fold_3.hdf5\n",
      "140/140 [==============================] - 44s 314ms/step - loss: 0.0612 - accuracy: 0.9784 - val_loss: 0.0728 - val_accuracy: 0.9687\n",
      "Epoch 14/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0587 - accuracy: 0.9785\n",
      "Epoch 14: val_loss did not improve from 0.07275\n",
      "140/140 [==============================] - 43s 310ms/step - loss: 0.0587 - accuracy: 0.9785 - val_loss: 0.0936 - val_accuracy: 0.9696\n",
      "Epoch 15/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0533 - accuracy: 0.9812\n",
      "Epoch 15: val_loss did not improve from 0.07275\n",
      "140/140 [==============================] - 44s 311ms/step - loss: 0.0533 - accuracy: 0.9812 - val_loss: 0.0807 - val_accuracy: 0.9674\n",
      "Epoch 16/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0496 - accuracy: 0.9800\n",
      "Epoch 16: val_loss did not improve from 0.07275\n",
      "140/140 [==============================] - 43s 310ms/step - loss: 0.0496 - accuracy: 0.9800 - val_loss: 0.0812 - val_accuracy: 0.9692\n",
      "Epoch 17/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0388 - accuracy: 0.9857\n",
      "Epoch 17: val_loss did not improve from 0.07275\n",
      "140/140 [==============================] - 44s 313ms/step - loss: 0.0388 - accuracy: 0.9857 - val_loss: 0.0857 - val_accuracy: 0.9727\n",
      "Epoch 18/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0461 - accuracy: 0.9812\n",
      "Epoch 18: val_loss did not improve from 0.07275\n",
      "140/140 [==============================] - 43s 309ms/step - loss: 0.0461 - accuracy: 0.9812 - val_loss: 0.0740 - val_accuracy: 0.9763\n",
      "Epoch 19/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0375 - accuracy: 0.9859\n",
      "Epoch 19: val_loss did not improve from 0.07275\n",
      "140/140 [==============================] - 43s 309ms/step - loss: 0.0375 - accuracy: 0.9859 - val_loss: 0.0859 - val_accuracy: 0.9772\n",
      "Epoch 20/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0327 - accuracy: 0.9876\n",
      "Epoch 20: val_loss did not improve from 0.07275\n",
      "140/140 [==============================] - 44s 313ms/step - loss: 0.0327 - accuracy: 0.9876 - val_loss: 0.0999 - val_accuracy: 0.9750\n",
      "Epoch 21/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0447 - accuracy: 0.9867\n",
      "Epoch 21: val_loss did not improve from 0.07275\n",
      "140/140 [==============================] - 44s 311ms/step - loss: 0.0447 - accuracy: 0.9867 - val_loss: 0.1154 - val_accuracy: 0.9714\n",
      "Epoch 22/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0360 - accuracy: 0.9885\n",
      "Epoch 22: val_loss did not improve from 0.07275\n",
      "140/140 [==============================] - 43s 310ms/step - loss: 0.0360 - accuracy: 0.9885 - val_loss: 0.0870 - val_accuracy: 0.9763\n",
      "Epoch 23/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0290 - accuracy: 0.9907\n",
      "Epoch 23: val_loss did not improve from 0.07275\n",
      "140/140 [==============================] - 44s 311ms/step - loss: 0.0290 - accuracy: 0.9907 - val_loss: 0.0986 - val_accuracy: 0.9772\n",
      "Epoch 24/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0342 - accuracy: 0.9899\n",
      "Epoch 24: val_loss did not improve from 0.07275\n",
      "140/140 [==============================] - 44s 314ms/step - loss: 0.0342 - accuracy: 0.9899 - val_loss: 0.1036 - val_accuracy: 0.9772\n",
      "Epoch 25/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0296 - accuracy: 0.9887\n",
      "Epoch 25: val_loss did not improve from 0.07275\n",
      "140/140 [==============================] - 43s 309ms/step - loss: 0.0296 - accuracy: 0.9887 - val_loss: 0.1086 - val_accuracy: 0.9799\n",
      "Epoch 26/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0254 - accuracy: 0.9918\n",
      "Epoch 26: val_loss did not improve from 0.07275\n",
      "140/140 [==============================] - 43s 309ms/step - loss: 0.0254 - accuracy: 0.9918 - val_loss: 0.0968 - val_accuracy: 0.9750\n",
      "Epoch 27/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0278 - accuracy: 0.9901\n",
      "Epoch 27: val_loss did not improve from 0.07275\n",
      "140/140 [==============================] - 43s 310ms/step - loss: 0.0278 - accuracy: 0.9901 - val_loss: 0.1082 - val_accuracy: 0.9772\n",
      "Epoch 28/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0284 - accuracy: 0.9905\n",
      "Epoch 28: val_loss did not improve from 0.07275\n",
      "140/140 [==============================] - 44s 312ms/step - loss: 0.0284 - accuracy: 0.9905 - val_loss: 0.0882 - val_accuracy: 0.9794\n",
      "Epoch 29/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0320 - accuracy: 0.9899\n",
      "Epoch 29: val_loss did not improve from 0.07275\n",
      "140/140 [==============================] - 44s 311ms/step - loss: 0.0320 - accuracy: 0.9899 - val_loss: 0.0887 - val_accuracy: 0.9786\n",
      "Epoch 30/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0310 - accuracy: 0.9892\n",
      "Epoch 30: val_loss improved from 0.07275 to 0.07228, saving model to my_best_model_fold_3.hdf5\n",
      "140/140 [==============================] - 44s 311ms/step - loss: 0.0310 - accuracy: 0.9892 - val_loss: 0.0723 - val_accuracy: 0.9808\n",
      "Epoch 31/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0224 - accuracy: 0.9932\n",
      "Epoch 31: val_loss did not improve from 0.07228\n",
      "140/140 [==============================] - 44s 312ms/step - loss: 0.0224 - accuracy: 0.9932 - val_loss: 0.0804 - val_accuracy: 0.9808\n",
      "Epoch 32/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0198 - accuracy: 0.9940\n",
      "Epoch 32: val_loss did not improve from 0.07228\n",
      "140/140 [==============================] - 43s 310ms/step - loss: 0.0198 - accuracy: 0.9940 - val_loss: 0.1119 - val_accuracy: 0.9803\n",
      "Epoch 33/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0187 - accuracy: 0.9941\n",
      "Epoch 33: val_loss did not improve from 0.07228\n",
      "140/140 [==============================] - 43s 310ms/step - loss: 0.0187 - accuracy: 0.9941 - val_loss: 0.1133 - val_accuracy: 0.9817\n",
      "Epoch 34/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0189 - accuracy: 0.9934\n",
      "Epoch 34: val_loss did not improve from 0.07228\n",
      "140/140 [==============================] - 43s 310ms/step - loss: 0.0189 - accuracy: 0.9934 - val_loss: 0.1051 - val_accuracy: 0.9830\n",
      "Epoch 35/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0272 - accuracy: 0.9903\n",
      "Epoch 35: val_loss did not improve from 0.07228\n",
      "140/140 [==============================] - 43s 310ms/step - loss: 0.0272 - accuracy: 0.9903 - val_loss: 0.0807 - val_accuracy: 0.9817\n",
      "Epoch 36/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0162 - accuracy: 0.9950\n",
      "Epoch 36: val_loss did not improve from 0.07228\n",
      "140/140 [==============================] - 43s 309ms/step - loss: 0.0162 - accuracy: 0.9950 - val_loss: 0.0738 - val_accuracy: 0.9830\n",
      "Epoch 37/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0152 - accuracy: 0.9951\n",
      "Epoch 37: val_loss did not improve from 0.07228\n",
      "140/140 [==============================] - 44s 313ms/step - loss: 0.0152 - accuracy: 0.9951 - val_loss: 0.0873 - val_accuracy: 0.9830\n",
      "Epoch 38/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0175 - accuracy: 0.9941\n",
      "Epoch 38: val_loss did not improve from 0.07228\n",
      "140/140 [==============================] - 44s 311ms/step - loss: 0.0175 - accuracy: 0.9941 - val_loss: 0.0979 - val_accuracy: 0.9835\n",
      "Epoch 39/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0162 - accuracy: 0.9951\n",
      "Epoch 39: val_loss did not improve from 0.07228\n",
      "140/140 [==============================] - 44s 311ms/step - loss: 0.0162 - accuracy: 0.9951 - val_loss: 0.0987 - val_accuracy: 0.9826\n",
      "Epoch 40/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0175 - accuracy: 0.9956\n",
      "Epoch 40: val_loss did not improve from 0.07228\n",
      "140/140 [==============================] - 44s 311ms/step - loss: 0.0175 - accuracy: 0.9956 - val_loss: 0.0854 - val_accuracy: 0.9808\n",
      "Epoch 41/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0121 - accuracy: 0.9956\n",
      "Epoch 41: val_loss did not improve from 0.07228\n",
      "140/140 [==============================] - 44s 311ms/step - loss: 0.0121 - accuracy: 0.9956 - val_loss: 0.1239 - val_accuracy: 0.9839\n",
      "Epoch 42/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0194 - accuracy: 0.9937\n",
      "Epoch 42: val_loss did not improve from 0.07228\n",
      "140/140 [==============================] - 43s 309ms/step - loss: 0.0194 - accuracy: 0.9937 - val_loss: 0.0905 - val_accuracy: 0.9821\n",
      "Epoch 43/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0126 - accuracy: 0.9963\n",
      "Epoch 43: val_loss did not improve from 0.07228\n",
      "140/140 [==============================] - 43s 309ms/step - loss: 0.0126 - accuracy: 0.9963 - val_loss: 0.1054 - val_accuracy: 0.9803\n",
      "Epoch 44/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0146 - accuracy: 0.9960\n",
      "Epoch 44: val_loss did not improve from 0.07228\n",
      "140/140 [==============================] - 43s 310ms/step - loss: 0.0146 - accuracy: 0.9960 - val_loss: 0.1220 - val_accuracy: 0.9799\n",
      "Epoch 45/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0095 - accuracy: 0.9974\n",
      "Epoch 45: val_loss did not improve from 0.07228\n",
      "140/140 [==============================] - 44s 311ms/step - loss: 0.0095 - accuracy: 0.9974 - val_loss: 0.1315 - val_accuracy: 0.9817\n",
      "Epoch 46/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0071 - accuracy: 0.9975\n",
      "Epoch 46: val_loss did not improve from 0.07228\n",
      "140/140 [==============================] - 43s 309ms/step - loss: 0.0071 - accuracy: 0.9975 - val_loss: 0.1229 - val_accuracy: 0.9808\n",
      "Epoch 47/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0069 - accuracy: 0.9975\n",
      "Epoch 47: val_loss did not improve from 0.07228\n",
      "140/140 [==============================] - 43s 310ms/step - loss: 0.0069 - accuracy: 0.9975 - val_loss: 0.1328 - val_accuracy: 0.9799\n",
      "Epoch 48/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0132 - accuracy: 0.9966\n",
      "Epoch 48: val_loss did not improve from 0.07228\n",
      "140/140 [==============================] - 43s 310ms/step - loss: 0.0132 - accuracy: 0.9966 - val_loss: 0.1052 - val_accuracy: 0.9817\n",
      "Epoch 49/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0069 - accuracy: 0.9975\n",
      "Epoch 49: val_loss did not improve from 0.07228\n",
      "140/140 [==============================] - 43s 310ms/step - loss: 0.0069 - accuracy: 0.9975 - val_loss: 0.1257 - val_accuracy: 0.9826\n",
      "Epoch 50/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0051 - accuracy: 0.9985\n",
      "Epoch 50: val_loss did not improve from 0.07228\n",
      "140/140 [==============================] - 44s 311ms/step - loss: 0.0051 - accuracy: 0.9985 - val_loss: 0.1220 - val_accuracy: 0.9826\n",
      "Epoch 51/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0111 - accuracy: 0.9969\n",
      "Epoch 51: val_loss did not improve from 0.07228\n",
      "140/140 [==============================] - 43s 310ms/step - loss: 0.0111 - accuracy: 0.9969 - val_loss: 0.0946 - val_accuracy: 0.9799\n",
      "Epoch 52/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0145 - accuracy: 0.9965\n",
      "Epoch 52: val_loss did not improve from 0.07228\n",
      "140/140 [==============================] - 44s 312ms/step - loss: 0.0145 - accuracy: 0.9965 - val_loss: 0.1210 - val_accuracy: 0.9781\n",
      "Epoch 53/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0102 - accuracy: 0.9974\n",
      "Epoch 53: val_loss did not improve from 0.07228\n",
      "140/140 [==============================] - 44s 312ms/step - loss: 0.0102 - accuracy: 0.9974 - val_loss: 0.1152 - val_accuracy: 0.9786\n",
      "Epoch 54/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0096 - accuracy: 0.9978\n",
      "Epoch 54: val_loss did not improve from 0.07228\n",
      "140/140 [==============================] - 44s 311ms/step - loss: 0.0096 - accuracy: 0.9978 - val_loss: 0.1230 - val_accuracy: 0.9799\n",
      "Epoch 55/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0087 - accuracy: 0.9972\n",
      "Epoch 55: val_loss did not improve from 0.07228\n",
      "140/140 [==============================] - 43s 310ms/step - loss: 0.0087 - accuracy: 0.9972 - val_loss: 0.1406 - val_accuracy: 0.9808\n",
      "Epoch 56/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0141 - accuracy: 0.9960\n",
      "Epoch 56: val_loss did not improve from 0.07228\n",
      "140/140 [==============================] - 43s 308ms/step - loss: 0.0141 - accuracy: 0.9960 - val_loss: 0.1302 - val_accuracy: 0.9794\n",
      "Epoch 57/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0061 - accuracy: 0.9988\n",
      "Epoch 57: val_loss did not improve from 0.07228\n",
      "140/140 [==============================] - 43s 311ms/step - loss: 0.0061 - accuracy: 0.9988 - val_loss: 0.1343 - val_accuracy: 0.9821\n",
      "Epoch 58/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0073 - accuracy: 0.9983\n",
      "Epoch 58: val_loss did not improve from 0.07228\n",
      "140/140 [==============================] - 43s 310ms/step - loss: 0.0073 - accuracy: 0.9983 - val_loss: 0.1527 - val_accuracy: 0.9808\n",
      "Epoch 59/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0204 - accuracy: 0.9955\n",
      "Epoch 59: val_loss did not improve from 0.07228\n",
      "140/140 [==============================] - 43s 310ms/step - loss: 0.0204 - accuracy: 0.9955 - val_loss: 0.0966 - val_accuracy: 0.9799\n",
      "Epoch 60/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0170 - accuracy: 0.9958\n",
      "Epoch 60: val_loss did not improve from 0.07228\n",
      "140/140 [==============================] - 43s 310ms/step - loss: 0.0170 - accuracy: 0.9958 - val_loss: 0.1074 - val_accuracy: 0.9808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-02 04:52:22.384571: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2023-06-02 04:52:22.384796: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - 9s 244ms/step\n",
      "======================= manual compute ==============\n",
      "accuracy: 0.9807864164432529, precision: 0.9797421731123389, recall: 0.9806451612903225, specificity: 0.9809193408499567, mcc: 0.9615387539482408 ,f1-score: 0.9801934592353754, auc: 0.9939668506520758, prc: 0.9917386974768381\n",
      "=========================== FOLD 4 ==================================\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-02 04:52:32.811201: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2023-06-02 04:52:32.811433: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    140/Unknown - 50s 253ms/step - loss: 0.5906 - accuracy: 0.6757"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-02 04:53:23.070854: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2023-06-02 04:53:23.071092: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 0.41389, saving model to my_best_model_fold_4.hdf5\n",
      "140/140 [==============================] - 60s 326ms/step - loss: 0.5906 - accuracy: 0.6757 - val_loss: 0.4139 - val_accuracy: 0.8190\n",
      "Epoch 2/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.3609 - accuracy: 0.8506\n",
      "Epoch 2: val_loss improved from 0.41389 to 0.26636, saving model to my_best_model_fold_4.hdf5\n",
      "140/140 [==============================] - 44s 312ms/step - loss: 0.3609 - accuracy: 0.8506 - val_loss: 0.2664 - val_accuracy: 0.8914\n",
      "Epoch 3/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.2265 - accuracy: 0.9065\n",
      "Epoch 3: val_loss improved from 0.26636 to 0.20487, saving model to my_best_model_fold_4.hdf5\n",
      "140/140 [==============================] - 44s 312ms/step - loss: 0.2265 - accuracy: 0.9065 - val_loss: 0.2049 - val_accuracy: 0.9169\n",
      "Epoch 4/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.1769 - accuracy: 0.9240\n",
      "Epoch 4: val_loss improved from 0.20487 to 0.14811, saving model to my_best_model_fold_4.hdf5\n",
      "140/140 [==============================] - 44s 312ms/step - loss: 0.1769 - accuracy: 0.9240 - val_loss: 0.1481 - val_accuracy: 0.9455\n",
      "Epoch 5/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.1359 - accuracy: 0.9460\n",
      "Epoch 5: val_loss improved from 0.14811 to 0.12202, saving model to my_best_model_fold_4.hdf5\n",
      "140/140 [==============================] - 43s 310ms/step - loss: 0.1359 - accuracy: 0.9460 - val_loss: 0.1220 - val_accuracy: 0.9549\n",
      "Epoch 6/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.1201 - accuracy: 0.9521\n",
      "Epoch 6: val_loss did not improve from 0.12202\n",
      "140/140 [==============================] - 43s 310ms/step - loss: 0.1201 - accuracy: 0.9521 - val_loss: 0.1292 - val_accuracy: 0.9401\n",
      "Epoch 7/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.1030 - accuracy: 0.9607\n",
      "Epoch 7: val_loss improved from 0.12202 to 0.09434, saving model to my_best_model_fold_4.hdf5\n",
      "140/140 [==============================] - 43s 311ms/step - loss: 0.1030 - accuracy: 0.9607 - val_loss: 0.0943 - val_accuracy: 0.9692\n",
      "Epoch 8/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0977 - accuracy: 0.9665\n",
      "Epoch 8: val_loss did not improve from 0.09434\n",
      "140/140 [==============================] - 43s 310ms/step - loss: 0.0977 - accuracy: 0.9665 - val_loss: 0.1248 - val_accuracy: 0.9611\n",
      "Epoch 9/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0859 - accuracy: 0.9699\n",
      "Epoch 9: val_loss improved from 0.09434 to 0.08328, saving model to my_best_model_fold_4.hdf5\n",
      "140/140 [==============================] - 44s 311ms/step - loss: 0.0859 - accuracy: 0.9699 - val_loss: 0.0833 - val_accuracy: 0.9696\n",
      "Epoch 10/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0741 - accuracy: 0.9729\n",
      "Epoch 10: val_loss improved from 0.08328 to 0.08106, saving model to my_best_model_fold_4.hdf5\n",
      "140/140 [==============================] - 43s 310ms/step - loss: 0.0741 - accuracy: 0.9729 - val_loss: 0.0811 - val_accuracy: 0.9723\n",
      "Epoch 11/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0630 - accuracy: 0.9774\n",
      "Epoch 11: val_loss improved from 0.08106 to 0.07286, saving model to my_best_model_fold_4.hdf5\n",
      "140/140 [==============================] - 44s 312ms/step - loss: 0.0630 - accuracy: 0.9774 - val_loss: 0.0729 - val_accuracy: 0.9709\n",
      "Epoch 12/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0582 - accuracy: 0.9770\n",
      "Epoch 12: val_loss did not improve from 0.07286\n",
      "140/140 [==============================] - 44s 312ms/step - loss: 0.0582 - accuracy: 0.9770 - val_loss: 0.0805 - val_accuracy: 0.9665\n",
      "Epoch 13/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0499 - accuracy: 0.9803\n",
      "Epoch 13: val_loss did not improve from 0.07286\n",
      "140/140 [==============================] - 43s 308ms/step - loss: 0.0499 - accuracy: 0.9803 - val_loss: 0.1055 - val_accuracy: 0.9678\n",
      "Epoch 14/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0518 - accuracy: 0.9792\n",
      "Epoch 14: val_loss did not improve from 0.07286\n",
      "140/140 [==============================] - 43s 309ms/step - loss: 0.0518 - accuracy: 0.9792 - val_loss: 0.0922 - val_accuracy: 0.9674\n",
      "Epoch 15/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0554 - accuracy: 0.9793\n",
      "Epoch 15: val_loss did not improve from 0.07286\n",
      "140/140 [==============================] - 43s 308ms/step - loss: 0.0554 - accuracy: 0.9793 - val_loss: 0.0761 - val_accuracy: 0.9732\n",
      "Epoch 16/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0450 - accuracy: 0.9822\n",
      "Epoch 16: val_loss did not improve from 0.07286\n",
      "140/140 [==============================] - 43s 310ms/step - loss: 0.0450 - accuracy: 0.9822 - val_loss: 0.0913 - val_accuracy: 0.9741\n",
      "Epoch 17/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0394 - accuracy: 0.9846\n",
      "Epoch 17: val_loss did not improve from 0.07286\n",
      "140/140 [==============================] - 43s 308ms/step - loss: 0.0394 - accuracy: 0.9846 - val_loss: 0.0938 - val_accuracy: 0.9709\n",
      "Epoch 18/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0404 - accuracy: 0.9841\n",
      "Epoch 18: val_loss did not improve from 0.07286\n",
      "140/140 [==============================] - 44s 313ms/step - loss: 0.0404 - accuracy: 0.9841 - val_loss: 0.0780 - val_accuracy: 0.9723\n",
      "Epoch 19/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0365 - accuracy: 0.9863\n",
      "Epoch 19: val_loss did not improve from 0.07286\n",
      "140/140 [==============================] - 43s 310ms/step - loss: 0.0365 - accuracy: 0.9863 - val_loss: 0.0852 - val_accuracy: 0.9736\n",
      "Epoch 20/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0345 - accuracy: 0.9876\n",
      "Epoch 20: val_loss did not improve from 0.07286\n",
      "140/140 [==============================] - 43s 309ms/step - loss: 0.0345 - accuracy: 0.9876 - val_loss: 0.0980 - val_accuracy: 0.9718\n",
      "Epoch 21/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0431 - accuracy: 0.9850\n",
      "Epoch 21: val_loss did not improve from 0.07286\n",
      "140/140 [==============================] - 43s 309ms/step - loss: 0.0431 - accuracy: 0.9850 - val_loss: 0.0888 - val_accuracy: 0.9736\n",
      "Epoch 22/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0425 - accuracy: 0.9845\n",
      "Epoch 22: val_loss improved from 0.07286 to 0.06570, saving model to my_best_model_fold_4.hdf5\n",
      "140/140 [==============================] - 43s 310ms/step - loss: 0.0425 - accuracy: 0.9845 - val_loss: 0.0657 - val_accuracy: 0.9785\n",
      "Epoch 23/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0298 - accuracy: 0.9887\n",
      "Epoch 23: val_loss did not improve from 0.06570\n",
      "140/140 [==============================] - 43s 308ms/step - loss: 0.0298 - accuracy: 0.9887 - val_loss: 0.0861 - val_accuracy: 0.9768\n",
      "Epoch 24/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0275 - accuracy: 0.9905\n",
      "Epoch 24: val_loss did not improve from 0.06570\n",
      "140/140 [==============================] - 43s 309ms/step - loss: 0.0275 - accuracy: 0.9905 - val_loss: 0.0946 - val_accuracy: 0.9790\n",
      "Epoch 25/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0281 - accuracy: 0.9894\n",
      "Epoch 25: val_loss did not improve from 0.06570\n",
      "140/140 [==============================] - 43s 308ms/step - loss: 0.0281 - accuracy: 0.9894 - val_loss: 0.1082 - val_accuracy: 0.9776\n",
      "Epoch 26/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0391 - accuracy: 0.9851\n",
      "Epoch 26: val_loss improved from 0.06570 to 0.06459, saving model to my_best_model_fold_4.hdf5\n",
      "140/140 [==============================] - 43s 309ms/step - loss: 0.0391 - accuracy: 0.9851 - val_loss: 0.0646 - val_accuracy: 0.9723\n",
      "Epoch 27/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0330 - accuracy: 0.9892\n",
      "Epoch 27: val_loss did not improve from 0.06459\n",
      "140/140 [==============================] - 43s 310ms/step - loss: 0.0330 - accuracy: 0.9892 - val_loss: 0.0662 - val_accuracy: 0.9808\n",
      "Epoch 28/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0345 - accuracy: 0.9887\n",
      "Epoch 28: val_loss did not improve from 0.06459\n",
      "140/140 [==============================] - 44s 310ms/step - loss: 0.0345 - accuracy: 0.9887 - val_loss: 0.0878 - val_accuracy: 0.9785\n",
      "Epoch 29/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0240 - accuracy: 0.9908\n",
      "Epoch 29: val_loss did not improve from 0.06459\n",
      "140/140 [==============================] - 43s 308ms/step - loss: 0.0240 - accuracy: 0.9908 - val_loss: 0.0962 - val_accuracy: 0.9776\n",
      "Epoch 30/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0274 - accuracy: 0.9911\n",
      "Epoch 30: val_loss did not improve from 0.06459\n",
      "140/140 [==============================] - 43s 309ms/step - loss: 0.0274 - accuracy: 0.9911 - val_loss: 0.0784 - val_accuracy: 0.9799\n",
      "Epoch 31/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0201 - accuracy: 0.9930\n",
      "Epoch 31: val_loss did not improve from 0.06459\n",
      "140/140 [==============================] - 44s 311ms/step - loss: 0.0201 - accuracy: 0.9930 - val_loss: 0.0868 - val_accuracy: 0.9812\n",
      "Epoch 32/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0192 - accuracy: 0.9935\n",
      "Epoch 32: val_loss did not improve from 0.06459\n",
      "140/140 [==============================] - 43s 310ms/step - loss: 0.0192 - accuracy: 0.9935 - val_loss: 0.0862 - val_accuracy: 0.9808\n",
      "Epoch 33/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0160 - accuracy: 0.9947\n",
      "Epoch 33: val_loss did not improve from 0.06459\n",
      "140/140 [==============================] - 44s 311ms/step - loss: 0.0160 - accuracy: 0.9947 - val_loss: 0.0865 - val_accuracy: 0.9839\n",
      "Epoch 34/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0225 - accuracy: 0.9934\n",
      "Epoch 34: val_loss did not improve from 0.06459\n",
      "140/140 [==============================] - 43s 310ms/step - loss: 0.0225 - accuracy: 0.9934 - val_loss: 0.0755 - val_accuracy: 0.9844\n",
      "Epoch 35/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0126 - accuracy: 0.9953\n",
      "Epoch 35: val_loss did not improve from 0.06459\n",
      "140/140 [==============================] - 43s 308ms/step - loss: 0.0126 - accuracy: 0.9953 - val_loss: 0.0775 - val_accuracy: 0.9852\n",
      "Epoch 36/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0118 - accuracy: 0.9962\n",
      "Epoch 36: val_loss did not improve from 0.06459\n",
      "140/140 [==============================] - 43s 308ms/step - loss: 0.0118 - accuracy: 0.9962 - val_loss: 0.0868 - val_accuracy: 0.9844\n",
      "Epoch 37/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0167 - accuracy: 0.9940\n",
      "Epoch 37: val_loss did not improve from 0.06459\n",
      "140/140 [==============================] - 44s 313ms/step - loss: 0.0167 - accuracy: 0.9940 - val_loss: 0.1014 - val_accuracy: 0.9790\n",
      "Epoch 38/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0171 - accuracy: 0.9946\n",
      "Epoch 38: val_loss did not improve from 0.06459\n",
      "140/140 [==============================] - 43s 310ms/step - loss: 0.0171 - accuracy: 0.9946 - val_loss: 0.0894 - val_accuracy: 0.9844\n",
      "Epoch 39/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0166 - accuracy: 0.9953\n",
      "Epoch 39: val_loss did not improve from 0.06459\n",
      "140/140 [==============================] - 43s 309ms/step - loss: 0.0166 - accuracy: 0.9953 - val_loss: 0.0997 - val_accuracy: 0.9839\n",
      "Epoch 40/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0073 - accuracy: 0.9971\n",
      "Epoch 40: val_loss did not improve from 0.06459\n",
      "140/140 [==============================] - 43s 310ms/step - loss: 0.0073 - accuracy: 0.9971 - val_loss: 0.1024 - val_accuracy: 0.9835\n",
      "Epoch 41/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0169 - accuracy: 0.9958\n",
      "Epoch 41: val_loss did not improve from 0.06459\n",
      "140/140 [==============================] - 43s 309ms/step - loss: 0.0169 - accuracy: 0.9958 - val_loss: 0.0914 - val_accuracy: 0.9826\n",
      "Epoch 42/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0111 - accuracy: 0.9968\n",
      "Epoch 42: val_loss did not improve from 0.06459\n",
      "140/140 [==============================] - 43s 309ms/step - loss: 0.0111 - accuracy: 0.9968 - val_loss: 0.0986 - val_accuracy: 0.9839\n",
      "Epoch 43/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0066 - accuracy: 0.9977\n",
      "Epoch 43: val_loss did not improve from 0.06459\n",
      "140/140 [==============================] - 43s 310ms/step - loss: 0.0066 - accuracy: 0.9977 - val_loss: 0.1138 - val_accuracy: 0.9835\n",
      "Epoch 44/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0109 - accuracy: 0.9973\n",
      "Epoch 44: val_loss did not improve from 0.06459\n",
      "140/140 [==============================] - 43s 309ms/step - loss: 0.0109 - accuracy: 0.9973 - val_loss: 0.1084 - val_accuracy: 0.9830\n",
      "Epoch 45/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0132 - accuracy: 0.9968\n",
      "Epoch 45: val_loss did not improve from 0.06459\n",
      "140/140 [==============================] - 43s 309ms/step - loss: 0.0132 - accuracy: 0.9968 - val_loss: 0.0750 - val_accuracy: 0.9835\n",
      "Epoch 46/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0087 - accuracy: 0.9975\n",
      "Epoch 46: val_loss did not improve from 0.06459\n",
      "140/140 [==============================] - 43s 309ms/step - loss: 0.0087 - accuracy: 0.9975 - val_loss: 0.0933 - val_accuracy: 0.9803\n",
      "Epoch 47/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0101 - accuracy: 0.9971\n",
      "Epoch 47: val_loss did not improve from 0.06459\n",
      "140/140 [==============================] - 43s 308ms/step - loss: 0.0101 - accuracy: 0.9971 - val_loss: 0.0905 - val_accuracy: 0.9852\n",
      "Epoch 48/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0149 - accuracy: 0.9946\n",
      "Epoch 48: val_loss did not improve from 0.06459\n",
      "140/140 [==============================] - 43s 308ms/step - loss: 0.0149 - accuracy: 0.9946 - val_loss: 0.0853 - val_accuracy: 0.9826\n",
      "Epoch 49/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0132 - accuracy: 0.9949\n",
      "Epoch 49: val_loss did not improve from 0.06459\n",
      "140/140 [==============================] - 43s 309ms/step - loss: 0.0132 - accuracy: 0.9949 - val_loss: 0.0966 - val_accuracy: 0.9866\n",
      "Epoch 50/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0108 - accuracy: 0.9968\n",
      "Epoch 50: val_loss did not improve from 0.06459\n",
      "140/140 [==============================] - 44s 311ms/step - loss: 0.0108 - accuracy: 0.9968 - val_loss: 0.0910 - val_accuracy: 0.9852\n",
      "Epoch 51/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0050 - accuracy: 0.9984\n",
      "Epoch 51: val_loss did not improve from 0.06459\n",
      "140/140 [==============================] - 43s 309ms/step - loss: 0.0050 - accuracy: 0.9984 - val_loss: 0.1039 - val_accuracy: 0.9848\n",
      "Epoch 52/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0072 - accuracy: 0.9975\n",
      "Epoch 52: val_loss did not improve from 0.06459\n",
      "140/140 [==============================] - 44s 311ms/step - loss: 0.0072 - accuracy: 0.9975 - val_loss: 0.0796 - val_accuracy: 0.9879\n",
      "Epoch 53/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0090 - accuracy: 0.9968\n",
      "Epoch 53: val_loss did not improve from 0.06459\n",
      "140/140 [==============================] - 43s 309ms/step - loss: 0.0090 - accuracy: 0.9968 - val_loss: 0.1058 - val_accuracy: 0.9866\n",
      "Epoch 54/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0103 - accuracy: 0.9960\n",
      "Epoch 54: val_loss did not improve from 0.06459\n",
      "140/140 [==============================] - 43s 309ms/step - loss: 0.0103 - accuracy: 0.9960 - val_loss: 0.1021 - val_accuracy: 0.9852\n",
      "Epoch 55/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0149 - accuracy: 0.9960\n",
      "Epoch 55: val_loss did not improve from 0.06459\n",
      "140/140 [==============================] - 43s 309ms/step - loss: 0.0149 - accuracy: 0.9960 - val_loss: 0.0797 - val_accuracy: 0.9861\n",
      "Epoch 56/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0094 - accuracy: 0.9969\n",
      "Epoch 56: val_loss did not improve from 0.06459\n",
      "140/140 [==============================] - 43s 310ms/step - loss: 0.0094 - accuracy: 0.9969 - val_loss: 0.0980 - val_accuracy: 0.9879\n",
      "Epoch 57/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0034 - accuracy: 0.9990\n",
      "Epoch 57: val_loss did not improve from 0.06459\n",
      "140/140 [==============================] - 43s 310ms/step - loss: 0.0034 - accuracy: 0.9990 - val_loss: 0.1147 - val_accuracy: 0.9870\n",
      "Epoch 58/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0195 - accuracy: 0.9949\n",
      "Epoch 58: val_loss did not improve from 0.06459\n",
      "140/140 [==============================] - 43s 310ms/step - loss: 0.0195 - accuracy: 0.9949 - val_loss: 0.1000 - val_accuracy: 0.9830\n",
      "Epoch 59/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0214 - accuracy: 0.9947\n",
      "Epoch 59: val_loss did not improve from 0.06459\n",
      "140/140 [==============================] - 43s 309ms/step - loss: 0.0214 - accuracy: 0.9947 - val_loss: 0.0817 - val_accuracy: 0.9866\n",
      "Epoch 60/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0096 - accuracy: 0.9975\n",
      "Epoch 60: val_loss did not improve from 0.06459\n",
      "140/140 [==============================] - 43s 310ms/step - loss: 0.0096 - accuracy: 0.9975 - val_loss: 0.1094 - val_accuracy: 0.9857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-02 05:36:14.590370: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2023-06-02 05:36:14.590761: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - 9s 237ms/step\n",
      "======================= manual compute ==============\n",
      "accuracy: 0.9856951274027715, precision: 0.9828473413379074, recall: 0.9896373056994818, specificity: 0.9814643188137164, mcc: 0.9713722043331194 ,f1-score: 0.9862306368330465, auc: 0.9943388540211063, prc: 0.9908892567872876\n",
      "=========================== FOLD 5 ==================================\n",
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-02 05:36:24.834385: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2023-06-02 05:36:24.834623: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    140/Unknown - 50s 244ms/step - loss: 0.5857 - accuracy: 0.6869"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-02 05:37:14.921779: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2023-06-02 05:37:14.922014: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 0.44978, saving model to my_best_model_fold_5.hdf5\n",
      "140/140 [==============================] - 59s 312ms/step - loss: 0.5857 - accuracy: 0.6869 - val_loss: 0.4498 - val_accuracy: 0.8207\n",
      "Epoch 2/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.3395 - accuracy: 0.8599\n",
      "Epoch 2: val_loss improved from 0.44978 to 0.25489, saving model to my_best_model_fold_5.hdf5\n",
      "140/140 [==============================] - 43s 309ms/step - loss: 0.3395 - accuracy: 0.8599 - val_loss: 0.2549 - val_accuracy: 0.9017\n",
      "Epoch 3/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.2264 - accuracy: 0.9082\n",
      "Epoch 3: val_loss improved from 0.25489 to 0.19102, saving model to my_best_model_fold_5.hdf5\n",
      "140/140 [==============================] - 43s 310ms/step - loss: 0.2264 - accuracy: 0.9082 - val_loss: 0.1910 - val_accuracy: 0.9133\n",
      "Epoch 4/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.1871 - accuracy: 0.9200\n",
      "Epoch 4: val_loss improved from 0.19102 to 0.17169, saving model to my_best_model_fold_5.hdf5\n",
      "140/140 [==============================] - 43s 310ms/step - loss: 0.1871 - accuracy: 0.9200 - val_loss: 0.1717 - val_accuracy: 0.9191\n",
      "Epoch 5/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.1548 - accuracy: 0.9319\n",
      "Epoch 5: val_loss improved from 0.17169 to 0.14162, saving model to my_best_model_fold_5.hdf5\n",
      "140/140 [==============================] - 44s 311ms/step - loss: 0.1548 - accuracy: 0.9319 - val_loss: 0.1416 - val_accuracy: 0.9182\n",
      "Epoch 6/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.1468 - accuracy: 0.9333\n",
      "Epoch 6: val_loss did not improve from 0.14162\n",
      "140/140 [==============================] - 43s 308ms/step - loss: 0.1468 - accuracy: 0.9333 - val_loss: 0.1457 - val_accuracy: 0.9110\n",
      "Epoch 7/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.1223 - accuracy: 0.9427\n",
      "Epoch 7: val_loss improved from 0.14162 to 0.12447, saving model to my_best_model_fold_5.hdf5\n",
      "140/140 [==============================] - 44s 311ms/step - loss: 0.1223 - accuracy: 0.9427 - val_loss: 0.1245 - val_accuracy: 0.9428\n",
      "Epoch 8/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.1227 - accuracy: 0.9422\n",
      "Epoch 8: val_loss did not improve from 0.12447\n",
      "140/140 [==============================] - 43s 308ms/step - loss: 0.1227 - accuracy: 0.9422 - val_loss: 0.1321 - val_accuracy: 0.9204\n",
      "Epoch 9/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.1155 - accuracy: 0.9470\n",
      "Epoch 9: val_loss did not improve from 0.12447\n",
      "140/140 [==============================] - 43s 309ms/step - loss: 0.1155 - accuracy: 0.9470 - val_loss: 0.1323 - val_accuracy: 0.9405\n",
      "Epoch 10/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.1036 - accuracy: 0.9533\n",
      "Epoch 10: val_loss improved from 0.12447 to 0.11769, saving model to my_best_model_fold_5.hdf5\n",
      "140/140 [==============================] - 43s 309ms/step - loss: 0.1036 - accuracy: 0.9533 - val_loss: 0.1177 - val_accuracy: 0.9508\n",
      "Epoch 11/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.1014 - accuracy: 0.9591\n",
      "Epoch 11: val_loss improved from 0.11769 to 0.10200, saving model to my_best_model_fold_5.hdf5\n",
      "140/140 [==============================] - 43s 310ms/step - loss: 0.1014 - accuracy: 0.9591 - val_loss: 0.1020 - val_accuracy: 0.9566\n",
      "Epoch 12/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0921 - accuracy: 0.9618\n",
      "Epoch 12: val_loss improved from 0.10200 to 0.08895, saving model to my_best_model_fold_5.hdf5\n",
      "140/140 [==============================] - 43s 309ms/step - loss: 0.0921 - accuracy: 0.9618 - val_loss: 0.0889 - val_accuracy: 0.9678\n",
      "Epoch 13/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0786 - accuracy: 0.9701\n",
      "Epoch 13: val_loss did not improve from 0.08895\n",
      "140/140 [==============================] - 43s 308ms/step - loss: 0.0786 - accuracy: 0.9701 - val_loss: 0.0957 - val_accuracy: 0.9714\n",
      "Epoch 14/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0851 - accuracy: 0.9659\n",
      "Epoch 14: val_loss improved from 0.08895 to 0.08866, saving model to my_best_model_fold_5.hdf5\n",
      "140/140 [==============================] - 43s 310ms/step - loss: 0.0851 - accuracy: 0.9659 - val_loss: 0.0887 - val_accuracy: 0.9665\n",
      "Epoch 15/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0698 - accuracy: 0.9711\n",
      "Epoch 15: val_loss did not improve from 0.08866\n",
      "140/140 [==============================] - 43s 308ms/step - loss: 0.0698 - accuracy: 0.9711 - val_loss: 0.1001 - val_accuracy: 0.9723\n",
      "Epoch 16/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0703 - accuracy: 0.9734\n",
      "Epoch 16: val_loss improved from 0.08866 to 0.08270, saving model to my_best_model_fold_5.hdf5\n",
      "140/140 [==============================] - 44s 311ms/step - loss: 0.0703 - accuracy: 0.9734 - val_loss: 0.0827 - val_accuracy: 0.9727\n",
      "Epoch 17/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0649 - accuracy: 0.9756\n",
      "Epoch 17: val_loss did not improve from 0.08270\n",
      "140/140 [==============================] - 43s 308ms/step - loss: 0.0649 - accuracy: 0.9756 - val_loss: 0.0874 - val_accuracy: 0.9772\n",
      "Epoch 18/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0610 - accuracy: 0.9760\n",
      "Epoch 18: val_loss improved from 0.08270 to 0.07749, saving model to my_best_model_fold_5.hdf5\n",
      "140/140 [==============================] - 43s 310ms/step - loss: 0.0610 - accuracy: 0.9760 - val_loss: 0.0775 - val_accuracy: 0.9776\n",
      "Epoch 19/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0552 - accuracy: 0.9802\n",
      "Epoch 19: val_loss improved from 0.07749 to 0.07484, saving model to my_best_model_fold_5.hdf5\n",
      "140/140 [==============================] - 44s 311ms/step - loss: 0.0552 - accuracy: 0.9802 - val_loss: 0.0748 - val_accuracy: 0.9808\n",
      "Epoch 20/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0550 - accuracy: 0.9782\n",
      "Epoch 20: val_loss improved from 0.07484 to 0.07091, saving model to my_best_model_fold_5.hdf5\n",
      "140/140 [==============================] - 43s 309ms/step - loss: 0.0550 - accuracy: 0.9782 - val_loss: 0.0709 - val_accuracy: 0.9794\n",
      "Epoch 21/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0436 - accuracy: 0.9820\n",
      "Epoch 21: val_loss did not improve from 0.07091\n",
      "140/140 [==============================] - 43s 309ms/step - loss: 0.0436 - accuracy: 0.9820 - val_loss: 0.0856 - val_accuracy: 0.9772\n",
      "Epoch 22/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0485 - accuracy: 0.9815\n",
      "Epoch 22: val_loss did not improve from 0.07091\n",
      "140/140 [==============================] - 43s 307ms/step - loss: 0.0485 - accuracy: 0.9815 - val_loss: 0.0739 - val_accuracy: 0.9776\n",
      "Epoch 23/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0539 - accuracy: 0.9813\n",
      "Epoch 23: val_loss did not improve from 0.07091\n",
      "140/140 [==============================] - 43s 308ms/step - loss: 0.0539 - accuracy: 0.9813 - val_loss: 0.0720 - val_accuracy: 0.9759\n",
      "Epoch 24/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0430 - accuracy: 0.9837\n",
      "Epoch 24: val_loss did not improve from 0.07091\n",
      "140/140 [==============================] - 43s 308ms/step - loss: 0.0430 - accuracy: 0.9837 - val_loss: 0.0799 - val_accuracy: 0.9768\n",
      "Epoch 25/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0434 - accuracy: 0.9821\n",
      "Epoch 25: val_loss did not improve from 0.07091\n",
      "140/140 [==============================] - 43s 309ms/step - loss: 0.0434 - accuracy: 0.9821 - val_loss: 0.0844 - val_accuracy: 0.9772\n",
      "Epoch 26/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0449 - accuracy: 0.9828\n",
      "Epoch 26: val_loss did not improve from 0.07091\n",
      "140/140 [==============================] - 43s 308ms/step - loss: 0.0449 - accuracy: 0.9828 - val_loss: 0.0771 - val_accuracy: 0.9781\n",
      "Epoch 27/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0344 - accuracy: 0.9845\n",
      "Epoch 27: val_loss did not improve from 0.07091\n",
      "140/140 [==============================] - 43s 308ms/step - loss: 0.0344 - accuracy: 0.9845 - val_loss: 0.1004 - val_accuracy: 0.9763\n",
      "Epoch 28/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0347 - accuracy: 0.9869\n",
      "Epoch 28: val_loss did not improve from 0.07091\n",
      "140/140 [==============================] - 43s 308ms/step - loss: 0.0347 - accuracy: 0.9869 - val_loss: 0.0806 - val_accuracy: 0.9776\n",
      "Epoch 29/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0331 - accuracy: 0.9858\n",
      "Epoch 29: val_loss did not improve from 0.07091\n",
      "140/140 [==============================] - 43s 308ms/step - loss: 0.0331 - accuracy: 0.9858 - val_loss: 0.0847 - val_accuracy: 0.9812\n",
      "Epoch 30/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0300 - accuracy: 0.9870\n",
      "Epoch 30: val_loss did not improve from 0.07091\n",
      "140/140 [==============================] - 43s 309ms/step - loss: 0.0300 - accuracy: 0.9870 - val_loss: 0.0879 - val_accuracy: 0.9817\n",
      "Epoch 31/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0320 - accuracy: 0.9873\n",
      "Epoch 31: val_loss did not improve from 0.07091\n",
      "140/140 [==============================] - 43s 306ms/step - loss: 0.0320 - accuracy: 0.9873 - val_loss: 0.0824 - val_accuracy: 0.9821\n",
      "Epoch 32/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0248 - accuracy: 0.9905\n",
      "Epoch 32: val_loss did not improve from 0.07091\n",
      "140/140 [==============================] - 43s 310ms/step - loss: 0.0248 - accuracy: 0.9905 - val_loss: 0.1273 - val_accuracy: 0.9772\n",
      "Epoch 33/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0324 - accuracy: 0.9880\n",
      "Epoch 33: val_loss did not improve from 0.07091\n",
      "140/140 [==============================] - 43s 308ms/step - loss: 0.0324 - accuracy: 0.9880 - val_loss: 0.0977 - val_accuracy: 0.9821\n",
      "Epoch 34/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0309 - accuracy: 0.9891\n",
      "Epoch 34: val_loss did not improve from 0.07091\n",
      "140/140 [==============================] - 43s 309ms/step - loss: 0.0309 - accuracy: 0.9891 - val_loss: 0.0801 - val_accuracy: 0.9812\n",
      "Epoch 35/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0226 - accuracy: 0.9915\n",
      "Epoch 35: val_loss did not improve from 0.07091\n",
      "140/140 [==============================] - 43s 310ms/step - loss: 0.0226 - accuracy: 0.9915 - val_loss: 0.0964 - val_accuracy: 0.9826\n",
      "Epoch 36/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0205 - accuracy: 0.9933\n",
      "Epoch 36: val_loss did not improve from 0.07091\n",
      "140/140 [==============================] - 43s 309ms/step - loss: 0.0205 - accuracy: 0.9933 - val_loss: 0.0980 - val_accuracy: 0.9830\n",
      "Epoch 37/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0220 - accuracy: 0.9925\n",
      "Epoch 37: val_loss did not improve from 0.07091\n",
      "140/140 [==============================] - 43s 307ms/step - loss: 0.0220 - accuracy: 0.9925 - val_loss: 0.1015 - val_accuracy: 0.9785\n",
      "Epoch 38/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0262 - accuracy: 0.9924\n",
      "Epoch 38: val_loss did not improve from 0.07091\n",
      "140/140 [==============================] - 43s 309ms/step - loss: 0.0262 - accuracy: 0.9924 - val_loss: 0.0780 - val_accuracy: 0.9861\n",
      "Epoch 39/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0196 - accuracy: 0.9935\n",
      "Epoch 39: val_loss did not improve from 0.07091\n",
      "140/140 [==============================] - 43s 309ms/step - loss: 0.0196 - accuracy: 0.9935 - val_loss: 0.0846 - val_accuracy: 0.9844\n",
      "Epoch 40/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0167 - accuracy: 0.9944\n",
      "Epoch 40: val_loss did not improve from 0.07091\n",
      "140/140 [==============================] - 43s 307ms/step - loss: 0.0167 - accuracy: 0.9944 - val_loss: 0.1015 - val_accuracy: 0.9852\n",
      "Epoch 41/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0153 - accuracy: 0.9952\n",
      "Epoch 41: val_loss did not improve from 0.07091\n",
      "140/140 [==============================] - 43s 308ms/step - loss: 0.0153 - accuracy: 0.9952 - val_loss: 0.0939 - val_accuracy: 0.9844\n",
      "Epoch 42/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0194 - accuracy: 0.9945\n",
      "Epoch 42: val_loss did not improve from 0.07091\n",
      "140/140 [==============================] - 43s 309ms/step - loss: 0.0194 - accuracy: 0.9945 - val_loss: 0.0969 - val_accuracy: 0.9857\n",
      "Epoch 43/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0175 - accuracy: 0.9947\n",
      "Epoch 43: val_loss did not improve from 0.07091\n",
      "140/140 [==============================] - 43s 308ms/step - loss: 0.0175 - accuracy: 0.9947 - val_loss: 0.0869 - val_accuracy: 0.9866\n",
      "Epoch 44/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0120 - accuracy: 0.9961\n",
      "Epoch 44: val_loss did not improve from 0.07091\n",
      "140/140 [==============================] - 43s 309ms/step - loss: 0.0120 - accuracy: 0.9961 - val_loss: 0.0928 - val_accuracy: 0.9866\n",
      "Epoch 45/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0133 - accuracy: 0.9964\n",
      "Epoch 45: val_loss did not improve from 0.07091\n",
      "140/140 [==============================] - 43s 307ms/step - loss: 0.0133 - accuracy: 0.9964 - val_loss: 0.0937 - val_accuracy: 0.9857\n",
      "Epoch 46/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0108 - accuracy: 0.9966\n",
      "Epoch 46: val_loss did not improve from 0.07091\n",
      "140/140 [==============================] - 43s 308ms/step - loss: 0.0108 - accuracy: 0.9966 - val_loss: 0.1183 - val_accuracy: 0.9870\n",
      "Epoch 47/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0188 - accuracy: 0.9950\n",
      "Epoch 47: val_loss did not improve from 0.07091\n",
      "140/140 [==============================] - 43s 308ms/step - loss: 0.0188 - accuracy: 0.9950 - val_loss: 0.0905 - val_accuracy: 0.9839\n",
      "Epoch 48/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0131 - accuracy: 0.9973\n",
      "Epoch 48: val_loss did not improve from 0.07091\n",
      "140/140 [==============================] - 43s 308ms/step - loss: 0.0131 - accuracy: 0.9973 - val_loss: 0.0984 - val_accuracy: 0.9844\n",
      "Epoch 49/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0093 - accuracy: 0.9969\n",
      "Epoch 49: val_loss did not improve from 0.07091\n",
      "140/140 [==============================] - 43s 309ms/step - loss: 0.0093 - accuracy: 0.9969 - val_loss: 0.1058 - val_accuracy: 0.9861\n",
      "Epoch 50/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0117 - accuracy: 0.9969\n",
      "Epoch 50: val_loss did not improve from 0.07091\n",
      "140/140 [==============================] - 43s 308ms/step - loss: 0.0117 - accuracy: 0.9969 - val_loss: 0.0965 - val_accuracy: 0.9848\n",
      "Epoch 51/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0149 - accuracy: 0.9961\n",
      "Epoch 51: val_loss did not improve from 0.07091\n",
      "140/140 [==============================] - 43s 307ms/step - loss: 0.0149 - accuracy: 0.9961 - val_loss: 0.1052 - val_accuracy: 0.9848\n",
      "Epoch 52/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0192 - accuracy: 0.9944\n",
      "Epoch 52: val_loss did not improve from 0.07091\n",
      "140/140 [==============================] - 43s 308ms/step - loss: 0.0192 - accuracy: 0.9944 - val_loss: 0.1102 - val_accuracy: 0.9826\n",
      "Epoch 53/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0182 - accuracy: 0.9955\n",
      "Epoch 53: val_loss did not improve from 0.07091\n",
      "140/140 [==============================] - 43s 308ms/step - loss: 0.0182 - accuracy: 0.9955 - val_loss: 0.0903 - val_accuracy: 0.9839\n",
      "Epoch 54/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0062 - accuracy: 0.9987\n",
      "Epoch 54: val_loss did not improve from 0.07091\n",
      "140/140 [==============================] - 43s 308ms/step - loss: 0.0062 - accuracy: 0.9987 - val_loss: 0.1100 - val_accuracy: 0.9866\n",
      "Epoch 55/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0082 - accuracy: 0.9977\n",
      "Epoch 55: val_loss did not improve from 0.07091\n",
      "140/140 [==============================] - 43s 308ms/step - loss: 0.0082 - accuracy: 0.9977 - val_loss: 0.1015 - val_accuracy: 0.9852\n",
      "Epoch 56/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0065 - accuracy: 0.9984\n",
      "Epoch 56: val_loss did not improve from 0.07091\n",
      "140/140 [==============================] - 43s 306ms/step - loss: 0.0065 - accuracy: 0.9984 - val_loss: 0.0983 - val_accuracy: 0.9875\n",
      "Epoch 57/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0057 - accuracy: 0.9980\n",
      "Epoch 57: val_loss did not improve from 0.07091\n",
      "140/140 [==============================] - 43s 306ms/step - loss: 0.0057 - accuracy: 0.9980 - val_loss: 0.1103 - val_accuracy: 0.9875\n",
      "Epoch 58/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0062 - accuracy: 0.9980\n",
      "Epoch 58: val_loss did not improve from 0.07091\n",
      "140/140 [==============================] - 43s 310ms/step - loss: 0.0062 - accuracy: 0.9980 - val_loss: 0.1206 - val_accuracy: 0.9857\n",
      "Epoch 59/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0086 - accuracy: 0.9980\n",
      "Epoch 59: val_loss did not improve from 0.07091\n",
      "140/140 [==============================] - 43s 307ms/step - loss: 0.0086 - accuracy: 0.9980 - val_loss: 0.1008 - val_accuracy: 0.9875\n",
      "Epoch 60/60\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0086 - accuracy: 0.9980\n",
      "Epoch 60: val_loss did not improve from 0.07091\n",
      "140/140 [==============================] - 43s 308ms/step - loss: 0.0086 - accuracy: 0.9980 - val_loss: 0.0948 - val_accuracy: 0.9866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-02 06:19:56.154855: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2023-06-02 06:19:56.155089: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - 9s 240ms/step\n",
      "======================= manual compute ==============\n",
      "accuracy: 0.9865891819400984, precision: 0.9871677360219981, recall: 0.9853613906678865, specificity: 0.9877622377622378, mcc: 0.973164878559456 ,f1-score: 0.9862637362637363, auc: 0.9932481173903863, prc: 0.9901856718838212\n"
     ]
    }
   ],
   "source": [
    "##### Hyperparameter\n",
    "EPOCHS=60\n",
    "DTYPE = 'float16'\n",
    "\n",
    "\n",
    "\n",
    "acc_list = []\n",
    "recall_list = []\n",
    "prec_list = []\n",
    "spec_list = []\n",
    "f1_list = []\n",
    "mcc_list = []\n",
    "auc_list = []\n",
    "prc_list = []\n",
    "dnn_fpr_list = []\n",
    "dnn_tpr_list = []\n",
    "dnn_fpr = []  \n",
    "dnn_tpr = []\n",
    "dnn_prc_prec = []\n",
    "dnn_prc_recall = []\n",
    "FOLD = 1\n",
    "for train, test in train_test:\n",
    "  print(f\"=========================== FOLD {FOLD} ==================================\")\n",
    "  train_dataset = tf.data.Dataset.from_generator(lambda: train, tf.uint32)\n",
    "\n",
    "  train_dataset = train_dataset.map(lambda i: tf.py_function(func=func, \n",
    "                                                inp=[i], \n",
    "                                                Tout=[DTYPE,DTYPE, DTYPE\n",
    "                                                      ]\n",
    "                                                ), \n",
    "                        num_parallel_calls=tf.data.AUTOTUNE).map(_fixup_shape).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "  val_dataset = tf.data.Dataset.from_generator(lambda: test, tf.uint32)\n",
    "\n",
    "  val_dataset = val_dataset.map(lambda i: tf.py_function(func=func, \n",
    "                                                inp=[i], \n",
    "                                                Tout=[DTYPE,\n",
    "                                                      DTYPE, DTYPE]\n",
    "                                                ), \n",
    "                        num_parallel_calls=tf.data.AUTOTUNE).map(_fixup_shape).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "  checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=f'my_best_model_fold_{FOLD}.hdf5', monitor='val_loss',verbose=1, save_best_only=True, mode='min')\n",
    "  model = multi_cnn()\n",
    "\n",
    "  model.fit(train_dataset, epochs=EPOCHS, validation_data=val_dataset, callbacks=[checkpoint])\n",
    "\n",
    "  test_dataset = tf.data.Dataset.from_generator(lambda: test, tf.uint32)\n",
    "  test_dataset = test_dataset.map(lambda i: tf.py_function(func=func, \n",
    "                                              inp=[i], \n",
    "                                              Tout=[DTYPE,\n",
    "                                                    DTYPE, DTYPE]\n",
    "                                              ), \n",
    "                      num_parallel_calls=tf.data.AUTOTUNE).map(_fixup_shape).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "  pred = model.predict(test_dataset)\n",
    "  cm1=confusion_matrix(pair_dataframe['label'][test].values, np.round(pred))\n",
    "  acc = (cm1[0,0]+cm1[1,1])/(cm1[0,0]+cm1[0,1]+cm1[1,0]+cm1[1,1])\n",
    "  spec= (cm1[0,0])/(cm1[0,0]+cm1[0,1])\n",
    "  sens = (cm1[1,1])/(cm1[1,0]+cm1[1,1])\n",
    "  prec=cm1[1,1]/(cm1[1,1]+cm1[0,1])\n",
    "  rec=cm1[1,1]/(cm1[1,1]+cm1[1,0])\n",
    "  f1 = 2 * (prec * rec) / (prec + rec)\n",
    "  mcc = matthews_corrcoef(pair_dataframe['label'][test].values, np.round(pred))\n",
    "\n",
    "  fpr, tpr, _= roc_curve(pair_dataframe['label'][test].values[:], pred[:])   \n",
    "  prc_prec, prc_recall, thresholds = precision_recall_curve(pair_dataframe['label'][test].values[:], pred[:])   \n",
    "  auc = metrics.roc_auc_score(pair_dataframe['label'][test].values, pred)\n",
    "  prc = metrics.average_precision_score(pair_dataframe['label'][test].values, pred)\n",
    "  acc_list.append(acc)\n",
    "  recall_list.append(rec)\n",
    "  prec_list.append(prec)\n",
    "  spec_list.append(spec)\n",
    "  f1_list.append(f1)\n",
    "  mcc_list.append(mcc)\n",
    "\n",
    "  dnn_fpr_list.append(fpr)\n",
    "  dnn_tpr_list.append(tpr)\n",
    "  auc_list.append(auc)\n",
    "  prc_list.append(prc)\n",
    "  FOLD += 1\n",
    "  print(\"======================= manual compute ==============\")\n",
    "  print (f'accuracy: {acc}, precision: {prec}, recall: {rec}, specificity: {spec}, mcc: {mcc} ,f1-score: {f1}, auc: {auc}, prc: {prc}')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "tqyXKE0PsZw1",
    "outputId": "06b9f8c9-d7b5-4f16-b73e-9473343b9eae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9858777733967241 ± 0.002719185119429832\n",
      "Specificity: 0.9846214314803199 ± 0.003053143629873357\n",
      "Precision: 0.9846537476939299 ± 0.0031221751648232887\n",
      "Recall: 0.9870574564388639 ± 0.0038838611160628487\n",
      "MCC: 0.9717464817861832 ± 0.005454518623884845\n",
      "AUC: 0.993853177184341 ± 0.00040063625049161834\n",
      "PRC:0.9908459036770327 ± 0.0009313701725177212\n",
      "F1_Score: 0.9858505375655113 ± 0.0029741656678989557\n",
      "0.9858777733967241\t0.9846537476939299\t0.9870574564388639\t0.9846214314803199\t0.9858505375655113\t0.9717464817861832\t0.993853177184341\t0.9908459036770327\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mean_acc = np.mean(acc_list)\n",
    "std_acc = np.std(acc_list)\n",
    "# var_acc = np.var(acc_list)\n",
    "print(\"Accuracy: \" + str(mean_acc) + \" ± \" + str(std_acc))\n",
    "# print(\"Accuracy_Var:\"+str(mean_acc)+\" ± \"+str(var_acc))\n",
    "\n",
    "mean_spec = np.mean(spec_list)\n",
    "std_spec = np.std(spec_list)\n",
    "print(\"Specificity: \" + str(mean_spec) + \" ± \" + str(std_spec))\n",
    "\n",
    "mean_prec = np.mean(prec_list)\n",
    "std_prec = np.std(prec_list)\n",
    "print(\"Precision: \" + str(mean_prec) + \" ± \" + str(std_prec))\n",
    "\n",
    "mean_recall = np.mean(recall_list)\n",
    "std_recall = np.std(recall_list)\n",
    "print(\"Recall: \" + str(mean_recall) + \" ± \" + str(std_recall))\n",
    "\n",
    "mean_mcc = np.mean(mcc_list)\n",
    "std_mcc = np.std(mcc_list)\n",
    "print(\"MCC: \" + str(mean_mcc) + \" ± \" + str(std_mcc))\n",
    "\n",
    "mean_auc = np.mean(auc_list)\n",
    "std_auc = np.std(auc_list)\n",
    "print(\"AUC: \" + str(mean_auc)  + \" ± \" + str(std_auc))\n",
    "\n",
    "mean_prc = np.mean(prc_list)\n",
    "std_prc = np.std(prc_list)\n",
    "print(\"PRC:\" + str(mean_prc)  + \" ± \" + str(std_prc)) \n",
    "\n",
    "mean_f1 = np.mean(f1_list)\n",
    "std_f1 = np.std(f1_list)\n",
    "print(\"F1_Score: \" + str(mean_f1)  + \" ± \" + str(std_f1)) \n",
    "\n",
    "print(str(mean_acc) + \"\\t\" + str(mean_prec) + \"\\t\" + str(mean_recall) + \"\\t\" + str(mean_spec) + \"\\t\" + str(mean_f1) + \"\\t\" + str(mean_mcc) + \"\\t\" + str(mean_auc) + \"\\t\" + str(mean_prc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OdEQZy7FIA1W"
   },
   "source": [
    "### 5-fold cross validation Hybrid model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "weuJfrXG2sFN"
   },
   "outputs": [],
   "source": [
    "# Use intermediate layer to transform pairs matrix\n",
    "pred_dataset = tf.data.Dataset.from_generator(lambda: range(len(pair_dataframe)), tf.uint32)\n",
    "\n",
    "pred_dataset = pred_dataset.map(lambda i: tf.py_function(func=func, \n",
    "                                              inp=[i], \n",
    "                                              Tout=[tf.float16,\n",
    "                                                    tf.float16, tf.float16]\n",
    "                                              ), \n",
    "                      num_parallel_calls=tf.data.AUTOTUNE).map(_fixup_shape).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "MxPKg_1UsZw2",
    "outputId": "6fcd2971-4b16-4634-ecc0-8184ec063cbe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175/175 [==============================] - 41s 229ms/step\n",
      "========================== START TRAINING TIME 1 ===========================\n",
      "========================== PERFOMANCE METRICS AT TRAINING TIME 1 ===========================\n",
      "accuracy: 0.9973190348525469, specificity: 0.9973190348525469, sensitivity: 0.9973190348525469, precision= 0.9973190348525469, recall: 0.9973190348525469 \n",
      "========================== START TRAINING TIME 2 ===========================\n",
      "========================== PERFOMANCE METRICS AT TRAINING TIME 2 ===========================\n",
      "accuracy: 0.9964253798033958, specificity: 0.998212689901698, sensitivity: 0.9946380697050938, precision= 0.9982062780269059, recall: 0.9946380697050938 \n",
      "========================== START TRAINING TIME 3 ===========================\n",
      "========================== PERFOMANCE METRICS AT TRAINING TIME 3 ===========================\n",
      "accuracy: 0.9955317247542449, specificity: 0.9964253798033958, sensitivity: 0.9946380697050938, precision= 0.9964189794091316, recall: 0.9946380697050938 \n",
      "========================== START TRAINING TIME 4 ===========================\n",
      "========================== PERFOMANCE METRICS AT TRAINING TIME 4 ===========================\n",
      "accuracy: 0.994188645507376, specificity: 0.9964221824686941, sensitivity: 0.9919571045576407, precision= 0.9964093357271095, recall: 0.9919571045576407 \n",
      "========================== START TRAINING TIME 5 ===========================\n",
      "========================== PERFOMANCE METRICS AT TRAINING TIME 5 ===========================\n",
      "accuracy: 0.997764863656683, specificity: 0.9973190348525469, sensitivity: 0.998211091234347, precision= 0.9973190348525469, recall: 0.998211091234347 \n",
      "====================================== END CROSS VALIDATION ===============================\n",
      "\n",
      "Accuracy:0.9962459297148494 Â± 0.0012832321946209218\n",
      "Accuracy_Var:0.9962459297148494 Â± 1.6466848653116275e-06\n",
      "Specificity:0.9971396643757762 Â± 0.0006694346947215099\n",
      "Sensitivity:0.9953526740109444 Â± 0.0022175813185775993\n",
      "Precison:0.9971345325736483 Â± 0.0006715128138606816\n",
      "Recall:0.9953526740109444 Â± 0.0022175813185775993\n",
      "MCC:0.9924955037569779 Â± 0.002563387816119898\n",
      "AUC: 0.9992330306950319 Â± 0.00046804640248911984\n",
      "PRC: 0.9990883657438699 Â± 0.0005894937958634012\n",
      "F1_Score: 0.9910461529389334 ± 0.005678487874958315\n",
      "0.9962459297148494\t0.9971345325736483\t0.9953526740109444\t0.9971396643757762\t0.9910461529389334\t0.9924955037569779\t0.9992330306950319\t0.9990883657438699\n"
     ]
    }
   ],
   "source": [
    "intermediate_layer_model = Model(inputs=model.input,outputs=model.get_layer(model.layers[-2].name).output)\n",
    "\n",
    "# Use intermediate layer to transform pairs matrix\n",
    "pred = intermediate_layer_model.predict(pred_dataset)\n",
    "p_merge=pd.DataFrame(pred)    \n",
    "Trainlabels = pair_dataframe['label']\n",
    "# create dataframe use transformed pairs matrix outputs and labels\n",
    "X_train_feat=pd.concat((p_merge,pd.DataFrame(pd.DataFrame(Trainlabels))),axis=1,ignore_index=True)\n",
    "\n",
    "# write to file dataframe of transformed pairs matrix and labels\n",
    "X_train_feat.to_csv('X_train.csv',header=False, index=False)\n",
    "\n",
    "# read dataframe of transformed pairs matrix and labels\n",
    "Train=pd.read_csv(\"X_train.csv\",header=None)\n",
    "# Train=Train.sample(frac=1)\n",
    "\n",
    "X=Train.iloc[:,0:164].values\n",
    "y=Train.iloc[:,164:].values\n",
    "\n",
    "extracted_df=X_train_feat\n",
    "\n",
    "\n",
    "y = y.reshape(-1, )\n",
    "\n",
    "accuracy = []\n",
    "specificity = []\n",
    "sensitivity = []\n",
    "precision=[]\n",
    "recall=[]\n",
    "m_coef=[]\n",
    "\n",
    "auc_list=[]\n",
    "prc_list=[]\n",
    "training_time = 1\n",
    "kf=StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n",
    "for train_, test_ in kf.split(X,y):\n",
    "    model_= XGBClassifier(booster='gbtree', reg_lambda=1, alpha=1e-7, subsample=0.8, colsample_bytree=0.2, n_estimators=1000, max_depth=5, min_child_weight=2, gamma=1e-7, eta=1e-6)\n",
    "    print(f\"========================== START TRAINING TIME {training_time} ===========================\")\n",
    "    hist=model_.fit(X[train_], y[train_],eval_set=[(X[test_], y[test_])],verbose=False)\n",
    "    y_pred = model_.predict_proba(X[test_])\n",
    "    y_true = tf.keras.utils.to_categorical(y[test_]) \n",
    "    cm1=confusion_matrix(y_true.argmax(axis=1), y_pred.argmax(axis=1))\n",
    "    acc = (cm1[0,0]+cm1[1,1])/(cm1[0,0]+cm1[0,1]+cm1[1,0]+cm1[1,1])\n",
    "    spec= (cm1[0,0])/(cm1[0,0]+cm1[0,1])\n",
    "    sens = (cm1[1,1])/(cm1[1,0]+cm1[1,1])\n",
    "    prec=cm1[1,1]/(cm1[1,1]+cm1[0,1])\n",
    "    rec=cm1[1,1]/(cm1[1,1]+cm1[1,0])\n",
    "    f1 = 2 * (prec * rec) / (prec + rec)\n",
    "    coef=matthews_corrcoef(y_true.argmax(axis=1), y_pred.argmax(axis=1), sample_weight=None)\n",
    "    auc = metrics.roc_auc_score(y_true, y_pred)\n",
    "    prc = metrics.average_precision_score(y_true, y_pred) # Added\n",
    "\n",
    "    auc_list.append(auc)\n",
    "    prc_list.append(prc)\n",
    "    m_coef.append(coef)\n",
    "    sensitivity.append(sens)\n",
    "    specificity.append(spec)\n",
    "    accuracy.append(acc)\n",
    "    precision.append(prec)\n",
    "    recall.append(rec)\n",
    "    f1_list.append(f1)\n",
    "    print(f\"========================== PERFOMANCE METRICS AT TRAINING TIME {training_time} ===========================\")\n",
    "    print(f\"accuracy: {acc}, specificity: {spec}, sensitivity: {sens}, precision= {prec}, recall: {rec} \")\n",
    "#     if max_accuracy<acc:\n",
    "#         max_accuracy=acc\n",
    "#         xgb_fpr=fpr\n",
    "#         xgb_tpr=tpr\n",
    "    training_time += 1\n",
    "        \n",
    "print(\"====================================== END CROSS VALIDATION ===============================\\n\")\n",
    "\n",
    "\n",
    "mean_acc=np.mean(accuracy)\n",
    "std_acc=np.std(accuracy)\n",
    "var_acc=np.var(accuracy)\n",
    "print(\"Accuracy:\"+str(mean_acc)+\" Â± \"+str(std_acc))\n",
    "print(\"Accuracy_Var:\"+str(mean_acc)+\" Â± \"+str(var_acc))\n",
    "mean_spec=np.mean(specificity)\n",
    "std_spec=np.std(specificity)\n",
    "print(\"Specificity:\"+str(mean_spec)+\" Â± \"+str(std_spec))\n",
    "mean_sens=np.mean(sensitivity)\n",
    "std_sens=np.std(sensitivity)\n",
    "print(\"Sensitivity:\"+str(mean_sens)+\" Â± \"+str(std_sens))\n",
    "mean_prec=np.mean(precision)\n",
    "std_prec=np.std(precision)\n",
    "print(\"Precison:\"+str(mean_prec)+\" Â± \"+str(std_prec))\n",
    "mean_rec=np.mean(recall)\n",
    "std_rec=np.std(recall)\n",
    "print(\"Recall:\"+str(mean_rec)+\" Â± \"+str(std_rec))\n",
    "mean_coef=np.mean(m_coef)\n",
    "std_coef=np.std(m_coef)\n",
    "print(\"MCC:\"+str(mean_coef)+\" Â± \"+str(std_coef))\n",
    "\n",
    "mean_auc = np.mean(auc_list)\n",
    "std_auc = np.std(auc_list)\n",
    "print(\"AUC: \" + str(mean_auc) + \" Â± \" + str(std_auc))\n",
    "\n",
    "mean_prc = np.mean(prc_list)\n",
    "std_prc = np.std(prc_list)\n",
    "print(\"PRC: \" + str(mean_prc) + \" Â± \" + str(std_prc))\n",
    "\n",
    "mean_f1 = np.mean(f1_list)\n",
    "std_f1 = np.std(f1_list)\n",
    "print(\"F1_Score: \" + str(mean_f1)  + \" ± \" + str(std_f1)) \n",
    "\n",
    "end2 = time.time()\n",
    "# print(f\"Runtime of the program is {end2 - start2}\")\n",
    "# print(f\"Total runtime of both DL and ML is {end2 - start2 + mean_train_time}\")\n",
    "\n",
    "print(str(mean_acc) + \"\\t\" + str(mean_prec) + \"\\t\" + str(mean_rec) + \"\\t\" + str(mean_spec) + \"\\t\" + str(mean_f1) \\\n",
    "      + \"\\t\" + str(mean_coef) + \"\\t\" + str(mean_auc) + \"\\t\" + str(mean_prc))\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
